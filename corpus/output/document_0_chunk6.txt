Source: document_0
Section Titles: []
Start Index: 1250
--------------------------------------------------
. in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in @xcite . the rates are about asymptotic behaviors of the excess risk @xmath82 and take the form @xmath83 with @xmath84 . they will be stated under three kinds of conditions involving the hypothesis space @xmath0 , the measure @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 . the first condition is about the approximation ability of the hypothesis space @xmath0 . since the output function @xmath19 is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space @xmath0 with respect to the optimal risk @xmath86 measured by the following approximation error . the approximation error of the triple @xmath87 is defined as @xmath88 to estimate the approximation error , we make an assumption about the minimizer of the risk @xmath89 for each @xmath90 , define the integral operator @xmath91 associated with the kernel @xmath41 by @xmath92 we mention that @xmath93 is a compact and positive operator on @xmath94 . hence we can find its normalized eigenpairs @xmath95 such that @xmath96 is an orthonormal basis of @xmath94 and @xmath97 as @xmath98 . fix @xmath99 . then we can define the @xmath100-th power @xmath101 of @xmath93 by @xmath102 this is a positive and bounded operator and its range is well - defined . the assumption @xmath103 means @xmath104 lies in this range . we assume @xmath105 and @xmath106 where for some @xmath107 and each @xmath108 , @xmath109 is a function of the form @xmath110 with some @xmath111 . the case @xmath112 of assumption means each @xmath113 lies in the rkhs @xmath43 . a standard condition in