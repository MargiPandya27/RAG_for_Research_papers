Source: document_0
Section Titles: []
Start Index: 2000
--------------------------------------------------
that the covering numbers of balls of the sobolev space @xmath149 on the cube @xmath150^s ] , we have @xmath174 2 . let @xmath175 ] since we can use @xmath186\} ] ) . the following theorem , to be proved in section , gives a learning rate for the regularization scheme ( ) in the special case of quantile regression . suppose that @xmath191 almost surely for some constant @xmath192 , and that each kernel @xmath41 is @xmath193 with @xmath194 for some @xmath195 . if assumption holds with @xmath112 and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 ] and a positive constant @xmath220 such that @xmath221 assumption always holds true for @xmath222 . if the triple @xmath223 satisfies some conditions , the exponent @xmath224 can be larger . for example , when @xmath12 is the pinball loss ( ) and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath225 for some @xmath196 ] and @xmath50 ^ 2. ] , then by taking @xmath197 , for any @xmath145 and @xmath199 , ( ) holds with confidence at least @xmath200 . it is unknown whether the above learning rate can be derived by existing approaches in the literature ( e.g. @xcite ) even after projection . note that the kernel in the above example is independent of the sample size . it would be interesting to see whether there exists some @xmath99 such that the function @xmath57 defined by ( ) lies in the range of the operator @xmath254 . the existence of such a positive index would lead to the approximation error condition ( ) , see @xcite . let us now add some numerical comparisons on the goodness of our learning rates given by theorem with those given by @xcite . their corollary 4.12 gives ( essentially