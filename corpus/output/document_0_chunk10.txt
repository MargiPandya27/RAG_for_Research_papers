Source: document_0
Section Titles: []
Start Index: 2250
--------------------------------------------------
. the existence of such a positive index would lead to the approximation error condition ( ) , see @xcite . let us now add some numerical comparisons on the goodness of our learning rates given by theorem with those given by @xcite . their corollary 4.12 gives ( essentially ) minmax optimal learning rates for ( clipped ) svms in the context of nonparametric quantile regression using one gaussian rbf kernel on the whole input space under appropriate smoothness assumptions of the target function . let us consider the case that the distribution @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , where @xmath255 , and assume that both corollary 4.12 in @xcite and our theorem are applicable . i.e. , we assume in particular that @xmath6 is a probability measure on @xmath256 ] ) the additive structure @xmath207 with each @xmath104 as stated in assumption , where @xmath262 and @xmath263 , with minimal risk @xmath86 and additionally fulfills ( to make corollary 4.12 in @xcite applicable ) @xmath264 where @xmath265 ] , and @xmath278 is some user - defined positive constant independent of @xmath279 . for reasons of simplicity , let us fix @xmath280 . then ( * ? ? ? 4.12 ) gives learning rates for the risk of svms for @xmath159-quantile regression , if a single gaussian rbf - kernel on @xmath281 is used for @xmath159-quantile functions of @xmath177-average type @xmath170 with @xmath255 , which are of order @xmath282 hence the learning rate in theorem is better than the one in ( * ? ? ? 4.12 ) in this situation , if @xmath283 provided the assumption of the additive model is valid . table lists the values of @xmath284 from ( ) for some finite values of the dimension @xmath216 , where @xmath285 . all