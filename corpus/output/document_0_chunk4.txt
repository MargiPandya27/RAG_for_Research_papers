Source: document_0
Section Titles: []
Start Index: 750
--------------------------------------------------
a loss function has a long history , see e.g. @xcite in the context of m - estimators . it was shown in @xcite that @xmath22 is also a minimizer of the following optimization problem involving the original loss function @xmath12 if a minimizer exists : @xmath25 the additive model we consider consists of the _ input space decomposition _ @xmath26 with each @xmath27 a complete separable metric space and a _ hypothesis space _ @xmath28 where @xmath29 is a set of functions @xmath30 each of which is also identified as a map @xmath31 from @xmath3 to @xmath5 . hence the functions from @xmath32 take the additive form @xmath33 . we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity @xmath34 is an element of the set @xmath35 which is a subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 each quantity @xmath38 is an element of the full input space @xmath36 , where @xmath39 . because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols . the additive kernel @xmath40 is defined in terms of mercer kernels @xmath41 on @xmath27 as @xmath42 it generates an rkhs @xmath0 which can be written in terms of the rkhs @xmath43 generated by @xmath41 on @xmath27 corresponding to the form ( ) as @xmath44 with norm given by @xmath45 the norm of @xmath46 satisfies @xmath47 to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels . the first example deals with gaussian rbf kernels . all proofs will be given in section .