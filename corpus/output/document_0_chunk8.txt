Source: document_0
Section Titles: []
Start Index: 1750
--------------------------------------------------
. when we add up such functions @xmath126 , we get a good approximation of the target function @xmath121 , and thereby a good estimate of the approximation error . this is the first novelty of the paper . under assumption , we have @xmath127 where @xmath128 is the constant given by @xmath129 the second condition for our learning rates is about the capacity of the hypothesis space measured by @xmath130-empirical covering numbers . let @xmath131 be a set of functions on @xmath21 and @xmath132 for every @xmath133 the * covering number of @xmath131 * with respect to the empirical metric @xmath134 , given by @xmath135 is defined as @xmath136 and the * @xmath130-empirical covering number * of @xmath137 is defined as @xmath138 we assume @xmath139 and that for some @xmath140 , @xmath141 and every @xmath142 , the @xmath130-empirical covering number of the unit ball of @xmath43 satisfies @xmath143 the second novelty of this paper is to observe that the additive nature of the hypothesis space yields the following nice bound with a dimension - independent power exponent for the covering numbers of the balls of the hypothesis space @xmath0 , to be proved in section . under assumption , for any @xmath144 and @xmath145 , we have @xmath146 the bound for the covering numbers stated in theorem is special : the power @xmath147 is independent of the number @xmath148 of the components in the additive model . it is well - known @xcite in the literature of function spaces that the covering numbers of balls of the sobolev space @xmath149 on the cube @xmath150^s ] , we have @xmath174 2 . let @xmath175 ] since we can use @xmath186\} ] ) . the following theorem , to be proved in section , gives a learning rate for the regularization