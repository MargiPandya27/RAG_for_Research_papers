Source: document_0
Section Titles: []
Start Index: 2500
--------------------------------------------------
better than the one in ( * ? ? ? 4.12 ) in this situation , if @xmath283 provided the assumption of the additive model is valid . table lists the values of @xmath284 from ( ) for some finite values of the dimension @xmath216 , where @xmath285 . all of these values of @xmath284 are positive with the exceptions if @xmath286 or @xmath287 . this is in contrast to the corresponding exponent in the learning rate by ( * ? ? * cor . 4.12 ) , because @xmath288 table and figures to give additional information on the limit @xmath289 . of course , higher values of the exponent indicates faster rates of convergence . it is obvious , that an svm based on an additive kernel has a significantly faster rate of convergence in higher dimensions @xmath216 compared to svm based on a single gaussian rbf kernel defined on the whole input space , of course under the assumption that the additive model is valid . the figures seem to indicate that our learning rate from theorem is probably not optimal for small dimensions . however , the main focus of the present paper is on high dimensions . . the table lists the limits of the exponents @xmath290 from ( * ? ? ? * cor . 4.12 ) and @xmath291 from theorem , respectively , if the regularizing parameter @xmath292 is chosen in an optimal manner for the nonparametric setup , i.e. @xmath293 , with @xmath294 for @xmath295 and @xmath296 . recall that @xmath297 $ ] .