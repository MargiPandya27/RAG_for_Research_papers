[
    {
        "file_name": "1505.04597v1.pdf",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": "Olaf Ronneberger (ronneber@informatik.uni-freiburg.de), Philipp Fischer, Thomas Brox",
        "keywords": null,
        "creation_date": "2015-05-19T00:48:02+00:00",
        "text_snippet": "U-Net: Convolutional Networks for Biomedical\nImage Segmentation\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox\nComputer Science Department and BIOSS Centre for Biological Signalling Studies,\nUniversity of Freiburg, Germany\nronneber@informatik.uni-freiburg.de,\nWWW home page: http://lmb.informatik.uni-freiburg.de/\nAbstract. There is large consent that successful training of deep net-\nworks requires many thousand annotated training samples. In this pa-\nper, we present a network and training str",
        "abstract": null
    },
    {
        "file_name": "1612.01105v2.pdf",
        "title": "Pyramid Scene Parsing Network",
        "authors": "Pyramid Scene Parsing Network; Hengshuang Zhao1; Jianping Shi2; Xiaojuan Qi1; Xiaogang Wang1; Jiaya Jia1; 1The Chinese University of Hong Kong; 2SenseTime Group Limited; {hszhao, xjqi, leojia}@cse.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk, shijianping@sensetime.com",
        "keywords": null,
        "creation_date": "2017-04-28T00:19:08+00:00",
        "text_snippet": "Pyramid Scene Parsing Network\nHengshuang Zhao1\nJianping Shi2\nXiaojuan Qi1\nXiaogang Wang1\nJiaya Jia1\n1The Chinese University of Hong Kong\n2SenseTime Group Limited\n{hszhao, xjqi, leojia}@cse.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk, shijianping@sensetime.com\nAbstract\nScene parsing is challenging for unrestricted open vo-\ncabulary and diverse scenes. In this paper, we exploit the\ncapability of global context information by different-region-\nbased context aggregation through our pyramid pooling\nmodule tog",
        "abstract": "Scene parsing is challenging for unrestricted open vo- cabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region- based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is ef- fective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel- level prediction. The proposed approach achieves state-of- the-art performance on various datasets. It came ﬁrst in Im- ageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
    },
    {
        "file_name": "2501.18824v1.pdf",
        "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection Antoine Simoulin*, Namyong Park*, Xiaoyi Liu, Grey Yang",
        "authors": "Memory-Efficient Fine-Tuning of Transformers via Token Selection; Antoine Simoulin*, Namyong Park*, Xiaoyi Liu, Grey Yang; {antoinesimoulin,namyongp,xiaoyiliu,glyang}@meta.com",
        "keywords": null,
        "creation_date": "2025-02-03T01:12:51+00:00",
        "text_snippet": "Memory-Efficient Fine-Tuning of Transformers via Token Selection\nAntoine Simoulin*, Namyong Park*, Xiaoyi Liu, Grey Yang\nMeta AI\n{antoinesimoulin,namyongp,xiaoyiliu,glyang}@meta.com\nAbstract\nFine-tuning provides an effective means to spe-\ncialize pre-trained models for various down-\nstream tasks. However, fine-tuning often in-\ncurs high memory overhead, especially for\nlarge transformer-based models, such as LLMs.\nWhile existing methods may reduce certain\nparts of the memory required for fine-tun",
        "abstract": "Fine-tuning provides an effective means to spe- cialize pre-trained models for various down- stream tasks. However, fine-tuning often in- curs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate ac- tivations computed in the forward pass to up- date weights during the backward pass. In this work, we develop TOKENTUNE, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine- tuning of transformer-based models. During the backward pass, TOKENTUNE approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TOKENTUNE, only a subset of intermedi- ate activations are cached during the forward pass. Also, TOKENTUNE can be easily com- bined with existing methods like LoRA, fur- ther reducing the memory cost. We evaluate our approach on pre-trained transformer mod- els with up to billions of parameters, consider- ing the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Over- all, TOKENTUNE achieves performance on par with full fine-tuning or representative memory- efficient fine-tuning methods, while greatly re- ducing the memory footprint, especially when combined with other methods with comple- mentary memory reduction mechanisms. We hope that our approach will facilitate the fine- tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger sys- tem. Our code is available at https://github. com/facebookresearch/tokentune. 1"
    },
    {
        "file_name": "aaai25-hybrid_optimizer.pdf",
        "title": "Towards Efficient Low-order Hybrid Optimizer for Language Model Fine-tuning Minping Chen1, You-Liang Huang1, Zeyi Wen",
        "authors": "Towards Efficient Low-order Hybrid Optimizer for Language Model Fine-tuning; Minping Chen1, You-Liang Huang1, Zeyi Wen; 1 The Hong Kong University of Science and Technology (Guangzhou); 2 The Hong Kong University of Science and Technology; {mchen779, yhuang142}@connect.hkust-gz.edu.cn, wenzeyi@ust.hk; Low-order Hybrid Optimizer (LoHO) which merges zeroth-",
        "keywords": null,
        "creation_date": "2024-12-14T07:45:15+00:00",
        "text_snippet": "Towards Efficient Low-order Hybrid Optimizer for Language Model Fine-tuning\nMinping Chen1, You-Liang Huang1, Zeyi Wen\n*1,2\n1 The Hong Kong University of Science and Technology (Guangzhou)\n2 The Hong Kong University of Science and Technology\n{mchen779, yhuang142}@connect.hkust-gz.edu.cn, wenzeyi@ust.hk\nAbstract\nAs the size of language models notably grows, fine-tuning\nthe models becomes more challenging: fine-tuning with first-\norder optimizers (e.g., SGD and Adam) requires high mem-\nory consumpt",
        "abstract": "As the size of language models notably grows, fine-tuning the models becomes more challenging: fine-tuning with first- order optimizers (e.g., SGD and Adam) requires high mem- ory consumption, while fine-tuning with a memory-efficient zeroth-order optimizer (MeZO) has a significant accuracy drop and slower convergence rate. In this work, we propose a Low-order Hybrid Optimizer (LoHO) which merges zeroth- order (ZO) and first-order (FO) optimizers for fine-tuning. LoHO is empowered with inter-layer hybrid optimization and intra-layer hybrid optimization, which boosts the accuracy of MeZO while keeping memory usage within a budget. The inter-layer hybrid optimization exploits the FO optimizer in deep layers and the ZO optimizer in shallow ones, therefore avoiding unnecessary gradient propagation to improve mem- ory efficiency. The intra-layer hybrid optimization updates a proportion of parameters in a layer by the ZO optimizer, and the rest by the FO optimizer, taking advantage of gradient sparsity for high efficiency implementation. Our experimen- tal results across common datasets on different pre-trained backbones (i.e., RoBERTa-large, OPT-13B and OPT-30B) demonstrate that LoHO can significantly improve the pre- dictive accuracy and convergence rate of MeZO, while con- trolling the memory footprint during fine-tuning. Moreover, LoHO can achieve comparable performance with first-order fine-tuning using substantially fewer memory resources."
    },
    {
        "file_name": "ADDAX.pdf",
        "title": "Published as a conference paper at ICLR 2024 ADDAX: MEMORY-EFFICIENT FINE-TUNING OF LANGUAGE MODELS WITH A COMBINATION OF FORWARDBACKWARD AND FORWARD-ONLY PASSES",
        "authors": "Zeman Li; Xinwei Zhang; Meisam Razaviyayn; University of Southern California; {zemanli,xinweiz,razaviya}@usc.edu",
        "keywords": null,
        "creation_date": "2024-04-05T07:10:19+00:00",
        "text_snippet": "Published as a conference paper at ICLR 2024\nADDAX: MEMORY-EFFICIENT FINE-TUNING OF LAN-\nGUAGE MODELS WITH A COMBINATION OF FORWARD-\nBACKWARD AND FORWARD-ONLY PASSES\nZeman Li\nXinwei Zhang\nMeisam Razaviyayn\nUniversity of Southern California\n{zemanli,xinweiz,razaviya}@usc.edu\nABSTRACT\nFine-tuning language models (LMs) with ﬁrst-order optimizers often demands\nexcessive memory, limiting accessibility, while zeroth-order optimizers use less\nmemory, but suffer from slow convergence depending on model ",
        "abstract": "Fine-tuning language models (LMs) with ﬁrst-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We in- troduce a novel method named Addax that integrates the recently introduced Memory-Efﬁcient Zeroth-order Optimizer of Malladi et al. (2023) with Stochastic Gradient Descent (SGD). Addax obtains zeroth-order and ﬁrst-order gradient esti- mates and optimally combines them as the descent direction in each step. The ﬁrst- order updates are performed “in-place” to further save memory. Theoretically, we establish the convergence of Addax under mild assumptions, demonstrating less restrictive hyper-parameters and independence from model size. Our extensive ex- periments with diverse LMs and tasks show that Addax consistently outperforms zero-shot and MeZO in terms of accuracy. Moreover, Addax surpasses the per- formance of standard ﬁne-tuning approaches, such as SGD and Adam, in speciﬁc scenarios with signiﬁcantly less memory requirement. 1"
    },
    {
        "file_name": "DiZO.pdf",
        "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning",
        "authors": "Qitao Tan, Jun Liu, Zheng Zhan, Caiwen Ding, Yanzhi Wang, Jin Lu, Geng Yuan",
        "keywords": "Machine Learning, ICML",
        "creation_date": "2025-02-06T01:58:23+00:00",
        "text_snippet": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\nZeroth-order LLM Fine-tuning\nQitao Tan 1 Jun Liu 2 Zheng Zhan 2 Caiwen Ding 3 Yanzhi Wang 2 Jin Lu 1 Geng Yuan 1\nAbstract\nLarge language models (LLMs) excel across vari-\nous tasks, but standard first-order (FO) fine-tuning\ndemands considerable memory, significantly lim-\niting real-world deployment. Recently, zeroth-\norder (ZO) optimization stood out as a promis-\ning memory-efficient training paradigm, avoiding\nbackward passes an",
        "abstract": "Large language models (LLMs) excel across vari- ous tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly lim- iting real-world deployment. Recently, zeroth- order (ZO) optimization stood out as a promis- ing memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both conver- gence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learn- ing capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude up- dates precisely scaled to layer-wise individual op- timization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48% on var- ious datasets. Moreover, DiZO consistently out- performs the representative ZO baselines in fine- tuning RoBERTa-large, OPT-series, and Llama- series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning."
    },
    {
        "file_name": "Galore.pdf",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "authors": "Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian",
        "keywords": null,
        "creation_date": "2024-06-04T01:10:36+00:00",
        "text_snippet": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\nJiawei Zhao 1 Zhenyu Zhang 3 Beidi Chen 2 4 Zhangyang Wang 3 Anima Anandkumar * 1 Yuandong Tian * 2\nAbstract\nTraining\nLarge\nLanguage\nModels\n(LLMs)\npresents significant memory challenges, predom-\ninantly due to the growing size of weights and\noptimizer states.\nCommon memory-reduction\napproaches,\nsuch\nas\nlow-rank\nadaptation\n(LoRA), add a trainable low-rank matrix to the\nfrozen pre-trained weight in each layer. How-\never, such ap",
        "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predom- inantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer. How- ever, such approaches typically underperform training with full-rank weights in both pre- training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full- parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) with- out model parallel, checkpointing, or offloading strategies. Code is provided in the link."
    },
    {
        "file_name": "MaZO.pdf",
        "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models Zhen Zhang1, Yifan Yang1, Kai Zhen2, Nathan Susanj2, Athanasios Mouchtaris2, Siegfried Kunzmann2, Zheng Zhang1 1University of California, Santa Barbara",
        "authors": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of; Large Language Models; Zhen Zhang1, Yifan Yang1, Kai Zhen2, Nathan Susanj2, Athanasios Mouchtaris2,; Siegfried Kunzmann2, Zheng Zhang1; 1University of California, Santa Barbara; zhen_zhang@ucsb.edu, zhengzhang@ece.ucsb.edu",
        "keywords": null,
        "creation_date": "2025-02-18T02:29:09+00:00",
        "text_snippet": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of\nLarge Language Models\nZhen Zhang1, Yifan Yang1, Kai Zhen2, Nathan Susanj2, Athanasios Mouchtaris2,\nSiegfried Kunzmann2, Zheng Zhang1\n1University of California, Santa Barbara\n2Amazon\nzhen_zhang@ucsb.edu, zhengzhang@ece.ucsb.edu\nAbstract\nLarge language models have demonstrated ex-\nceptional capabilities across diverse tasks, but\ntheir fine-tuning demands significant mem-\nory, posing challenges for resource-constrained\nenvironment",
        "abstract": "Large language models have demonstrated ex- ceptional capabilities across diverse tasks, but their fine-tuning demands significant mem- ory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimiza- tion provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve gen- eralization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these chal- lenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first- order optimization. 1"
    },
    {
        "file_name": "NeurIPS-2024-lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning-Paper-Conference.pdf",
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning Rui Pan♠∗, Xiang Liu♣∗, Shizhe Diao♦, Renjie Pi♡, Jipeng Zhang♡, Chi Han♠, Tong Zhang♠",
        "authors": "LISA: Layerwise Importance Sampling for; Memory-Efficient Large Language Model Fine-Tuning; Rui Pan♠∗, Xiang Liu♣∗, Shizhe Diao♦, Renjie Pi♡, Jipeng Zhang♡,; Chi Han♠, Tong Zhang♠; ♠University of Illinois Urbana-Champaign; ♣The Hong Kong University of Science and Technology(Guangzhou); ♦NVIDIA ♡The Hong Kong University of Science and Technology; {ruip4, chihan3, tozhang}@illinois.edu; xliu886@connect.hkust-gz.edu.cn; {sdiaoaa, rpi, jzhanggr}@ust.hk",
        "keywords": null,
        "creation_date": "2024-12-20T06:31:49+00:00",
        "text_snippet": "LISA: Layerwise Importance Sampling for\nMemory-Efficient Large Language Model Fine-Tuning\nRui Pan♠∗, Xiang Liu♣∗, Shizhe Diao♦, Renjie Pi♡, Jipeng Zhang♡,\nChi Han♠, Tong Zhang♠\n♠University of Illinois Urbana-Champaign\n♣The Hong Kong University of Science and Technology(Guangzhou)\n♦NVIDIA ♡The Hong Kong University of Science and Technology\n{ruip4, chihan3, tozhang}@illinois.edu\nxliu886@connect.hkust-gz.edu.cn\n{sdiaoaa, rpi, jzhanggr}@ust.hk\nAbstract\nThe machine learning community has witnessed im",
        "abstract": "The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory con- sumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter efficient fine-tuning techniques such as Low-Rank Adap- tation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank sub- space. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to differ- ent layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains. 1"
    },
    {
        "file_name": "NeurIPS-2024-thinking-forward-memory-efficient-federated-finetuning-of-language-models-Paper-Conference.pdf",
        "title": "Thinking Forward: Memory-Efficient Federated Finetuning of Language Models",
        "authors": "Thinking Forward: Memory-Efficient; Federated Finetuning of Language Models; Kunjal Panchal; kpanchal@umass.edu; Nisarg Parikh; nkparikh@umass.edu; Sunav Choudhary; Adobe Research; schoudha@adobe.com; Lijun Zhang",
        "keywords": null,
        "creation_date": "2024-10-22T13:06:20+00:00",
        "text_snippet": "Thinking Forward: Memory-Efficient\nFederated Finetuning of Language Models\nKunjal Panchal\nUniversity of Massachusetts\nAmherst, MA 01003-9264\nkpanchal@umass.edu\nNisarg Parikh\nUniversity of Massachusetts\nAmherst, MA 01003-9264\nnkparikh@umass.edu\nSunav Choudhary\nAdobe Research\nBangalore, India 560103\nschoudha@adobe.com\nLijun Zhang\nUniversity of Massachusetts\nAmherst, MA 01003-9264\nlijunzhang@cs.umass.edu\nYuriy Brun\nUniversity of Massachusetts\nAmherst, MA 01003-9264\nbrun@cs.umass.edu\nHui Guan\nUniver",
        "abstract": "Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropa- gation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce SPRY, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimations of the true gradients. SPRY achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in SPRY are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive SPRY’s convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, SPRY reduces the memory footprint during training by 1.4–7.1× in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. SPRY reduces the convergence time by 1.2–20.3× and achieves 5.2–13.5% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, SPRY only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. SPRY makes feasible previously impossible FL deployments on commodity mobile and edge devices. Our source code is available for replication at https://github.com/Astuary/Spry. 1"
    },
    {
        "file_name": "Parameter-Efficient Fine-Tuning for Large Models.pdf",
        "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey Zeyu Han1, Chao Gao2, Jinyang Liu1, Jeff (Jun) Zhang3, and Sai Qian Zhang*4 2University of California, Riverside",
        "authors": "Parameter-Efficient Fine-Tuning for Large Models:; A Comprehensive Survey; Zeyu Han1, Chao Gao2, Jinyang Liu1, Jeff (Jun) Zhang3, and Sai Qian Zhang*4; 3Arizona State University; 4New York University; {han.zeyu,liu.jinyan}@northeastern.edu, cgao037@ucr.edu, jeffzhang@asu.edu, sai.zhang@nyu.edu",
        "keywords": null,
        "creation_date": "2024-09-17T01:09:26+00:00",
        "text_snippet": "1\nParameter-Efficient Fine-Tuning for Large Models:\nA Comprehensive Survey\nZeyu Han1, Chao Gao2, Jinyang Liu1, Jeff (Jun) Zhang3, and Sai Qian Zhang*4\n1Northeastern University\n2University of California, Riverside\n3Arizona State University\n4New York University\n{han.zeyu,liu.jinyan}@northeastern.edu, cgao037@ucr.edu, jeffzhang@asu.edu, sai.zhang@nyu.edu\nAbstract—Large models represent a groundbreaking advance-\nment in multiple application fields, enabling remarkable achieve-\nments across various t",
        "abstract": null
    },
    {
        "file_name": "QuZO.pdf",
        "title": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models Jiajun Zhou1,3† Yifan Yang1, Kai Zhen2, Ziyue Liu1, Yequan Zhao1, Ershad Banijamali2, Athanasios Mouchtaris2, Ngai Wong3, Zheng Zhang1 1University of California, Santa Barbara, 2Amazon AGI",
        "authors": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models; Jiajun Zhou1,3† Yifan Yang1, Kai Zhen2, Ziyue Liu1, Yequan Zhao1,; Ershad Banijamali2, Athanasios Mouchtaris2, Ngai Wong3, Zheng Zhang1; 1University of California, Santa Barbara, 2Amazon AGI; 3The University of Hong Kong; {jjzhou,nwong}@eee.hku.hk, zhengzhang@ece.ucsb.edu; Language Models (LLMs) are often quantized",
        "keywords": null,
        "creation_date": "2025-02-19T01:10:01+00:00",
        "text_snippet": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models\nJiajun Zhou1,3† Yifan Yang1, Kai Zhen2, Ziyue Liu1, Yequan Zhao1,\nErshad Banijamali2, Athanasios Mouchtaris2, Ngai Wong3, Zheng Zhang1\n1University of California, Santa Barbara, 2Amazon AGI\n3The University of Hong Kong\n{jjzhou,nwong}@eee.hku.hk, zhengzhang@ece.ucsb.edu\nAbstract\nLanguage Models (LLMs) are often quantized\nto lower precision to reduce the memory cost\nand latency in inference. However, quantiza-\ntion often degrades mo",
        "abstract": "Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantiza- tion often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimiza- tion require backpropagation, which are error- prone in the low-precision settings. To over- come these limitations, we propose the Quan- tized Zeroth-Order (QuZO) framework, specif- ically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low- precision straight-through estimator, and uti- lizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the train- ing process, while achieving results compara- ble to first-order methods in FP8 and superior accuracy in INT8 and INT4 training. Experi- ments demonstrate that low-bit training QuZO achieves performance comparable to MeZO op- timization on GLUE, Multi-Choice, and Gen- eration tasks, while reducing memory cost by 2.94× in LLaMA2-7B fine-tuning compared to quantized first-order methods. 1"
    },
    {
        "file_name": "Survey of MeZO paper.pdf",
        "title": "Fine-Tuning Language Models with Just",
        "authors": "Fine-Tuning Language Models with Just; Forward Passes; Sadhika Malladi∗; Tianyu Gao∗; Eshaan Nichani; Alex Damian; Danqi Chen; Sanjeev Arora; Princeton University; {smalladi, tianyug, eshnich, ad27, jasonlee, danqic, arora}@princeton.edu",
        "keywords": null,
        "creation_date": "2024-01-12T01:40:40+00:00",
        "text_snippet": "Fine-Tuning Language Models with Just\nForward Passes\nSadhika Malladi∗\nTianyu Gao∗\nEshaan Nichani\nAlex Damian\nJason D. Lee\nDanqi Chen\nSanjeev Arora\nPrinceton University\n{smalladi, tianyug, eshnich, ad27, jasonlee, danqic, arora}@princeton.edu\nAbstract\nFine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two",
        "abstract": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zeroth- order optimizer (MeZO), adapting the classical ZO-SGD method to operate in- place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12× memory reduction and up to 2× GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.1 1"
    }
]