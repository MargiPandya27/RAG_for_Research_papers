{
    "Paper_1": {
        "title": "U-Net: Convolutional Networks for Biomedical",
        "sections": [
            {
                "section": "Abstract",
                "content": ". There is large consent that successful training of deep net- works requires many thousand annotated training samples. In this pa- per, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localiza- tion. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu- ronal structures in electron microscopic stacks. Using the same net- work trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these cate- gories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. 1"
            },
            {
                "section": "Introduction",
                "content": "In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks have already existed for a long time [8], their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classiﬁcation tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. More- over, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel arXiv:1505.04597v1  [cs.CV]  18 May 2015 2 copy and crop input image tile output segmentation map 64 1 128 256 512 1024 max pool 2x2 up-conv 2x2 conv 3x3, ReLU 572 x 572 284² 64 128 256 512 570 x 570 568 x 568 282² 280² 140² 138² 136² 68² 66² 64² 32² 28² 56² 54² 52² 512 104² 102² 100² 200² 30² 198² 196² 392 x 392 390 x 390 388 x 388 388 x 388 1024 512 256 256 128 64 128 64 2 conv 1x1 Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the diﬀerent operations. as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-oﬀbetween localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classiﬁer output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time. In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network” [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled 3 Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modiﬁcation in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training im- ages. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simu- lated eﬃciently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touch- ing objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation prob- lems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out- 4 performed the network of Ciresan et al. [1]. Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking chal- lenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets. 2 Network Architecture The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectiﬁed linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each fol- lowed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the ﬁnal layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. 3 Training The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caﬀe [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. The energy function is computed by a pixel-wise soft-max over the ﬁnal feature map combined with the cross entropy loss function. The soft-max is deﬁned as pk(x) = exp(ak(x))/ \u0010PK k′=1 exp(ak′(x)) \u0011 where ak(x) denotes the activation in feature channel k at the pixel position x ∈Ωwith Ω⊂Z2. K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈1 for the k that has the maximum activation ak(x) and pk(x) ≈0 for all other k. The cross entropy then penalizes at each position the deviation of pℓ(x)(x) from 1 using E = X x∈Ω w(x) log(pℓ(x)(x)) (1) 5 a b c d Fig. 3. HeLa cells on glass recorded with DIC (diﬀerential interference contrast) mi- croscopy. (a) raw image. (b) overlay with ground truth segmentation. Diﬀerent colors indicate diﬀerent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels. where ℓ: Ω→{1, . . . , K} is the true label of each pixel and w : Ω→R is a weight map that we introduced to give some pixels more importance in the training. We pre-compute the weight map for each ground truth segmentation to com- pensate the diﬀerent frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d). The separation border is computed using morphological operations. The weight map is then computed as w(x) = wc(x) + w0 · exp −(d1(x) + d2(x))2 2σ2 ! (2) where wc : Ω→R is the weight map to balance the class frequencies, d1 : Ω→R denotes the distance to the border of the nearest cell and d2 : Ω→R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and σ ≈5 pixels. In deep networks with many convolutional layers and diﬀerent paths through the network, a good initialization of the weights is extremely important. Oth- erwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of p 2/N, where N denotes the number of incoming nodes of one neu- ron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer N = 9 · 64 = 576. 3.1 Data Augmentation Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 6 microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation. 4 Experiments We demonstrate the application of the u-net to three diﬀerent segmentation tasks. The ﬁrst task is the segmentation of neuronal structures in electron mi- croscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila ﬁrst instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its seg- mentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 diﬀerent levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14]. The u-net (averaged over 7 rotated versions of the input data) achieves with- out any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is signiﬁcantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error. Rank Group name Warping Error Rand Error Pixel Error ** human values ** 0.000005 0.0021 0.0010 1. u-net 0.000353 0.0382 0.0611 2. DIVE-SCI 0.000355 0.0305 0.0584 3. IDSIA [1] 0.000420 0.0504 0.0613 4. DIVE 0.000430 0.0545 0.0582 ... 10. IDSIA-SCI 0.000653 0.0189 0.1027 7 a b c d Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the “PhC-U373” data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the “DIC-HeLa” data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border). Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015. Name PhC-U373 DIC-HeLa IMCB-SG (2014) 0.2669 0.2935 KTH-SE (2014) 0.7953 0.4607 HOUS-US (2014) 0.5323 - second-best 2015 0.83 0.46 u-net (2015) 0.9203 0.7756 algorithms on this data set use highly data set speciﬁc post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic im- ages. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. The ﬁrst data set “PhC-U373”2 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated train- ing images. Here we achieve an average IOU (“intersection over union”) of 92%, which is signiﬁcantly better than the second best algorithm with 83% (see Ta- ble 2). The second data set “DIC-HeLa”3 are HeLa cells on a ﬂat glass recorded by diﬀerential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is signiﬁcantly better than the second best algorithm with 46%. 5"
            },
            {
                "section": "Conclusion",
                "content": "The u-net architecture achieves very good performance on very diﬀerent biomed- ical segmentation applications. Thanks to data augmentation with elastic defor- 1 The authors of this algorithm have submitted 78 diﬀerent solutions to achieve this result. 2 Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA) 3 Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands 8 mations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caﬀe[6]-based implementation and the trained networks4. We are sure that the u-net architecture can be applied easily to many more tasks. Acknowlegements This study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B)."
            }
        ]
    },
    "Paper_2": {
        "title": "Pyramid Scene Parsing Network",
        "sections": [
            {
                "section": "Abstract",
                "content": "Scene parsing is challenging for unrestricted open vo- cabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region- based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is ef- fective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel- level prediction. The proposed approach achieves state-of- the-art performance on various datasets. It came ﬁrst in Im- ageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
            },
            {
                "section": "Introduction",
                "content": "Scene parsing, based on semantic segmentation, is a fun- damental topic in computer vision. The goal is to assign each pixel in the image a category label. Scene parsing pro- vides complete understanding of the scene. It predicts the label, location, as well as shape for each element. This topic is of broad interest for potential applications of automatic driving, robot sensing, to name a few. Difﬁculty of scene parsing is closely related to scene and label variety. The pioneer scene parsing task [23] is to clas- sify 33 scenes for 2,688 images on LMO dataset [22]. More recent PASCAL VOC semantic segmentation and PASCAL context datasets [8, 29] include more labels with similar context, such as chair and sofa, horse and cow, etc. The new ADE20K dataset [43] is the most challenging one with a large and unrestricted open vocabulary and more scene classes. A few representative images are shown in Fig. 1. To develop an effective algorithm for these datasets needs to conquer a few difﬁculties. State-of-the-art scene parsing frameworks are mostly based on the fully convolutional network (FCN) [26]. The deep convolutional neural network (CNN) based methods boost dynamic object understanding, and yet still face chal- Figure 1. Illustration of complex scenes in ADE20K dataset. lenges considering diverse scenes and unrestricted vocabu- lary. One example is shown in the ﬁrst row of Fig. 2, where a boat is mistaken as a car. These errors are due to similar appearance of objects. But when viewing the image regard- ing the context prior that the scene is described as boathouse near a river, correct prediction should be yielded. Towards accurate scene perception, the knowledge graph relies on prior information of scene context. We found that the major issue for current FCN based models is lack of suitable strategy to utilize global scene category clues. For typical complex scene understanding, previously to get a global image-level feature, spatial pyramid pooling [18] was widely employed where spatial statistics provide a good descriptor for overall scene interpretation. Spatial pyramid pooling network [12] further enhances the ability. Different from these methods, to incorporate suitable global features, we propose pyramid scene parsing network (PSPNet). In addition to traditional dilated FCN [3, 40] for pixel prediction, we extend the pixel-level feature to the specially designed global pyramid pooling one. The local and global clues together make the ﬁnal prediction more reliable. We also propose an optimization strategy with 1 arXiv:1612.01105v2  [cs.CV]  27 Apr 2017 deeply supervised loss. We give all implementation details, which are key to our decent performance in this paper, and make the code and trained models publicly available 1. Our approach achieves state-of-the-art performance on all available datasets. It is the champion of ImageNet scene parsing challenge 2016 [43], and arrived the 1st place on PASCAL VOC 2012 semantic segmentation benchmark [8], and the 1st place on urban scene Cityscapes data [6]. They manifest that PSPNet gives a promising direction for pixel- level prediction tasks, which may even beneﬁt CNN-based stereo matching, optical ﬂow, depth estimation, etc. in follow-up work. Our main contributions are threefold. • We propose a pyramid scene parsing network to em- bed difﬁcult scenery context features in an FCN based pixel prediction framework. • We develop an effective optimization strategy for deep ResNet [13] based on deeply supervised loss. • We build a practical system for state-of-the-art scene parsing and semantic segmentation where all crucial implementation details are included. 2. Related Work In the following, we review recent advances in scene parsing and semantic segmentation tasks. Driven by pow- erful deep neural networks [17, 33, 34, 13], pixel-level prediction tasks like scene parsing and semantic segmen- tation achieve great progress inspired by replacing the fully-connected layer in classiﬁcation with the convolution layer [26]. To enlarge the receptive ﬁeld of neural networks,"
            },
            {
                "section": "methods",
                "content": "of [3, 40] used dilated convolution. Noh et al. [30] proposed a coarse-to-ﬁne structure with deconvolution net- work to learn the segmentation mask. Our baseline network is FCN and dilated network [26, 3]. Other work mainly proceeds in two directions. One line [26, 3, 5, 39, 11] is with multi-scale feature ensembling. Since in deep networks, higher-layer feature contains more semantic meaning and less location information. Combin- ing multi-scale features can improve the performance. The other direction is based on structure prediction. The pioneer work [3] used conditional random ﬁeld (CRF) as post processing to reﬁne the segmentation result. Following"
            },
            {
                "section": "methods",
                "content": "[25, 41, 1] reﬁned networks via end-to-end model- ing. Both of the two directions ameliorate the localization ability of scene parsing where predicted semantic boundary ﬁts objects. Yet there is still much room to exploit necessary information in complex scenes. To make good use of global image-level priors for di- verse scene understanding, methods of [18, 27] extracted global context information with traditional features not from deep neural networks. Similar improvement was made 1https://github.com/hszhao/PSPNet under object detection frameworks [35]. Liu et al. [24] proved that global average pooling with FCN can improve semantic segmentation results. However, our experiments show that these global descriptors are not representative enough for the challenging ADE20K data. Therefore, dif- ferent from global pooling in [24], we exploit the capabil- ity of global context information by different-region-based context aggregation via our pyramid scene parsing network. 3. Pyramid Scene Parsing Network We start with our observation and analysis of represen- tative failure cases when applying FCN methods to scene parsing. They motivate proposal of our pyramid pooling module as the effective global context prior. Our pyramid scene parsing network (PSPNet) illustrated in Fig. 3 is then described to improve performance for open-vocabulary ob- ject and stuff identiﬁcation in complex scene parsing. 3.1. Important Observations The new ADE20K dataset [43] contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 image- level scene descriptors (e.g., airport terminal, bedroom, and street). So a large amount of labels and vast distributions of scenes come into existence. Inspecting the prediction"
            },
            {
                "section": "results",
                "content": "of the FCN baseline provided in [43], we summarize several common issues for complex-scene parsing. Mismatched Relationship Context relationship is uni- versal and important especially for complex scene under- standing. There exist co-occurrent visual patterns. For ex- ample, an airplane is likely to be in runway or ﬂy in sky while not over a road. For the ﬁrst-row example in Fig. 2, FCN predicts the boat in the yellow box as a “car” based on its appearance. But the common knowledge is that a car is seldom over a river. Lack of the ability to collect contextual information increases the chance of misclassiﬁcation. Confusion Categories There are many class label pairs in the ADE20K dataset [43] that are confusing in classiﬁ- cation. Examples are ﬁeld and earth; mountain and hill; wall, house, building and skyscraper. They are with simi- lar appearance. The expert annotator who labeled the entire dataset, still makes 17.60% pixel error as described in [43]. In the second row of Fig. 2, FCN predicts the object in the box as part of skyscraper and part of building. These re- sults should be excluded so that the whole object is either skyscraper or building, but not both. This problem can be remedied by utilizing the relationship between categories. Inconspicuous Classes Scene contains objects/stuff of arbitrary size. Several small-size things, like streetlight and signboard, are hard to ﬁnd while they may be of great im- portance. Contrarily, big objects or stuff may exceed the Figure 2. Scene parsing issues we observe on ADE20K [43] dataset. The ﬁrst row shows the issue of mismatched relationship – cars are seldom over water than boats. The second row shows confusion categories where class “building” is easily confused as “skyscraper”. The third row illustrates inconspicuous classes. In this example, the pillow is very similar to the bed sheet in terms of color and texture. These inconspicuous objects are easily misclassiﬁed by FCN. receptive ﬁeld of FCN and thus cause discontinuous pre- diction. As shown in the third row of Fig. 2, the pillow has similar appearance with the sheet. Overlooking the global scene category may fail to parse the pillow. To im- prove performance for remarkably small or large objects, one should pay much attention to different sub-regions that contain inconspicuous-category stuff. To summarize these observations, many errors are par- tially or completely related to contextual relationship and global information for different receptive ﬁelds. Thus a deep network with a suitable global-scene-level prior can much improve the performance of scene parsing. 3.2. Pyramid Pooling Module With above analysis, in what follows, we introduce the pyramid pooling module, which empirically proves to be an effective global contextual prior. In a deep neural network, the size of receptive ﬁeld can roughly indicates how much we use context information. Although theoretically the receptive ﬁeld of ResNet [13] is already larger than the input image, it is shown by Zhou et al. [42] that the empirical receptive ﬁeld of CNN is much smaller than the theoretical one especially on high-level lay- ers. This makes many networks not sufﬁciently incorporate the momentous global scenery prior. We address this issue by proposing an effective global prior representation. Global average pooling is a good baseline model as the global contextual prior, which is commonly used in image classiﬁcation tasks [34, 13]. In [24], it was successfully ap- plied to semantic segmentation. But regarding the complex- scene images in ADE20K [43], this strategy is not enough to cover necessary information. Pixels in these scene images are annotated regarding many stuff and objects. Directly fusing them to form a single vector may lose the spatial rela- tion and cause ambiguity. Global context information along with sub-region context is helpful in this regard to distin- guish among various categories. A more powerful represen- tation could be fused information from different sub-regions with these receptive ﬁelds. Similar conclusion was drawn in classical work [18, 12] of scene/image classiﬁcation. In [12], feature maps in different levels generated by pyramid pooling were ﬁnally ﬂattened and concatenated to be fed into a fully connected layer for classiﬁcation. This global prior is designed to remove the ﬁxed-size constraint of CNN for image classiﬁcation. To further reduce context information loss between different sub-regions, we propose a hierarchical global prior, containing information with dif- ferent scales and varying among different sub-regions. We Figure 3. Overview of our proposed PSPNet. Given an input image (a), we ﬁrst use CNN to get the feature map of the last convolutional layer (b), then a pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatena- tion layers to form the ﬁnal feature representation, which carries both local and global context information in (c). Finally, the representation is fed into a convolution layer to get the ﬁnal per-pixel prediction (d). call it pyramid pooling module for global scene prior con- struction upon the ﬁnal-layer-feature-map of the deep neu- ral network, as illustrated in part (c) of Fig. 3. The pyramid pooling module fuses features under four different pyramid scales. The coarsest level highlighted in red is global pooling to generate a single bin output. The following pyramid level separates the feature map into dif- ferent sub-regions and forms pooled representation for dif- ferent locations. The output of different levels in the pyra- mid pooling module contains the feature map with varied sizes. To maintain the weight of global feature, we use 1×1 convolution layer after each pyramid level to reduce the di- mension of context representation to 1/N of the original one if the level size of pyramid is N. Then we directly up- sample the low-dimension feature maps to get the same size feature as the original feature map via bilinear interpolation. Finally, different levels of features are concatenated as the ﬁnal pyramid pooling global feature. Noted that the number of pyramid levels and size of each level can be modiﬁed. They are related to the size of feature map that is fed into the pyramid pooling layer. The struc- ture abstracts different sub-regions by adopting varying-size pooling kernels in a few strides. Thus the multi-stage ker- nels should maintain a reasonable gap in representation. Our pyramid pooling module is a four-level one with bin sizes of 1×1, 2×2, 3×3 and 6×6 respectively. For the type of pooling operation between max and average, we perform extensive experiments to show the difference in Section 5.2. 3.3. Network Architecture With the pyramid pooling module, we propose our pyra- mid scene parsing network (PSPNet) as illustrated in Fig. 3. Given an input image in Fig. 3(a), we use a pretrained ResNet [13] model with the dilated network strategy [3, 40] to extract the feature map. The ﬁnal feature map size is 1/8 of the input image, as shown in Fig. 3(b). On top of the Figure 4. Illustration of auxiliary loss in ResNet101. Each blue box denotes a residue block. The auxiliary loss is added after the res4b22 residue block. map, we use the pyramid pooling module shown in (c) to gather context information. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior. Then we concatenate the prior with the original feature map in the ﬁnal part of (c). It is followed by a convolution layer to generate the ﬁnal prediction map in (d). To explain our structure, PSPNet provides an effective global contextual prior for pixel-level scene parsing. The pyramid pooling module can collect levels of information, more representative than global pooling [24]. In terms of computational cost, our PSPNet does not much increase it compared to the original dilated FCN network. In end-to- end learning, the global pyramid pooling module and the local FCN feature can be optimized simultaneously. 4. Deep Supervision for ResNet-Based FCN Deep pretrained networks lead to good performance [17, 33, 13]. However, increasing depth of the network may introduce additional optimization difﬁculty as shown in [32, 19] for image classiﬁcation. ResNet solves this prob- lem with skip connection in each block. Latter layers of deep ResNet mainly learn residues based on previous ones. We contrarily propose generating initial results by super- vision with an additional loss, and learning the residue af- terwards with the ﬁnal loss. Thus, optimization of the deep network is decomposed into two, each is simpler to solve. An example of our deeply supervised ResNet101 [13] model is illustrated in Fig. 4. Apart from the main branch using softmax loss to train the ﬁnal classiﬁer, another clas- siﬁer is applied after the fourth stage, i.e., the res4b22 residue block. Different from relay backpropagation [32] that blocks the backward auxiliary loss to several shallow layers, we let the two loss functions pass through all pre- vious layers. The auxiliary loss helps optimize the learning process, while the master branch loss takes the most respon- sibility. We add weight to balance the auxiliary loss. In the testing phase, we abandon this auxiliary branch and only use the well optimized master branch for ﬁnal pre- diction. This kind of deeply supervised training strategy for ResNet-based FCN is broadly useful under different ex- perimental settings and works with the pre-trained ResNet model. This manifests the generality of such a learning strategy. More details are provided in Section 5.2. 5. Experiments Our proposed method is successful on scene parsing and semantic segmentation challenges. We evaluate it in this section on three different datasets, including ImageNet scene parsing challenge 2016 [43], PASCAL VOC 2012 semantic segmentation [8] and urban scene understanding dataset Cityscapes [6]. 5.1. Implementation Details For a practical deep learning system, devil is always in the details. Our implementation is based on the public plat- form Caffe [15]. Inspired by [4], we use the “poly” learning rate policy where current learning rate equals to the base one multiplying (1 − iter maxiter)power. We set base learning rate to 0.01 and power to 0.9. The performance can be improved by increasing the iteration number, which is set to 150K for ImageNet experiment, 30K for PASCAL VOC and 90K for Cityscapes. Momentum and weight decay are set to 0.9 and 0.0001 respectively. For data augmentation, we adopt ran- dom mirror and random resize between 0.5 and 2 for all datasets, and additionally add random rotation between - 10 and 10 degrees, and random Gaussian blur for ImageNet and PASCAL VOC. This comprehensive data augmentation scheme makes the network resist overﬁtting. Our network contains dilated convolution following [4]. During the course of experiments, we notice that an ap- propriately large “cropsize” can yield good performance and “batchsize” in the batch normalization [14] layer is of great importance. Due to limited physical memory on GPU cards, we set the “batchsize” to 16 during training. To achieve this, we modify Caffe from [37] together with Method Mean IoU(%) Pixel Acc.(%) ResNet50-Baseline 37.23 78.01 ResNet50+B1+MAX 39.94 79.46 ResNet50+B1+AVE 40.07 79.52 ResNet50+B1236+MAX 40.18 79.45 ResNet50+B1236+AVE 41.07 79.97 ResNet50+B1236+MAX+DR 40.87 79.61 ResNet50+B1236+AVE+DR 41.68 80.04 Table 1. Investigation of PSPNet with different settings. Baseline is ResNet50-based FCN with dilated network. ‘B1’ and ‘B1236’ denote pooled feature maps of bin sizes {1 × 1} and {1 × 1, 2 × 2, 3 × 3, 6 × 6} respectively. ‘MAX’ and ‘AVE’ represent max pooling and average pooling operations individually. ‘DR’ means that dimension reduction is taken after pooling. The results are tested on the validation set with the single-scale input. branch [4] and make it support batch normalization on data gathered from multiple GPUs based on OpenMPI. For the auxiliary loss, we set the weight to 0.4 in experiments. 5.2. ImageNet Scene Parsing Challenge 2016 Dataset and Evaluation Metrics The ADE20K dataset [43] is used in ImageNet scene parsing challenge 2016. Dif- ferent from other datasets, ADE20K is more challenging for the up to 150 classes and diverse scenes with a total of 1,038 image-level labels. The challenge data is divided into 20K/2K/3K images for training, validation and testing. Also, it needs to parse both objects and stuff in the scene, which makes it more difﬁcult than other datasets. For eval- uation, both pixel-wise accuracy (Pixel Acc.) and mean of class-wise intersection over union (Mean IoU) are used. Ablation Study for PSPNet To evaluate PSPNet, we con- duct experiments with several settings, including pooling types of max and average, pooling with just one global fea- ture or four-level features, with and without dimension re- duction after the pooling operation and before concatena- tion. As listed in Table 1, average pooling works better than max pooling in all settings. Pooling with pyramid parsing outperforms that using global pooling. With dimension re- duction, the performance is further enhanced. With our pro- posed PSPNet, the best setting yields results 41.68/80.04 in terms of Mean IoU and Pixel Acc. (%), exceeding global average pooling of 40.07/79.52 as idea in Liu et al. [24] by 1.61/0.52. And compared to the baseline, PSPNet outper- forming it by 4.45/2.03 in terms of absolute improvement and 11.95/2.60 in terms of relative difference. Ablation Study for Auxiliary Loss The introduced aux- iliary loss helps optimize the learning process while not in- ﬂuencing learning in the master branch. We experiment with setting the auxiliary loss weight α between 0 and 1 and show the results in Table 2. The baseline uses ResNet50- based FCN with dilated network, with the master branch’s softmax loss for optimization. Adding the auxiliary loss Loss Weight α Mean IoU(%) Pixel Acc.(%) ResNet50 (without AL) 35.82 77.07 ResNet50 (with α = 0.3) 37.01 77.87 ResNet50 (with α = 0.4) 37.23 78.01 ResNet50 (with α = 0.6) 37.09 77.84 ResNet50 (with α = 0.9) 36.99 77.87 Table 2. Setting an appropriate loss weight α in the auxiliary branch is important. ‘AL’ denotes the auxiliary loss. Baseline is ResNet50-based FCN with dilated network. Empirically, α = 0.4 yields the best performance. The results are tested on the valida- tion set with the single-scale input. Figure 5. Performance grows with deeper networks. The results are obtained on the validation set with the single-scale input. Method Mean IoU(%) Pixel Acc.(%) PSPNet(50) 41.68 80.04 PSPNet(101) 41.96 80.64 PSPNet(152) 42.62 80.80 PSPNet(269) 43.81 80.88 PSPNet(50)+MS 42.78 80.76 PSPNet(101)+MS 43.29 81.39 PSPNet(152)+MS 43.51 81.38 PSPNet(269)+MS 44.94 81.69 Table 3. Deeper pre-trained model get higher performance. Num- ber in the brackets refers to the depth of ResNet and ‘MS’ denotes multi-scale testing. branch, α = 0.4 yields the best performance. It outperforms the baseline with an improvement of 1.41/0.94 in terms of Mean IoU and Pixel Acc. (%). We believe deeper networks will beneﬁt more given the new augmented auxiliary loss. Ablation Study for Pre-trained Model Deeper neural networks have been shown in previous work to be beneﬁcial to large scale data classiﬁcation. To further analyze PSPNet, we conduct experiments for different depths of pre-trained ResNet. We test four depths of {50, 101, 152, 269}. As shown in Fig. 5, with the same setting, increasing the depth of ResNet from 50 to 269 can improve the score of (Mean IoU + Pixel Acc.) / 2 (%) from 60.86 to 62.35, with 1.49 ab- solute improvement. Detailed scores of PSPNet pre-trained from different depth ResNet models are listed in Table 3. Method Mean IoU(%) Pixel Acc.(%) FCN [26] 29.39 71.32 SegNet [2] 21.64 71.00 DilatedNet [40] 32.31 73.55 CascadeNet [43] 34.90 74.52 ResNet50-Baseline 34.28 76.35 ResNet50+DA 35.82 77.07 ResNet50+DA+AL 37.23 78.01 ResNet50+DA+AL+PSP 41.68 80.04 ResNet269+DA+AL+PSP 43.81 80.88 ResNet269+DA+AL+PSP+MS 44.94 81.69 Table 4. Detailed analysis of our proposed PSPNet with compar- ison with others. Our results are obtained on the validation set with the single-scale input except for the last row. Results of FCN, SegNet and DilatedNet are reported in [43]. ‘DA’ refers to data augmentation we performed, ‘AL’ denotes the auxiliary loss we added and ‘PSP’ represents the proposed PSPNet. ‘MS’ means that multi-scale testing is used. Rank Team Name Final Score (%) 1 Ours 57.21 2 Adelaide 56.74 3 360+MCG-ICT-CAS SP 55.56 - (our single model) (55.38) 4 SegModel 54.65 5 CASIA IVA 54.33 - DilatedNet [40] 45.67 - FCN [26] 44.80 - SegNet [2] 40.79 Table 5. Results of ImageNet scene parsing challenge 2016. The best entry of each team is listed. The ﬁnal score is the mean of Mean IoU and Pixel Acc. Results are evaluated on the testing set. More Detailed Performance Analysis We show our more detailed analysis on the validation set of ADE20K in Table 4. All our results except the last-row one use single- scale test. “ResNet269+DA+AL+PSP+MS” uses multi- scale testing. Our baseline is adapted from ResNet50 with dilated network, which yields MeanIoU 34.28 and Pixel Acc. 76.35. It already outperforms other prior systems pos- sibly due to the powerful ResNet [13]. Our proposed architecture makes further improvement compared to the baseline. Using data augmentation, our result exceeds the baseline by 1.54/0.72 and reaches 35.82/77.07. Using the auxiliary loss can further improve it by 1.41/0.94 and reaches 37.23/78.01. With PSPNet, we notice relatively more signiﬁcant progress for improvement of 4.45/2.03. The result reaches 41.68/80.04. The differ- ence from the baseline result is 7.40/3.69 in terms of abso- lute improvement and 21.59/4.83 (%) in terms of relativity. A deeper network of ResNet269 yields even higher perfor- mance up to 43.81/80.88. Finally, the multi-scale testing scheme moves the scores to 44.94/81.69."
            },
            {
                "section": "Results",
                "content": "in Challenge Using the proposed architecture, our team came in the 1st place in ImageNet scene parsing Figure 6. Visual improvements on ADE20K, PSPNet produces more accurate and detailed results. challenge 2016. Table 5 shows a few results in this com- petition. Our ensemble submission achieves score 57.21% on the testing set. Our single-model yields score 55.38%, which is even higher than a few other multi-model ensem- ble submissions. This score is lower than that on the valida- tion set possibly due to the difference of data distributions between validation and testing sets. As shown in column (d) of Fig. 2, PSPNet solves the common problems in FCN. Fig. 6 shows another few parsing results on validation set of ADE20K. Our results contain more accurate and detailed structures compared to the baseline. 5.3. PASCAL VOC 2012 Our PSPNet also works satisfyingly on semantic seg- mentation. We carry out experiments on the PASCAL VOC 2012 segmentation dataset [8], which contains 20 object categories and one background class. Following the proce- dure of [26, 7, 31, 3], we use augmented data with the anno- tation of [10] resulting 10,582, 1,449 and 1,456 images for training, validation and testing. Results are shown in Ta- ble 6, we compare PSPNet with previous best-performing"
            },
            {
                "section": "methods",
                "content": "on the testing set based on two settings, i.e., with or without pre-training on MS-COCO dataset [21]. Meth- ods pre-trained with MS-COCO are marked by ‘†’. For fair comparison with current ResNet based frameworks [38, 9, 4] in scene parsing/semantic segmentation task, we build our architecture based on ResNet101 while without post- processing like CRF. We evaluate PSPNet with several- scale input and use the average results following [3, 24]. Figure 7. Visual improvements on PASCAL VOC 2012 data. PSP- Net produces more accurate and detailed results. As shown in Table 6, PSPNet outperforms prior meth- ods on both settings. Trained with only VOC 2012 data, we achieve 82.6% accuracy2 – we get the highest accuracy on all 20 classes. When PSPNet is pre-trained with MS-COCO dataset, it reaches 85.4% accuracy3 where 19 out of the 20 classes receive the highest accuracy. Intriguingly, our PSP- Net trained with only VOC 2012 data outperforms existing"
            },
            {
                "section": "methods",
                "content": "trained with the MS-COCO pre-trained model. One may argue that our based classiﬁcation model is more powerful than several prior methods since ResNet was recently proposed. To exhibit our unique contribu- tion, we show that our method also outperforms state- of-the-art frameworks that use the same model, including FCRNs [38], LRR [9], and DeepLab [4]. In this process, we even do not employ time-consuming but effective post- processing, such as CRF, as that in [4, 9]. Several examples are shown in Fig. 7. For “cows” in row one, our baseline model treats it as “horse” and “dog” while PSPNet corrects these errors. For “aeroplane” and “table” in the second and third rows, PSPNet ﬁnds missing parts. For “person”, “bottle” and “plant” in following rows, PSP- Net performs well on these small-size-object classes in the images compared to the baseline model. More visual com- parisons between PSPNet and other methods are included in Fig. 9. 5.4. Cityscapes Cityscapes [6] is a recently released dataset for semantic urban scene understanding. It contains 5,000 high quality pixel-level ﬁnely annotated images collected from 50 cities 2http://host.robots.ox.ac.uk:8080/anonymous/0OOWLP.html 3http://host.robots.ox.ac.uk:8080/anonymous/6KIR41.html Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU FCN [26] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 Zoom-out [28] 85.6 37.3 83.2 62.5 66.0 85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2 81.1 77.1 53.6 74.0 49.2 71.7 63.3 69.6 DeepLab [3] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 CRF-RNN [41] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [30] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 GCRF [36] 85.2 43.9 83.3 65.2 68.3 89.0 82.7 85.3 31.1 79.5 63.3 80.5 79.3 85.5 81.0 60.5 85.5 52.0 77.3 65.1 73.2 DPN [25] 87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1 Piecewise [20] 90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3 PSPNet 91.8 71.9 94.7 71.2 75.8 95.2 89.9 95.9 39.3 90.7 71.7 90.5 94.5 88.8 89.6 72.8 89.6 64.0 85.1 76.3 82.6 CRF-RNN† [41] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 BoxSup† [7] 89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7 75.2 Dilation8† [40] 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84.0 63.0 83.3 89.0 83.8 85.1 56.8 87.6 56.0 80.2 64.7 75.3 DPN† [25] 89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5 Piecewise† [20] 94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0 85.8 86.0 67.5 90.2 63.8 80.9 73.0 78.0 FCRNs† [38] 91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1 LRR† [9] 92.4 45.1 94.6 65.2 75.8 95.1 89.1 92.3 39.0 85.7 70.4 88.6 89.4 88.6 86.6 65.8 86.2 57.4 85.7 77.3 79.3 DeepLab† [4] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7 PSPNet† 95.8 72.7 95.0 78.9 84.4 94.7 92.0 95.7 43.1 91.0 80.3 91.3 96.3 92.3 90.1 71.5 94.4 66.9 88.8 82.0 85.4 Table 6. Per-class results on PASCAL VOC 2012 testing set. Methods pre-trained on MS-COCO are marked with ‘†’. Method IoU cla. iIoU cla. IoU cat. iIoU cat. CRF-RNN [41] 62.5 34.4 82.7 66.0 FCN [26] 65.3 41.7 85.7 70.1 SiCNN [16] 66.3 44.9 85.0 71.2 DPN [25] 66.8 39.1 86.0 69.1 Dilation10 [40] 67.1 42.0 86.5 71.1 LRR [9] 69.7 48.0 88.2 74.7 DeepLab [4] 70.4 42.6 86.4 67.7 Piecewise [20] 71.6 51.7 87.3 74.1 PSPNet 78.4 56.7 90.6 78.6 LRR‡ [9] 71.8 47.9 88.4 73.9 PSPNet‡ 80.2 58.1 90.6 78.2 Table 7. Results on Cityscapes testing set. Methods trained using both ﬁne and coarse data are marked with ‘‡’. in different seasons. The images are divided into sets with numbers 2,975, 500, and 1,525 for training, validation and testing. It deﬁnes 19 categories containing both stuff and objects. Also, 20,000 coarsely annotated images are pro- vided for two settings in comparison, i.e., training with only ﬁne data or with both the ﬁne and coarse data. Methods trained using both ﬁne and coarse data are marked with ‘‡’. Detailed results are listed in Table 7. Our base model is ResNet101 as in DeepLab [4] for fair comparison and the testing procedure follows Section 5.3. Statistics in Table 7 show that PSPNet outperforms other"
            },
            {
                "section": "methods",
                "content": "with notable advantage. Using both ﬁne and coarse data for training makes our method yield 80.2 accuracy. Several examples are shown in Fig. 8. Detailed per-class"
            },
            {
                "section": "results",
                "content": "on testing set are shown in Table 8. 6. Concluding Remarks We have proposed an effective pyramid scene parsing network for complex scene understanding. The global pyra- Figure 8. Examples of PSPNet results on Cityscapes dataset. mid pooling feature provides additional contextual informa- tion. We have also provided a deeply supervised optimiza- tion strategy for ResNet-based FCN network. We hope the implementation details publicly available can help the com- munity adopt these useful strategies for scene parsing and semantic segmentation and advance related techniques. Acknowledgements We would like to thank Gang Sun and Tong Xiao for their help in training the basic classiﬁcation models, Qun Luo for technical support. This work is supported by a grant from the Research Grants Council of the Hong Kong SAR (project No. 2150760). Figure 9. Visual comparison on PASCAL VOC 2012 data. (a) Image. (b) Ground Truth. (c) FCN [26]. (d) DPN [24]. (e) DeepLab [4]. (f) PSPNet. Method road swalk build. wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU CRF-RNN [41] 96.3 73.9 88.2 47.6 41.3 35.2 49.5 59.7 90.6 66.1 93.5 70.4 34.7 90.1 39.2 57.5 55.4 43.9 54.6 62.5 FCN [26] 97.4 78.4 89.2 34.9 44.2 47.4 60.1 65.0 91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 65.3 SiCNN+CRF [16] 96.3 76.8 88.8 40.0 45.4 50.1 63.3 69.6 90.6 67.1 92.2 77.6 55.9 90.1 39.2 51.3 44.4 54.4 66.1 66.3 DPN [25] 97.5 78.5 89.5 40.4 45.9 51.1 56.8 65.3 91.5 69.4 94.5 77.5 54.2 92.5 44.5 53.4 49.9 52.1 64.8 66.8 Dilation10 [40] 97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55.0 93.3 45.5 53.4 47.7 52.2 66.0 67.1 LRR [9] 97.7 79.9 90.7 44.4 48.6 58.6 68.2 72.0 92.5 69.3 94.7 81.6 60.0 94.0 43.6 56.8 47.2 54.8 69.7 69.7 DeepLab [4] 97.9 81.3 90.3 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8 70.4 Piecewise [20] 98.0 82.6 90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 71.6 PSPNet 98.6 86.2 92.9 50.8 58.8 64.0 75.6 79.0 93.4 72.3 95.4 86.5 71.3 95.9 68.2 79.5 73.8 69.5 77.2 78.4 LRR‡ [9] 97.9 81.5 91.4 50.5 52.7 59.4 66.8 72.7 92.5 70.1 95.0 81.3 60.1 94.3 51.2 67.7 54.6 55.6 69.6 71.8 PSPNet‡ 98.6 86.6 93.2 58.1 63.0 64.5 75.2 79.2 93.4 72.1 95.1 86.3 71.4 96.0 73.5 90.4 80.3 69.9 76.9 80.2 Table 8. Per-class results on Cityscapes testing set. Methods trained using both ﬁne and coarse set are marked with ‘‡’."
            }
        ]
    },
    "Paper_3": {
        "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection Antoine Simoulin*, Namyong Park*, Xiaoyi Liu, Grey Yang",
        "sections": [
            {
                "section": "Abstract",
                "content": "Fine-tuning provides an effective means to spe- cialize pre-trained models for various down- stream tasks. However, fine-tuning often in- curs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate ac- tivations computed in the forward pass to up- date weights during the backward pass. In this work, we develop TOKENTUNE, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine- tuning of transformer-based models. During the backward pass, TOKENTUNE approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TOKENTUNE, only a subset of intermedi- ate activations are cached during the forward pass. Also, TOKENTUNE can be easily com- bined with existing methods like LoRA, fur- ther reducing the memory cost. We evaluate our approach on pre-trained transformer mod- els with up to billions of parameters, consider- ing the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Over- all, TOKENTUNE achieves performance on par with full fine-tuning or representative memory- efficient fine-tuning methods, while greatly re- ducing the memory footprint, especially when combined with other methods with comple- mentary memory reduction mechanisms. We hope that our approach will facilitate the fine- tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger sys- tem. Our code is available at https://github. com/facebookresearch/tokentune. 1"
            },
            {
                "section": "Introduction",
                "content": "Fine-tuning is an effective method for specializ- ing large pre-trained models, either by using direct * Equal contribution −𝟔𝟑% −𝟏% −𝟑𝟕% −𝟕𝟗% −𝟐𝟗% Figure 1: TOKENTUNE greatly reduces the GPU mem- ory usage for fine-tuning the Llama2-7B model (e.g., using only 37% of the memory QLoRA (Dettmers et al., 2023) requires), while achieving similar accuracy to representative memory-efficient fine-tuning methods. Accuracy and memory usage numbers are listed in Ta- ble 2 and Fig. 4. See Sec. 5 for details on experiments. supervision from the training set of a given task (Howard and Ruder, 2018; Devlin et al., 2019; Raf- fel et al., 2020), from curated instruction datasets (Mishra et al., 2022; Wei et al., 2022; Taori et al., 2023), or from human feedback via reinforcement learning (Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023). However, fine-tuning is not necessarily an efficient method, especially for transformer-based large language models (LLMs), since their large number of parameters leads to large compute and memory requirements. For instance, fine-tuning GPT-3 175B (Brown et al., 2020) or LLama 65B (Touvron et al., 2023) typi- cally requires 1,200 GB and 780 GB of GPU mem- ory, as reported in Hu et al. (2022) and Dettmers et al. (2023), respectively. GPU memory usage during fine-tuning can be broken down into three parts: storing (1) the model parameters, (2) the parameter gradients and opti- mizer states, and (3) the intermediate activations. Parameter-Efficient Fine-Tuning (PEFT) (Houlsby et al., 2019; Hu et al., 2022) aims at updating a small number of parameters, e.g., by optimiz- ing a subset of the backbone model’s parameters arXiv:2501.18824v1  [cs.CL]  31 Jan 2025 while freezing others, which reduces the mem- ory requirements to store the parameters’ gradi- ents and optimizer states. Alternatively, quanti- zation techniques (Dettmers et al., 2022, 2023; Liu et al., 2024) use low precision data types for model parameters, which reduces the memory cost. For example, in fine-tuning the Llama2-7B model, LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023), which are representative PEFT and quantization-based methods, reduce the memory needed for full fine-tuning by 12% and 43%, re- spectively (Figure 1). However, such existing ap- proaches still require caching all of the intermediate activations computed in the forward pass to obtain the gradients during the backward pass. In this work, we propose a method for memory- efficient fine-tuning, named TOKENTUNE, which aims to significantly reduce the GPU memory dedi- cated to storing intermediate activations during the forward pass without sacrificing the model perfor- mance on various downstream tasks. To this end, TOKENTUNE selects a subset of the input tokens in the context, and fine-tunes the model with respect to those selected tokens. More specifically, during the backward pass, TOKENTUNE approximates the gradient computation by backpropagating through the selected tokens, and thus only a subset of the in- termediate activations need to be cached during the forward pass, thereby reducing the memory cost. We demonstrate the effectiveness of TOKEN- TUNE using both medium- and large-size language models, namely, BERT (Devlin et al., 2019) and Llama (Touvron et al., 2023), which have hundreds of millions, and billions of parameters, respectively. Overall, our results show that fine-tuning with TO- KENTUNE leads to downstream task performance on par with that of full fine-tuning or representative"
            },
            {
                "section": "methods",
                "content": "for memory-efficient fine-tuning, while drastically reducing the memory footprint. Notably, TOKENTUNE can be effectively combined with ex- isting methods, achieving a greater reduction in memory usage. For instance, by combining TO- KENTUNE with QLoRA (Dettmers et al., 2023), we can fine-tune Llama2-7B using just about one third of the memory QLoRA alone requires as Figure 1 shows. To sum, our contributions are as follows. • Novelty. TOKENTUNE, to the best of our knowl- edge, is the first method that reduces GPU mem- ory usage for fine-tuning via token selection1. 1A preliminary version of this work was presented at a non-archival workshop (Simoulin et al., 2023). • Combinability. TOKENTUNE can be combined with existing memory-efficient fine-tuning meth- ods, leading to further memory reduction. • Effectiveness. We perform extensive experi- ments, showing that TOKENTUNE achieves sim- ilar accuracy to representative memory-efficient"
            },
            {
                "section": "methods",
                "content": ", while greatly reducing the memory footprint during fine-tuning, e.g., using only 21% of what full fine-tuning requires (Figure 1). 2 Related Work 2.1 Parameter-Efficient Fine-Tuning (PEFT) PEFT methods, which aim to limit the computing resources for fine-tuning LLMs, can be divided into four categories (Han et al., 2024; Xu et al., 2023). Selective PEFT methods update only a subset of the backbone model parameters using weight masking strategies, such as learnable binary mask- ing (Guo et al., 2021) and parameter importance estimation using Fisher information (Sung et al., 2021; Das et al., 2023). Other selective PEFT meth- ods focus on updating specific modules, e.g., the cross-attention layers (Gheini et al., 2021) and the bias terms (Zaken et al., 2022; Lawton et al., 2023). Additive PEFT methods add a few parameters to the frozen pre-trained model, and fine-tune only the added parameters. E.g., adapters inject small layers within the transformer block, either sequentially after its sublayers (Houlsby et al., 2019; Pfeiffer et al., 2021), or as a side network running in parallel to the sublayers (He et al., 2022a; Zhu et al., 2021). Alternatively, soft prompt-based approaches (Li and Liang, 2021; Qin and Eisner, 2021; Liu et al., 2022) prepend continuous learnable vectors to the input of a frozen model and tune them for each task. Reparameterized PEFT methods perform low- rank transformation, utilizing the low intrinsic dimension of LLMs (Aghajanyan et al., 2021). LoRA (Hu et al., 2022) is the most representative approach, where an update to the model weights is captured via its low-rank decomposition. Several studies followed to improve LoRA, e.g., to sup- port dynamic rank selection (Valipour et al., 2023; Zhang et al., 2023b), and to address overfitting (Lin et al., 2024) and overconfidence (Yang et al., 2024). Hybrid PEFT methods aim to combine different PEFT approaches, e.g., adapters, prefix-tuning, and LoRA. The design space of combinations of PEFT Figure 2: TOKENTUNE achieves memory-efficient fine-tuning of transformers via token selection. During the backward pass, we compute the gradient for only a subset of k input tokens, while the others are frozen (in gray in the figure). During the forward pass, all input positions are used, but only a subset of the activations is cached in memory (in blue in the figure). TOKENTUNE is applicable to various transformer-based models, as well as different language modeling tasks, as our experiments with BERT (Devlin et al., 2019) and Llama (Touvron et al., 2023) show."
            },
            {
                "section": "methods",
                "content": "has been explored either manually (He et al., 2022a; Mao et al., 2022), or automatically, e.g., by leveraging neural architecture search meth- ods (Zhang et al., 2022b; Zhou et al., 2024). While the above PEFT methods effectively improve parameter efficiency, they may still incur signifi- cant memory overhead during fine-tuning (Sung et al., 2022; Jin et al., 2023). The proposed TOKEN- TUNE can be combined with these PEFT methods, enabling them to achieve both parameter and mem- ory efficiency, as Sections 4 and 5 show. 2.2 Memory-Efficient Fine-Tuning There exist several techniques that can be used to improve the memory efficiency in fine-tuning LLMs, which we organize into four groups. Memory-Efficient PEFT. Some PEFT methods aim to achieve memory and parameter efficiency simultaneously. Side tuning methods (Zhang et al., 2020; Sung et al., 2022) introduce small learnable side networks separated from the backbone model, and channel backpropagation only through the side networks, thereby reducing the memory require- ments for gradients and intermediate activations. By utilizing the reversible model, MEFT (Liao et al., 2023) avoids the need to cache intermediate activations in the forward pass. LoRA-FA (Zhang et al., 2023a) improves LoRA by addressing its high memory usage for input activations via freez- ing LoRA’s down-projection weights. Gradient Checkpointing (Chen et al., 2016; Gruslys et al., 2016) reduces the memory require- ment for model training by storing only a subset of intermediate activations in the forward pass, and recomputing the others during the backward pass. Quantization is a compression technique that re- duces the number of bits for storing numerical val- ues. With quantization, parameters are represented with lower-precision data types (Dettmers et al., 2022, 2023; Liu et al., 2024), leading to memory reduction in both fine-tuning and inference. Approximate Gradient Methods reduce the mem- ory usage by avoiding the exact gradient compu- tation involved with full fine-tuning, and instead using an approximate estimate of the gradient for weight updates. To this end, a few methods employ low-rank factorization, where they reduce mem- ory cost by utilizing the low-rank structure of the gradients (Zhao et al., 2024) or the second-order statistics (Shazeer and Stern, 2018). Alternatively, MeZO (Malladi et al., 2023) approximates the gra- dient using only forward passes, building upon the zeroth-order optimization technique (Spall, 1992). The proposed TOKENTUNE can be considered an approximate gradient method, as its token-selective fine-tuning strategy leads to an approximation of the full gradient, which is a completely new di- rection investigated to improve memory efficiency in fine-tuning. Also, being complementary to prior methods, TOKENTUNE can be combined with them, resulting in further memory reduction. 3 TOKENTUNE Previous studies analyzing the structure of the spar- sity of activations and gradients (Kurtz et al., 2020; Liu et al., 2023; Dai et al., 2022) suggest that some neurons and activations could have a pre- dominant importance, while some others may have smaller contributions to the loss and output com- putation. Inspired by these works, we hypothesize that for many downstream tasks, not all tokens in the sequence would need to be involved in the fine- tuning—more specifically, backpropagation—of transformer models. Instead, we conjecture that, when restricted to backpropagating through a sub- set of tokens, transformers could be further opti- mized for the downstream task by enabling the additional learning and adjustments, which need to happen during the fine-tuning for the given task, to be done in a more compact way, i.e., by incorporat- ing the additional knowledge more succinctly with respect to the selected subset of tokens. Figure 2 illustrates TOKENTUNE, aiming at re- ducing the memory needed to store the intermediate activations used for gradient computation. Given an input sequence X, a transformer associates each token from the input sequence to an embedding and computes a corresponding sequence of hid- den states h through multiple layer applications. For each input sequence, we select k random po- sitions.2 We organize each layer’s input in two groups, one with the k selected input positions, hG, and the other with the remaining un-selected posi- tions, h ¯G, such that h = [hG, h ¯G], with [ ] denot- ing the concatenation operator and G = k. The re-ordering does not impact the computation as the position is directly encoded in the hidden states. With this token selection scheme, the classification objective LCLS and the language modeling objec- tive LLM used by TOKENTUNE are as follows. Classification Task. The goal is to assign the right class or label y for the given sequence. Given the hidden states from the transformer layers, we use the average of the hidden states from the k selected positions of the last layer as input for an 2We select the positions using a uniform distribution. How- ever, we always include the [CLS] token—a special symbol prepended as the beginning of every input sentence. MLP, which outputs a probability distribution over the classes of the task, as given by Eq. 1. During the evaluation, we use the average from all hidden states of the last layer as input for the MLP. π = MLP 1 k X i∈G hi ! p(y|X) = softmax(π) LCLS = −log p(y|X) (1) Language Modeling Task. The goal is to learn the probability distribution of a token, given all preceding tokens. We train the language model by applying the traditional cross-entropy loss to the set of k randomly selected positions as given by Eq. 2 below, with Wlm denoting the head projecting the hidden state back into the vocabulary dimension. p(xi|x<i) = softmax(hiWlm) LLM = − X i∈G log P(xi|x<i) (2) The key element of our method is that we disable the gradient computation for the un-selected to- kens in ¯G. Thus, only the k selected tokens in G contribute to the gradient computation during the backward pass. We detail the method in the case of dense layers and attention mechanism in Sec- tion 3.1 and Section 3.2, respectively. 3.1 TOKENTUNE for Dense and Normalization Layers We consider a dense layer a = σ(z) = σ(hW + b) with weight W, bias b, nonlinear function σ, input h, pre-activation z, and output a. Eq. 3 computes the gradient with respect to W and b when back- propagating a loss L through the layer: ∂L dW = ∂L ∂a ∂a ∂z ∂z ∂W = ∂L ∂a σ′h ∂L db = ∂L ∂a ∂a ∂z ∂z ∂b = ∂L ∂a σ′ (3) If we backpropagate the error only through the selected tokens in G, and disable the gradient com- putation for the unselected positions in ¯G, we have: ∂L ∂a = \u0014 ∂L ∂aG , ∂L ∂a ¯G \u0015 = \u0014 ∂L ∂aG , 0 \u0015 (4) Plugging that into Eq. 3, we have: ∂L dW = \u0014 ∂L ∂aG σ′hG, 0 \u0015 ; ∂L db = \u0014 ∂L ∂aG σ′, 0 \u0015 (5) Given Eq. 5, we only need to cache hG for applying the chain rule, instead of the full activation h. Regarding implementation, we use Algorithm 1 which explicitly splits the hidden states into two groups where hG corresponds to the tokens selected to be fine-tuned and h ¯G corresponds to the un- selected tokens. As shown in Eq. 6 and Eq. 7, the forward pass is identical to standard fine-tuning except that we disable the gradient computation for the positions for h ¯G in Eq. 7 with the context \"torch.no_grad()\" in PyTorch. hG = hGW + b (6) h ¯G = h ¯GW + b (7) where W denotes the weights W1 and W2 for the feed-forward layers. We apply the same methodol- ogy for normalization layers. 3.2 TOKENTUNE for Attention Layers For attention layers, we compute the attention as: [QG, KG, VG] = hGW[Q,K,V ] + b[Q,K,V ] (8) \u0002 Q ¯G, K ¯G, V ¯G \u0003 = h ¯GW[Q,K,V ] + b[Q,K,V ] (9) hG = softmax \u0010 QG[K ¯ G,KG] ⊤/ √ d \u0011 \u0002 V ¯G, VG \u0003 (10) h ¯G = softmax \u0010 Q ¯ G[K ¯ G,KG] ⊤/ √ d \u0011 \u0002 V ¯G, VG \u0003 (11) where W[Q,K,V ] ∈Rd×3d denotes the concatenated weights for the queries, keys, and values. For the computation of un-selected positions in Eq. 9 and Eq. 11, we again disable the gradient computation in PyTorch. Algorithm 1 illustrates the steps for the forward pass of a transformer model with the proposed TOKENTUNE algorithm described in Sec- tions 3.1 and 3.2. 4 Application to Medium-Size Encoders Alternative methods such as zero-shot learning or prompting usually underperform fine-tuning (Brown et al., 2020). Thus, in many cases, fine- tuning medium size language models may offer a better balance in terms of cost and performance, compared with fine-tuning large language models (LLMs) or conditioning their outputs with prompt approaches (Li et al., 2022; Schick and Schütze, 2021). Medium-size models may also be used as individual components, co-trained to encode infor- mation for a larger system (Pfeiffer et al., 2023). Finally, as detailed in Appendix E, the distribu- tion of the GPU memory usage may be very differ- ent given the order of magnitude of the fine-tuned Algorithm 1: TOKENTUNE (We omit layer normalization, skip connections, non-linear functions, and multi-head attention for sim- plicity) Input: input sequence X Output: hG, h ¯ G 1 Compute input token embeddings h 2 Re-organize input tokens into two groups (hG and h ¯ G) 3 for layer in transformers’ layers do // Compute the attention layer 4 [QG, KG, VG] = hGW[Q,K,V ] + b[Q,K,V ] 5 hG = softmax \u0012 QG[K ¯ G,KG] ⊤ √ d \u0013 [V ¯ G, VG] 6 with torch.no_grad(): 7 [Q ¯ G, K ¯ G, V ¯ G] = h ¯ GW[Q,K,V ] + b[Q,K,V ] 8 h ¯ G = softmax \u0012 Q ¯ G[K ¯ G,KG] ⊤ √ d \u0013 [V ¯ G, VG] // Compute the feed-forward layer 9 hG = hGW1 + b1 10 hG = hGW2 + b2 11 with torch.no_grad(): 12 h ¯ G = h ¯ GW1 + b1 13 h ¯ G = h ¯ GW2 + b2 14 Re-organize input tokens into the original order model’s number of parameters. For large-size mod- els, the majority of the memory is often dedicated to storing parameters and optimizer states, thus maximizing the relevance of PEFT approaches. For medium-size language models, fine-tuned with large batch sizes, the majority of the memory may be dedicated to storing the intermediate activation, thus maximizing the impact of TOKENTUNE. 4.1 Downstream Task Performance We first validate the relevance of our method on the GLUE benchmark (Wang et al., 2018). We use a similar hyper-parameter search space as in (Zaken et al., 2022), by performing a cross val- idation on the dev set using a learning rate in [5e−5, 3e−5, 2e−5, 1e−5]. We set the batch size to 16 and perform 3 epochs on large datasets and 20 epochs on small ones (MRPC, STS-B, CoLA). We use BERT-large (Devlin et al., 2019) and either fine-tune the model fully, or use TOKENTUNE and propagate the gradient through 16 input positions. We then evaluate our model on the test set and report the results in Table 1. As shown in the second part of Table 1, the av- erage GLUE score of TOKENTUNE is comparable to that of full fine-tuning, thus empirically validat- Table 1: Results from BERT-large (Devlin et al., 2019) on GLUE test tasks scored using the benchmark server. We report the Matthew’s Correlation for CoLA, the Spearman correlation for STS-B, F1 score for MRPC and QQP. We report the accuracy on the MNLI matched test split and the accuracy for every other tasks. The “Param.” column indicates the ratio of the number of updated parameters for each task by the number of parameters in the backbone model. We indicate in bold the best result for each task. † indicates models we trained. We report adapter results from (Houlsby et al., 2019), BitFit from (Zaken et al., 2022) and Diff Pruning from (Guo et al., 2021). For LoRA (Hu et al., 2022) and Ladder Side Tuning (LST) (Sung et al., 2022), we select the best learning rate in the dev set between the values proposed in the original papers, [5e−4, 4e−4, 3e−4, 2e−4] and [3e−4, 1e−3, 3e−3], respectively. We do not use the initialization setup proposed in LoRA or LST nor do we drop any layers for the LST method. Method Param. (%) CoLA SST-2 MRPC QQP QNLI MNLI STS-B Avg. ↑ Avg. # Tokens — 11.3 13.3 53.2 30.6 49.4 39.8 27.8 32.2 Full Fine-Tuning† 100.0 60.7 94.6 88.3 72.0 92.4 85.8 85.8 82.8 Adapters 3.6 59.5 94.0 89.5 71.8 90.7 84.9 86.9 82.5 BitFit 0.1 59.7 94.2 88.9 70.5 92.0 84.5 85.0 82.1 Diff Pruning 0.5 61.1 94.1 89.7 71.1 93.3 86.4 86.0 83.1 Ladder Side Tuning† 2.4 56.4 93.4 88.0 66.9 89.1 82.9 86.6 80.5 LoRA† 0.3 58.5 94.0 89.2 71.1 91.1 84.7 84.6 81.9 TOKENTUNE † 100.0 59.6 93.9 88.0 70.8 91.0 85.4 86.0 82.1 ing the effectiveness of our approach.Table 1 also shows that TOKENTUNE either outperforms or per- forms similarly to existing SOTA approaches. Pre- cisely speaking, the performance of these memory- efficient fine-tuning methods, including TOKEN- TUNE, is often slightly worse than that of full fine- tuning. In comparison to full fine-tuning, some amount of performance loss with these methods is expected as they approximate or simplify the optimization process of full fine-tuning to reduce memory footprint. We hypothesize that some tasks, such as QQP and QNLI, are more difficult, or sensi- tive to overfitting than others, given that updating a small proportion of model parameters or using only a subset of input tokens for gradient computation achieves suboptimal performances on those tasks in most cases. The former case would require the development of sophisticated techniques to more effectively select a subset of parameters or input to- kens to optimize, while the latter case may benefit from the use of regularization techniques for neural networks, including Gouk et al. (2021); Foret et al. (2021); Li and Zhang (2021), the investigation of which we leave for future studies. 4.2 Ratio of Tuned Input Positions Given our token-selective fine-tuning approach, we then evaluate the impact of the number of frozen input positions on the performance. We use our selective procedure to fine-tune BERT-base on two tasks from the GLUE benchmark: MRPC and STS- B. We set the hyper-parameters as follows: 5e−5 for the learning rate, 32 for the batch size and 4 epochs. We use different values for k (i.e., the num- ber of trained input positions), ranging between 4 and 64. We report in Figure 3 (right), the average performance on the dev set of the tasks.3 As seen in Figure 3, the performance increases from 84.8 to 88.8 as the number of trained posi- tions increases from 4 to 64. However, by only tuning 32 positions, we already reach an average performance of 88.4, close to the 88.8 obtained by training 64 input positions. Our method surpasses the performance of freezing some bottom layers, as shown in (Lee et al., 2019), where only tuning the four bottom layers resulted in a 10% decrease in performance on the GLUE benchmark. 4.3 GPU Memory Impact Finally, we analyze the GPU memory required to fine-tune models using various approaches. We train our BERT-base model for 100 steps on the CoLA task using various batch sizes and report the peak GPU memory used. We compare with two other PEFT fine-tuning approaches close to ours: Ladder Side Tuning (Sung et al., 2022) and LoRA (Hu et al., 2022). LoRA freezes most of the model 3We provide some descriptive statistics in Appendix F to better understand how the absolute number of frozen input positions relates with the relative number of frozen input posi- tions. The statistics include distribution of the sentence length for the two subtasks (MRPC and STS-B) used to produce Figure 3 (right). Figure 3: (left) We plot the GPU memory required to train BERT-base on the CoLA task given varying batch sizes. We compare our approach with two PEFT approaches: Ladder Side Tuning (LST) and LoRA. (right) We plot the mean and standard deviation performance on the dev set of five runs when training BERT-base on two tasks from the GLUE benchmark: MRPC and STS-B. We use our memory efficient fine-tuning approach with a different number of selected input tokens for the gradient computation. parameters, while only training additional low-rank matrices, whose weights are added to the backbone network. Ladder Side Tuning (LST) freezes the model parameters but trains a side-network with smaller dimensions, taking as input intermediate activations from the backbone model. Figure 3 shows the evolution of the required GPU memory with respect to the batch size. GPU memory increases with the batch size for every approach. TOKENTUNE is more memory efficient by a large margin. When using a batch size of 512, it requires two times less memory than full fine- tuning: 23, 196 MiB needed for full fine-tuning is reduced to 9, 952 MiB with our method. All methods minimize GPU memory usage. LoRA and LST reduce the memory required to store optimizer states and parameter gradients, while our method reduces the memory for storing intermediate activations. Interestingly enough, it is possible to use these approaches in conjunction to reduce the memory for all three contributions. Fig. 3 shows that we can further reduce the mem- ory by combining TOKENTUNE with LoRA, thus requiring only 7, 682 MiB with a batch size of 512, a third of the memory used for full fine-tuning. 5 Application to Large-Size Decoders We also seek to evaluate our method on larger size pre-trained language models (LLMs). 5.1 Instruction Tuning and Few-Shot Evaluation LLMs are typically further fine-tuned on curated datasets to tailor them to specific domains and en- hance their capacity to follow instructions (Wang et al., 2023; Taori et al., 2023; Mukherjee et al., 2023). In this section, we employ instruction tun- ing on these datasets to fine-tune the LLMs and then assess the performance of the resulting mod- els using few-shot benchmarks. Instruction Tuning. We fine-tune the Llama2-7B model (Touvron et al., 2023) via instruction tuning with the Open-Platypus4 (Lee et al., 2023) dataset. Note that, while Open-Platypus consists of 11 open- source datasets, we exclude two of them5 that in- clude outputs from GPT (OpenAI, 2023), and in- stead use the other nine datasets for fine-tuning. Hyper-Parameter Settings. We conduct all exper- iments in this section on Nvidia H100 GPU. Fol- lowing Lee et al. (2023), we fine-tune the model for one epoch, and use a learning rate of 4e−4 for LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023), and 4e−5 otherwise. We use a batch size of 1 with 32 gradient accumulation steps. We apply the adapters on the feed-forward modules from each layer, following the method described in He et al. (2022b). We prompt the model without 4https://huggingface.co/datasets/garage-bAInd/ Open-Platypus 5leetcode-solutions-python-testgen-gpt4 and airoboros-gpt4-1.4.1 Table 2: Few-shot evaluation on question-answering benchmarks including: AI2 Reasoning Challenge (25-shot) (Clark et al., 2018), MMLU (5-shot) (Hendrycks et al., 2021), HellaSwag (10-shot) (Zellers et al., 2019), TruthfulQA (0-shot) (Lin et al., 2022), and WinoGrande (0-shot) (Sakaguchi et al., 2020). We use the evaluation scripts and prompt formatting from the \"Language Model Evaluation Harness\" (Gao et al., 2021). We report the average accuracy on five MMLU ethics tasks and WinoGrande, the normed accuracy on ARC and HellaSwag, and the MC2 score on TruthfulQA. We indicate in bold the best result for each task. We report the results with the raw Llama2-7B model (Touvron et al., 2023) and the Llama2-7B fine-tuned on the Platypus curated instruction dataset (Lee et al., 2023) using LoRA (Hu et al., 2022), QLoRA (Dettmers et al., 2023) and the proposed TOKENTUNE. When fine-tuning with TOKENTUNE, we select 30% of the tokens for the gradient computation. Method MMLU ARC Hella Swag Truthful QA Wino Grande Avg. ↑ Llama 7B 64.44 52.39 78.97 38.97 68.90 60.73 Llama 7B w/ LoRA 65.89 55.38 78.76 42.64 68.35 62.20 Llama 7B w/ LoRA+TOKENTUNE (Ours) 65.42 54.01 78.82 43.78 68.35 62.08 Llama 7B w/ QLoRA 65.08 56.06 78.60 43.64 69.38 62.55 Llama 7B w/ QLoRA+TOKENTUNE (Ours) 65.78 53.92 78.74 41.91 69.38 61.95 Llama 7B w/ TOKENTUNE (Ours) 63.06 53.07 77.90 42.18 69.93 61.23 step-wise reasoning using the Alpaca (Taori et al., 2023) prompt template detailed in Appendix A. Few-Shot Evaluation. Then, we evaluate our method against other memory-efficient fine-tuning approaches by assessing its performance on several few-shot benchmarks, such as MMLU (Hendrycks et al., 2021), ARC easy and challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Truth- fulQA (Lin et al., 2022), and WinoGrande (Sak- aguchi et al., 2020). We utilize the evaluation scripts provided by the \"Language Model Eval- uation Harness\" (Gao et al., 2021). During the evaluation process, the model outputs the probabil- ity associated with closed-form problems defined by the context, question, and multiple potential an- swers. We select the answer choice with the text associated with the highest probability. Table 2 reports the accuracy of the model out- put against the ground truth answer. Our method achieves competitive performance gains that are comparable to the performance improvements ob- tained by other memory efficient fine-tuning ap- proaches. We are able to improve the evaluation accuracy upon the base LLama2-7B model, in- creasing the average accuracy from 60.7 to 61.2. We observe the most significant improvements for TruthfulQA (+3.2) and WinoGrande (+1.0) tasks. We also combine TOKENTUNE with LoRA and QLoRA, further improving the evaluation accuracy compared to the use of TOKENTUNE alone. 5.2 Ratio of Tuned Input Positions As done for medium-size encoders in Section 4.2, we then evaluate the impact of the ratio of tuned input positions on the few-shot accuracy. We mea- sure the few-shot accuracy of Llama2-7B models fine-tuned using TOKENTUNE with varying ratio of tuned input positions. Table 3 shows few-shot evaluation accuracy of Llama2-7B when the ratio of fine-tuned positions ranges from 10% to 50% . Contrary to what we observed in Section 4.2, we do not necessarily observe a strong correlation be- tween the few-shot accuracy and the ratio of tuned positions. In fact, we obtain the best performances most often when 20%–30% of input positions are fine-tuned. It is important to observe that the av- erage sequence length in these experiments far ex- ceeds the one from the experiments on the GLUE benchmark. This suggests that tuning a relatively small number of positions may be sufficient to suc- cessfully fine-tune the model on specific datasets. 5.3 GPU Memory Impact As in Section 4.3, we analyze the impact of our method on the GPU memory required to fine-tune large language models. Figure 4 and Table 3 report the GPU memory usage for fine-tuning Llama2- 7B as the number of trained input tokens changes. Given an input sequence of length 2,048, Figure 4 shows that our model reduces the memory usage by up to 28%, from 89 GiB to 64 GiB when reducing the number of trained positions from 2,046 to 256. Table 3: Few-shot evaluation results and peak mem- ory usage (GiB) as Llama2-7B is fine-tuned on instruc- tion datasets with (a) TOKENTUNE, (b) TOKENTUNE + LoRA and (c) TOKENTUNE + QLoRA, varying the selection ratio of input tokens. Best results in bold. (a) TOKENTUNE Selection Ratio Peak Mem. MMLU ARC Hella Swag Truthful QA Wino Grande Avg. Perf. 10% 64.40 61.56 51.71 78.35 41.88 70.01 60.70 20% 65.08 65.01 52.65 78.37 42.02 69.46 61.50 30% 65.94 63.06 53.07 77.90 42.18 69.93 61.23 40% 68.42 63.78 52.90 77.90 41.45 70.32 61.27 50% 74.32 62.98 52.73 78.32 42.11 69.38 61.10 (b) TOKENTUNE + LoRA Selection Ratio Peak Mem. MMLU ARC Hella Swag Truthful QA Wino Grande Avg. Perf. 10% 45.47 64.17 54.44 78.68 38.77 69.61 61.13 20% 48.21 65.41 54.35 79.01 42.21 69.38 62.07 30% 52.77 65.42 54.01 78.82 43.78 68.35 62.08 40% 56.31 64.35 52.65 78.69 41.05 68.90 61.13 50% 64.34 65.87 54.01 78.68 42.46 69.38 62.08 (c) TOKENTUNE + QLoRA Selection Ratio Peak Mem. MMLU ARC Hella Swag Truthful QA Wino Grande Avg. Perf. 10% 11.47 63.54 54.18 78.58 39.79 68.98 61.02 20% 15.68 64.05 53.92 78.81 40.33 69.85 61.39 30% 19.71 65.78 53.92 78.74 41.91 69.38 61.95 40% 24.11 64.85 54.35 78.70 41.98 69.14 61.80 50% 31.06 65.29 53.75 78.70 40.63 69.06 61.49 The advantage of the proposed method is that it can be combined with other memory saving meth- ods. We measure the peak memory required to fine- tune LLama2-7B when combining TOKENTUNE with LoRA or QLoRA. Since these approaches target different parts of the memory footprint, we observe cumulative savings when they are used to- gether. When combining LoRA with TOKENTUNE, the peak memory ranges between 78 GiB to 45 GiB depending on the number of tuned positions. Simi- larly, when combining QLoRA with TOKENTUNE, the peak memory decreases from 49 GiB to 12 GiB as a smaller selection ratio is used. Overall, Figure 4 and Table 3 show that the per- formance of TokenTune is not very sensitive to the choice of token selection ratio, while the memory cost is significantly reduced with a smaller token selection ratio. Based on these results, our recom- mendation is to use 20%–30% as the default token selection ratio, and test if further improvements in performance and memory usage can be obtained for the given task, with a smaller selection ratio. Figure 4: GPU memory required to fine-tune Llama2- 7B (Touvron et al., 2023). We measure the memory by fine-tuning the model on artificially generated data with a given sequence length and batch size. We set the batch size to 1 and the sequence length to 2,048. We show the memory usage when combining TOKENTUNE with LoRA and QLoRA and plot the evolution of the memory required to fine-tune the model on a H100 GPU with a number of trained positions ranging between 256 and 2,046 (we leave at least 2 positions not tuned). Since we could not perform full fine-tuning on our hardware, we estimate the full fine-tuning memory based on the memory reported for TOKENTUNE and LoRA. Specific memory usage values can be found in Table 4. 6"
            },
            {
                "section": "Conclusion",
                "content": "In this paper, we propose TOKENTUNE, a method for reducing the GPU memory required to fine-tune transformer-based models, such as large language models. Our contributions are as follows. • Novelty. TOKENTUNE is the first approach that reduces the GPU memory footprint for fine- tuning via token selection, which selects a subset of the input positions through which the gradient is propagated, while keeping the others frozen. • Combinability. The proposed token selection strategy can be combined with other memory- and parameter-efficient fine-tuning approaches, achieving a greater memory reduction together. • Effectiveness. We empirically benchmark TO- KENTUNE using large language models with up to billions of parameters. As Figure 1 and Ta- ble 1 show, TOKENTUNE achieves similar pre- diction accuracy to representative memory- and parameter-efficient methods, such as LoRA and QLoRA, while significantly reducing the mem- ory usage for fine-tuning (e.g., a joint applica- tion of TOKENTUNE and QLoRA uses 79% less memory than full fine-tuning). 7 Limitations While TOKENTUNE effectively reduces the mem- ory required for storing intermediate activations, it does not affect the other parts of GPU memory us- age, such as the one for parameter gradients. How- ever, as we showed in experiments, TOKENTUNE can be combined with memory-efficient methods that reduce those other parts of memory footprint. Also, the evaluation of TOKENTUNE in this work focused on one domain, namely, language models. Given the applicability of TOKENTUNE to other domains, such as vision (Dosovitskiy et al., 2021), we hope to investigate its effectiveness in broader settings in the future. Potential Risks. Since this paper presents a method for memory-efficient fine-tuning of transformer-based models, such as LLMs, and is not tied to particular applications, we do not see potential risks of the proposed method."
            },
            {
                "section": "results",
                "content": "for the WNLI task.13 C License The majority of TOKENTUNE is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Transform- ers is licensed under the Apache 2.0 license. The license of other libraries used for this paper is as follows. The PEFT and Datasets libraries from HuggingFace are under the Apache-2.0 license. The lm-evaluation-harness framework is under the MIT license. PyTorch is under the modified BSD-3 6https://github.com/huggingface/transformers 7https://github.com/huggingface/peft 8https://github.com/huggingface/datasets 9https://huggingface.co/datasets/garage-bAInd/ Open-Platypus 10https://github.com/EleutherAI/ lm-evaluation-harness 11https://github.com/pytorch/pytorch 12https://gluebenchmark.com/leaderboard 13See (12) from https://gluebenchmark.com/faq license. Open-Platypus used for fine-tuning con- sists of multiple datasets; their license informa- tion can be found at https://huggingface.co/ datasets/garage-bAInd/Open-Platypus. D Training and Evaluation Data BERT model has been pre-trained on 3,300M words. Regarding the instruction tuning experi- ments, we tuned the Llama2-7B on 21,221 samples from the Open-Platypus (Lee et al., 2023) dataset. Note that, while Open-Platypus consists of 11 open- source datasets, we exclude two of them14 that include outputs from GPT (OpenAI, 2023), and instead use the other nine datasets for fine-tuning. Llama2-7B has been pre-trained on 2T tokens and fine-tuned on 100,000 samples.15 E Memory Breakdown Parameter-Efficient Fine-Tuning (PEFT) ap- proaches aim at reducing the compute and storage requirements to fine-tune LLMs by only updating a small subset of the model parameters. As a result, we do not need to store any corresponding gradi- ents and optimizer states for the frozen parameters. When parameters, gradients, and optimizer states represent the majority of the GPU memory usage, these PEFT methods can effectively reduce the memory cost. However, when most GPU memory is used to store intermediate activations, which are required for gradient computation during the backward pass, these PEFT methods cannot effectively cut down the memory cost. Table 5 presents the GPU memory required to perform one training step with BERT-base (Devlin et al., 2019) and OPT (Zhang et al., 2022a) on a consumer hardware GPU. We calibrate the exam- ple such that the memory requirement is roughly the same for both models. In this configuration we can only fit a single example for OPT, while we can use a batch size of 256 for BERT. We observe that the memory breakdown is very different between the two configurations. The required memory dras- tically increases during the forward pass for BERT and during the backward pass for OPT. When com- paring the execution of forward pass with and with- out enabling gradient computation in PyTorch, we estimate that the memory cost to store intermedi- ate activations represents around 22 Gb for BERT 14leetcode-solutions-python-testgen-gpt4 and airoboros-gpt4-1.4.1 15https://llama.meta.com/llama2/ Table 4: GPU memory required to fine-tune Llama2-7B (Touvron et al., 2023) using TOKENTUNE with a varying selection ratio, as well as QLoRA and LoRA. Since we could not perform full fine-tuning on our hardware, we estimate the full fine-tuning memory based on the memory reported for TOKENTUNE, TOKENTUNE + LoRA, and LoRA. See Section 5.3 and Figure 4 for details of the experiment. Selection Ratio TOKENTUNE (Ours) + QLoRA QLoRA TOKENTUNE (Ours) + LoRA LoRA TOKENTUNE (Ours) Full Fine-Tuning 12.5% 11.7 GiB 51.9 GiB 44.6 GiB 80.4 GiB 64.0 GiB 91.4 GiB 25.0% 17.2 GiB 51.9 GiB 48.5 GiB 80.4 GiB 65.0 GiB 91.4 GiB 37.5% 22.0 GiB 51.9 GiB 53.7 GiB 80.4 GiB 66.3 GiB 91.4 GiB 50.0% 27.4 GiB 51.9 GiB 58.3 GiB 80.4 GiB 70.2 GiB 91.4 GiB 62.5% 32.7 GiB 51.9 GiB 63.0 GiB 80.4 GiB 74.6 GiB 91.4 GiB 75.0% 38.8 GiB 51.9 GiB 68.1 GiB 80.4 GiB 79.5 GiB 91.4 GiB 87.5% 43.7 GiB 51.9 GiB 73.4 GiB 80.4 GiB 83.8 GiB 91.4 GiB 99.9% 49.0 GiB 51.9 GiB 77.7 GiB 80.4 GiB 88.7 GiB 91.4 GiB Table 5: Using two models requiring roughly the same GPU memory, we observe that the memory breakdown and the impact of PEFT methods application are very different. For each model, we show the evolution of the GPU memory (×103 MiB) required for performing one training step for OPT-1B3 (Zhang et al., 2022a) with a batch size of 1 and a sequence length of 128 and BERT- base (Devlin et al., 2019) with a batch size of 256, a sequence length of 128. Fwd (w/o grad) corresponds to the execution of the forward pass, while disabling gradient computation. w/ LoRA BERT OPT BERT OPT Cuda Context 0.8 0.8 0.8 0.8 + Model weights 1.3 5.8 1.3 5.8 + Fwd (w/o grad) 2.9 6.1 2.9 6.1 + Fwd (w/ grad) 24.8 6.3 20.6 6.3 + Bwd 25.2 11.3 21.0 6.3 + Optimizer step 25.2 21.4 21.0 6.3 and less than 1 Gb for OPT. On the contrary, we estimate that computing and storing the parame- ter gradients increase the memory requirement by less than 1 Gb for BERT and around 5 Gb for OPT. When applying LoRA (Hu et al., 2022), a PEFT method, we observe that the memory drastically decreases for OPT, while having a less significant impact on BERT. These examples demonstrate that an effective memory reduction across different us- age scenarios can be achieved by combining a suite of memory-efficient fine-tuning methods that can complement each other by reducing different parts of the memory footprint simultaneously. F MRPC and STS-B Descriptive Statistics Table 6 describes the relation between the absolute and relative number of frozen input positions. The Table 6: Distribution of the sentence length for the two GLUE subtasks (MRPC and STS-B). Task 25th per- centile (P25%) Avg. tokens per sentence 75th per- centile (P75%) Max tokens per sentence # Training Sen- tences STS-B 19.0 27.8 31.0 125 5,749 MRPC 44.0 53.2 62.0 103 3,668 Table 7: Relative proportion of fine-tuned tokens aver- aged over MRPC and STS-B tasks with respect to the number of fine-tuned tokens, along with the correspond- ing average performance (reported in Figure 3 (right)). # Fine-Tuned Tokens Average Relative Proportion of Fine-Tuned Tokens Average Perf. 4 13.6% 84.9 8 27.2% 86.4 16 53.9% 87.6 32 81.4% 88.4 64 99.0% 88.8 statistics include distribution of the sentence length for the two subtasks (MRPC and STS-B) used to produce Figure 3 (right). We also report in Table 7 the relative proportion of fine-tuned tokens aver- aged over MRPC and STS-B tasks, as the absolute number of fine-tuned tokens changes, along with the corresponding average performance, which is reported in Figure 3 (right). G GPU Memory Usage Table 4 shows the GPU memory usage required to fine-tune Llama2-7B (Touvron et al., 2023) using the proposed TOKENTUNE with a varying selection ratio, as well as QLoRA and LoRA. Figure 4 also visualizes the same results. See Section 5.3 and Figure 4 for further details of the experiment."
            }
        ]
    },
    "Paper_4": {
        "title": "Towards Efficient Low-order Hybrid Optimizer for Language Model Fine-tuning Minping Chen1, You-Liang Huang1, Zeyi Wen",
        "sections": [
            {
                "section": "Abstract",
                "content": "As the size of language models notably grows, fine-tuning the models becomes more challenging: fine-tuning with first- order optimizers (e.g., SGD and Adam) requires high mem- ory consumption, while fine-tuning with a memory-efficient zeroth-order optimizer (MeZO) has a significant accuracy drop and slower convergence rate. In this work, we propose a Low-order Hybrid Optimizer (LoHO) which merges zeroth- order (ZO) and first-order (FO) optimizers for fine-tuning. LoHO is empowered with inter-layer hybrid optimization and intra-layer hybrid optimization, which boosts the accuracy of MeZO while keeping memory usage within a budget. The inter-layer hybrid optimization exploits the FO optimizer in deep layers and the ZO optimizer in shallow ones, therefore avoiding unnecessary gradient propagation to improve mem- ory efficiency. The intra-layer hybrid optimization updates a proportion of parameters in a layer by the ZO optimizer, and the rest by the FO optimizer, taking advantage of gradient sparsity for high efficiency implementation. Our experimen- tal results across common datasets on different pre-trained backbones (i.e., RoBERTa-large, OPT-13B and OPT-30B) demonstrate that LoHO can significantly improve the pre- dictive accuracy and convergence rate of MeZO, while con- trolling the memory footprint during fine-tuning. Moreover, LoHO can achieve comparable performance with first-order fine-tuning using substantially fewer memory resources."
            },
            {
                "section": "Introduction",
                "content": "Fine-tuning with first-order (FO) optimizers, such as Adam (Kingma and Ba 2014) and AdamW (Loshchilov and Hutter 2018), has been the standard paradigm to adapt pre- trained language models (PLMs) to the specific downstream tasks (Liu et al. 2019; Raffel et al. 2020; Zhang et al. 2023). However, with the growth of the number of model param- eters, the increasing memory requirement for fine-tuning with these first-order optimizers becomes a bottleneck, as the back-propagation process is memory-consuming due to the storage of gradients, optimizer states, and activations. For example, Malladi et al. (2024) demonstrate that full fine-tuning an OPT-13B model with Adam requires approx- imately 12 times the memory cost as the inference. *Corresponding Author Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. In response to this challenge, several approaches have been proposed. In-context learning (ICL) (Brown et al. 2020) makes an inference with only forward passes based on the input prompts (i.e., labeled samples). However, the con- text sizes of most PLMs are limited and the predictive accu- racy of ICL is often much worse than fine-tuning (Brown et al. 2020). Another type of method, namely parameter- efficient fine-tuning (PEFT) (Houlsby et al. 2019; Li and Liang 2021; Hu et al. 2022), only updates a small number of model parameters while approaching similar predictive accuracy of full fine-tuning methods, but it still needs to store numerous activations (intermediate results of the for- ward pass) as the trainable parameters are dispersed across the entire model. To address these problems, MeZO (Mal- ladi et al. 2024) proposes a memory-efficient zeroth-order (ZO) optimizer to estimate the gradients based on finite dif- ferences of function values with just two forward passes, thus saving a significant amount of memory. Despite its ben- efits in memory efficiency, MeZO suffers from a significant accuracy degradation compared with full fine-tuning using first-order optimizers on various tasks and needs substan- tially more training steps to converge (Malladi et al. 2024). We observe that MeZO only requires the same memory as inference, and the free/unused memory is wasted when the GPUs are solely occupied to maximize the computation ef- ficiency during fine-tuning. For example, we find that when using an A800 GPU to fine-tune OPT-13B with MeZO, it exhibits over 10GB of free memory. It would be beneficial to use this free memory to mitigate MeZO’s accuracy drop and convergence issues. To this end, we propose Low-order Hybrid Optimizer (LoHO), which exploits the memory ef- ficiency of ZO optimizers and the optimization quality of FO.1 LoHO is equipped with two hybrid optimization strate- gies, i.e., inter-layer hybrid optimization and intra-layer hy- brid optimization. The inter-layer hybrid optimization em- ploys an FO optimizer for fine-tuning a PLM’s several lay- ers, while a ZO optimizer updates its rest layers, avoiding unnecessary gradient propagation with the suitable setting of the FO layers. Different from inter-layer hybrid optimiza- tion, intra-layer hybrid optimization selects some parame- 1In this paper, we classify zeroth-order optimizers and first- order optimizers as low-order optimizers to distinguish them from high-order optimizers, i.e., second-order optimizers. ters in a layer to be updated by the FO optimizer, while the rest are updated by the ZO optimizer. Furthermore, LoHO makes use of the gradient sparsity due to the mixture of FO and ZO to achieve more efficient fine-tuning, by stor- ing the trainable parameters and their gradients in a sparse format. To enhance the convergence in fine-tuning, we cus- tomize the learning rates of both optimizers in LoHO, based on the Frobenius norm of gradients estimated by the ZO op- timizer and that computed by the FO optimizer, thus keeping the progress of the two optimizers in a similar pace. A nice property of LoHO is that it enables practitioners to control the GPU memory usage (to the fullest extent if needed) by setting the number of layers or ratio of parameters to be han- dled by the FO optimizer and ZO optimizer. To summarize, our key contributions are as follows. • We propose a low-order hybrid optimizer (called LoHO) which integrates a zeroth-order optimizer with a first- order optimizer for fine-tuning language models. LoHO is equipped with inter-layer hybrid optimization and intra-layer hybrid optimization, which maximize the memory usage within a given budget while improving the model quality of the MeZO method. • To further optimize LoHO, we design a customization strategy of the learning rate for FO and ZO optimizers, based on the Frobenius norm of gradients, thus achieving better generalization. • Our experiments across common datasets on three pre- trained backbones (i.e., RoBERTa-large, OPT-13B, and OPT-30B) show the effectiveness of LoHO on pre- dictive accuracy and convergence rate compared with MeZO. Furthermore, LoHO achieves comparable perfor- mance with first-order based full fine-tuning, enjoying less memory consumption (e.g., reducing the GPU re- sources from 4 GPUs to a single GPU). Methodology In this section, we introduce the technical details of the proposed Low-order Hybrid Optimizer (LoHO), which con- tains two hybrid optimization strategies, as shown in Fig- ure 1. The left part of Figure 1 shows the inter-layer hy- brid optimization and the right part shows the intra-layer hy- brid optimization. For completeness, we first introduce the MeZO (Malladi et al. 2024) method and then elaborate on the details of our methods. The MeZO Solution A traditional ZO gradient estimator known as Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall and J.C 2002), and the corresponding SGD algorithm, ZO-SGD are the basic ZO technique used in MeZO (Malladi et al. 2024). Suppose D = {(xi, yi)}i∈∥D∥is a labeled dataset and B ∈D is a minibatch of data. We denote the loss on the minibatch as L(θ; B), where θ ∈Rd denotes the parameters of a PLM and d is the number of the model parameters. Then the gradient estimate of Simultaneous Perturbation Stochas- tic Approximation (SPSA) is defined as follows: ˆ∇L(θ; B) = L(θ + ϵz; B) −L(θ −ϵz; B) 2ϵ z ≈zzT ∇L(θ; B), (1) ZO forward FO forward FO back-propagation Transformer Layer 0 Transformer Layer 1 Transformer Layer i Transformer Layer L-2 Transformer Layer L-1 FO optimizer ZO optimizer Input Output Inter-layer hybrid optimization Intra-layer hybrid optimization Transformer Layer 0 Transformer Layer 1 Transformer Layer i Transformer Layer L-2 Transformer Layer L-1 Input Output FO optimizer ZO optimizer weight Figure 1: Low-order Hybrid Optimizer (LoHO). where z ∈Rd which is sampled from a Gaussian distribu- tion, i.e, z ∼N(0, Id), and ϵ is the perturbation scale. SPSA only needs two forward passes of the model to estimate the gradients. Parameter perturbation of the model is performed before each forward pass, and both of the forward passes aim to compute a loss value. Then the gradients are estimated based on the difference between the two loss values. Based on the SPSA, the zeroth-order SGD (ZO-SGD) op- timizer updates the parameters as θt+1 = θt −η ˆ∇L(θ; Bt), where η is the learning rate, and Bt is the minibatch at the t-th iteration. However, the memory requirement for the vanilla ZO-SGD algorithm is twice as the inference be- cause it necessitates the storage of z ∈Rd, whose size is the same as the model parameters. To address this issue, MeZO (Malladi et al. 2024) proposes a memory-efficient implementation for ZO-SGD based on the in-place opera- tion, which only requires the same memory footprint as in- ference. Specifically, MeZO stores the seed of the random number generator used for sampling z, and resamples the same random noise z with the seed when z is needed. Be- sides, MeZO modifies the parameters in place when per- forming model perturbation and parameter updating. Our Proposed LoHO Solution Although MeZO offers benefits in memory efficiency, it has significant accuracy degradation compared with full fine- tuning with SGD or Adam. Moreover, it often exhibits “wasted” memory when the GPU is solely used for the fine- tuning task. In response, we propose a low-order hybrid op- timizer (LoHO) to improve the performance of MeZO while keeping the memory usage within a budget, by integrating a ZO optimizer and a FO optimizer. There are two kinds of solutions for combining the ZO optimizer and the FO op- timizer, i.e., the inter-layer solution and the intra-layer so- lution. The inter-layer solution has three variants: (1) Z+F which uses the ZO optimizer to train the shallow layers and the FO optimizer to train the deep layers; (2) F+Z which uses the FO optimizer to train the shallow layers and the ZO optimizer to train the deep layers; (3) F+Z+F which uses the FO optimizer to train the first several layers and the last sev- eral layers, while using the ZO optimizer to train the mid- dle layers. In comparison, the intra-layer solution updates some parameters in a layer with the ZO optimizer, and the rest are updated by the FO optimizer. Although these four solutions theoretically can be used for hybrid optimization, we explore the performance of inter-layer solution with Z+F and intra-layer solution in this paper considering the mem- ory efficiency problem. Next, we present the details of where the peak memory footprint comes from during fine-tuning to explain why choosing these two solutions. The memory consumption mainly comes from three parts: model parameters, gradients (may include the optimizer states if using Adam), and activations. We take a multi-layer perception (MLP) network as an example: hN = MLPN(MLPN−1(...(MLP2(MLP1(h0)))...)), (2) where h0 is the input, and N is the number of the MLP lay- ers. The output of the i-th layer is hi = MLPi(hi−1) = σ(Wihi−1), where σ is the activation function, and Wi is the weight matrix with the bias term omitted here for sim- plicity. If we denote the output of the i-th layer before per- forming the activation function as xi = Wihi−1, then in a back-propagation step with a loss function L, the gradient of Wi is computed using the chain rule as follows. ∂L ∂Wi = ∂L ∂hi ( YN j=i+1 ∂hj ∂xj ∂xj ∂hj−1 )∂hi ∂xi ∂xi ∂Wi = ∂L ∂hi ( YN j=i+1∆σjWj)∆σihi−1, (3) where ∆σ is the derivative of σ. Based on this, activations {xj}N j=i are cached in order to compute the gradient of Wi even when {Wj}j>i are frozen. Inter-layer Hybrid Optimization Upon analyzing the peak memory consumption during fine-tuning, it is evident that the positioning of the FO-optimized layers significantly impacts the overall memory footprint. The Z+F solution, which uses the ZO optimizer for the shallow layers and the FO optimizer for the deep layers, can reduce more mem- ory consumption compared with F+Z and F+Z+F solutions, since they require caching more activations. Therefore, we adopt this solution, as shown in the left part of Figure 1. In this solution, the gradients computation of the ZO and FO optimizer are independent. For the deep layers that are op- timized by the FO optimizer, we perform back-propagation to obtain their gradients. For the shallow layers that are opti- mized by the ZO optimizer, we perform two forward passes to estimate their gradients. Then we update the parameters for the shallow layers and the deep layers separately. In this paper, we integrate the recently proposed MeZO (Malladi et al. 2024) with FO optimizers such as SGD and Adam to demonstrate the effectiveness of inter-layer hybrid optimiza- tion, although our method can work with other zeroth-order optimizers (e.g. ZO-AdaMU (Jiang et al. 2024)). Intra-layer Hybrid Optimization In addition to the inter- layer hybrid optimization, we explore another strategy, i.e., intra-layer hybrid optimization, as shown in the right part in Figure 1. A question raised here is how to determine the 0 2500 5000 7500 10000 Steps 20 40 60 80 100 Gradient Norm (a) SGD 0 10000 20000 30000 40000 Steps 0.0 0.5 1.0 1.5 2.0 Gradient Norm 1e6 (b) MeZO Figure 2: Gradient Frobenius norm comparison during fine- tuning between SGD optimizer and MeZO optimizer. parameter subset in a layer to be updated by the FO opti- mizer. Although various strategies can be explored, we find that random selection can achieve promising performance. Specifically, for each weight matrix in a layer, we randomly select a ratio of parameters to be updated by the FO op- timizer, and the ZO optimizer updates the remainder. Al- though this intra-layer hybrid optimization is simple, its ef- ficient implementation is nontrivial. If we use the general implementation of training neural networks, we inherently necessitate the computation of gradients for the non-FO pa- rameters as the gradient of each weight matrix is computed as shown in Equation (3). Such implementation results in degradation in memory efficiency, making the memory re- quirement similar to that of full fine-tuning. To tackle this challenge, we employ a sparse training tech- nique based on a sparse operations library (Nikdan, Tabesh, and Alistarh 2024), where the trainable parameters of the FO optimizer and their gradients are stored in a sparse format. Specifically, in the forward step, for a dense pre- trained weight Wi, we add it with a trainable sparse ma- trix ∆i where the non-zero values indicate the FO train- able parameters of Wi. Then the output O is obtained by O = X(Wi+∆i), where X is the input. For the correspond- ing backward step, according to the chain rule, the gradients of X and ∆i are ∂L ∂X = ∂L ∂O(Wi+∆i )t and ∂L ∂∆i = Xt ∂L ∂O respectively. Note that ∂L ∂∆i is still sparse by using the Sam- pled Dense-Dense Matrix Multiplication (SDDMM) kernel. It multiplies two dense matrices where only the required el- ements of the output are computed. The gradient estimation for the ZO optimizer is similar to that in the inter-layer hy- brid optimization. However, in the intra-layer hybrid opti- mization, we only estimate the gradients of the parameters that belong to the ZO optimizer. Through this implemen- tation, we can control memory consumption by setting the proper ratio of parameters belonging to the FO optimizer. Customized Learning Rates for Hybrid Optimization In LoHO, we can use the same learning rate for the FO opti- mizer and ZO optimizer. However, we find that the Frobe- nius norm of the gradients obtained by the ZO optimizer (e.g., MeZO) and the FO optimizer (e.g., SGD) are sig- nificantly different, as shown in Figure 2. We transform the whole model parameters into a vector and calculate the Frobenius norm of its gradient G ∈Rd as ∥G∥F = qPd i=1 |Gi|2, where ∥∥F denotes the Frobenius norm. The Frobenius norm of the gradients of MeZO is significantly larger than that of SGD due to two main reasons. First, the absolute magnitudes of the gradients in SGD often decrease from the deep layers to the shallow layers, because most of the absolute gradient values are smaller than one, and per- forming the multiplication of the chain rule shown in Equa- tion (3) makes the absolute gradient values smaller. In con- trast, the calculation of the gradients of each layer in MeZO is independent, and thus the absolute magnitudes of the gra- dients in different layers maintain the same scales. Second, there is a perturbation scale ϵ in the gradient estimation func- tion (cf. Equation 1) which is commonly set to a value much smaller than one (e.g., 0.01 or 0.001) (Malladi et al. 2024), making the estimated gradient norm relatively larger than that in back-propagation. Therefore, we need to set differ- ent learning rates for the FO optimizer and the ZO optimizer to balance the parameter update amplitude of the two kinds of optimizers. Furthermore, as the FO optimizer often con- verges faster than the ZO optimizer, we can speed up the convergence rate in the hybrid optimization if we set the learning rates suitably. Consequently, the slow convergence problem of MeZO can be alleviated. In practice, the learning rate of the ZO optimizer can be configured to be several or- ders of magnitude lower than that of the FO optimizer. This customized learning rate setting can be utilized in both inter- layer hybrid optimization and intra-layer optimization. Experiments Datasets and Baselines Datasets Following the settings as the work of Malladi et al. (2024), we chose RoBERTa-large (Liu et al. 2019), OPT-13B and OPT-30B (Zhang et al. 2023) as the pre- trained backbones to conduct the experiments. Most of the datasets we selected exhibit performance gaps exceeding 4% between MeZO and full fine-tuning with Adam. Thus, for the RoBERTa-large experiments, we used the follow- ing datasets: SST-2 (Socher et al. 2013), RTE (Cer et al. 2017), MNLI (Williams, Nangia, and Bowman 2018) and SNLI (Bowman et al. 2015). We followed the settings of Malladi et al. (2024), which used 512 examples per class for both training and validation. Similarly, for the OPT experi- ments, we used the following datasets, including RTE (Cer et al. 2017), BoolQ (Clark et al. 2019), CB (De Marneffe, Si- mons, and Tonhauser 2019), MultiRC (Khashabi et al. 2018) and WIC (Pilehvar and Camacho-Collados 2018). We ran- domly sampled 1,000 examples for training, 500 examples for validation, and 1,000 examples for testing, which is the same as MeZO (Malladi et al. 2024). Baselines The baselines in our experiments include: • Zero-shot: it directly performs inference based on a sin- gle prompt without fine-tuning. • ICL (In context learning): it performs inference with prompts based on several labeled samples as input. • MeZO (Malladi et al. 2024): it updates all the model pa- rameters with the memory-efficient ZO-SGD optimizer. Table 1: Experiments on the RoBERTa-large model. † indi- cates results reported in MeZO (Malladi et al. 2024). Mem- ory budget: a single RTX 4090 GPU with 24GB memory. Dataset SST-2 RTE MNLI SNLI Average Zero-shot† 79.0 51.4 48.8 50.2 57.4 MeZO† 93.30.7 78.62.0 78.30.5 83.01.0 83.3 MeZO-Adam† 93.30.6 79.21.2 79.60.4 85.30.8 84.4 LoHO-SGDinter 94.60.5 79.81.8 80.61.9 85.80.6 85.2 LoHO-Adaminter 95.00.6 81.11.1 83.20.4 87.60.9 86.7 LoHO-Adamintra 94.90.5 80.40.7 82.20.9 87.60.4 86.3 • MeZO-Adam (Malladi et al. 2024): it updates all the model parameters with a variant of MeZO which uses Adam optimizer instead of SGD. As the naive imple- mentation of MeZO-Adam requires additional memory to store the gradient momentum estimates, Malladi et al. (2024) only investigate its performance on the RoBERTa- large experiments. We followed their setting in this work. Our methods include (i) LoHO-SGDinter and LoHO- Adaminter, which are inter-layer hybrid optimization meth- ods using SGD and Adam as the FO optimizer respectively, and (ii) LoHO-Adamintra, which is an intra-layer hybrid op- timization method using Adam as the FO optimizer. Experimental Setup A natural question raised in inter-layer hybrid optimization is how to set the number of FO-optimized layers. Although one can perform experiments to find out the best setting of the number of FO-optimized layers, in this work, our main experiments investigate the performance with the maximum number of FO-optimized layers that can be used in a sin- gle GPU. There are two reasons for this setting. On the one hand, using more FO layers is more promising for achieving better performance. On the other hand, we can best utilize the GPU memory as much as possible, since the free/unused memory is wasted when the GPUs are solely used for the specific task to maximize the computation efficiency. Other settings about the number of FO layers are explored in the analysis experiments. Another question is how to set the ratio of parameters to be updated by the FO optimizer in each layer. To make a fair comparison between inter-layer and intra-layer optimization, the only difference between LoHO-Adaminter and LoHO-Adamintra is that a ratio of parameters of the FO layers in LoHO-Adamintra is opti- mized by the ZO optimizer. For more details about the set- tings, please refer to the Appendix A. Our code is available at https://github.com/Chan-1996/LoHO. Main Results"
            },
            {
                "section": "Results",
                "content": "on RoBERTa-large The performance compari- son between different methods on RoBERTa-large is shown in Table 1. First, we can see that the performance of LoHO- SGDinter and LoHO-Adaminter substantially outperform MeZO and MeZO-Adam by 1.9% and 2.3% on average ac- curacy respectively, and LoHO-Adamintra also outperforms MeZO-Adam by 1.9% on average accuracy, showing the po- tential of making more efficient use of memory. Notably, Table 2: Experiments on the OPT-13B model. Memory bud- get: a single A800 GPU with 80GB memory. Dataset RTE CB BoolQ WIC MultiRC Average Zero-shot† 59.6 46.4 59.0 55.0 46.9 53.4 ICL† 62.1 57.1 66.9 50.5 53.1 57.9 MeZO† 66.1 67.9 67.6 61.1 60.1 64.6 LoHO-SGDinter 76.2 71.4 67.8 63.6 58.6 67.5 LoHO-Adaminter 77.6 69.6 64.7 65.4 67.2 68.9 LoHO-Adamintra 75.5 71.4 64.0 65.4 66.5 68.6 Table 3: Experiments on the OPT-30B model. Memory bud- get: a single A800 GPU with 80GB memory. Dataset RTE BoolQ WIC Average Zero-shot† 52.0 39.1 50.2 47.1 ICL† 66.8 66.2 51.3 61.4 MeZO† 66.4 67.2 56.3 63.3 LoHO-SGDinter 68.0 68.1 61.0 65.7 LoHO-Adaminter 72.6 67.2 60.7 66.8 LoHO-Adamintra 71.5 65.8 62.7 66.7 the performance difference between MeZO-Adam (resp. MeZO) and LoHO-Adaminter (resp. LoHO-SGDinter) is statistically significant with a p-value=1.1e-2 (resp. 1.6e- 2) < 0.05 by a two-tailed t-test. Second, LoHO-Adaminter outperforms LoHO-SGDinter, indicating that using the mo- mentum information can improve the quality of the esti- mated gradients. Finally, LoHO-Adamintra achieves com- parable performance with LoHO-Adaminter while involv- ing less trainable parameters for the FO optimizer."
            },
            {
                "section": "Results",
                "content": "on OPT-13B and OPT-30B The experimental re- sults of different methods on OPT-13B and OPT-30B are shown in Table 2 and Table 3 respectively. Our proposed LoHO-SGDinter and LoHO-Adaminter outperform MeZO by 2.9% and 4.3% in average accuracy respectively on OPT- 13B. Besides, our methods consistently outperform MeZO on the OPT-30B model. However, the predictive accuracies of our methods on OPT-30B are lower than that on OPT- 13B, due to the fewer FO layers used in the OPT-30B model as we set the memory budget to a single GPU. In summary, our methods are effective both for medium-sized language models and large language models compared with MeZO, and LoHO-Adaminter achieve the best performance among different variants of the proposed LoHO. Ablation Study We performed the ablation study and presented the results on two representative datasets, i.e., the MNLI dataset and RTE dataset using RoBERTa-large as the backbone. Inter-layer hybrid optimization The ablation baseline for the proposed inter-layer hybrid optimization include: FT- PT-SGD which freezes the layers optimized by MeZO in LoHO-SGDinter method and only updates the rest of the layers using SGD optimizer; FT-PT-Adam which freezes the layers optimized by MeZO in LoHO-Adaminter method and only updates the rest layers using Adam optimizer; LoHO-SGDinter−elr and LoHO-Adaminter−elr which uses MNLI RTE 65 70 75 80 85 90 Performance 80.2 79.4 82.7 80.8 75.2 76.7 77.4 77.8 80.6 79.8 83.2 81.1 FT-PT-SGD FT-PT-Adam LoHO-SGDinter elr LoHO-Adaminter elr LoHO-SGDinter LoHO-Adaminter (a) Inter-layer MNLI RTE 78 79 80 81 82 83 84 85 Performance 82.3 80.4 82.0 80.5 82.2 80.4 Absolute min Absolute max Random (b) Intra-layer Figure 3: Ablation study of hybrid optimization. the equal learning rate for the ZO optimizer and FO opti- mizer. From Figure 3 (a), we can observe that methods based on hybrid optimization outperform their corresponding vari- ant based on a single optimizer. This is reasonable as in hy- brid optimization, the searching space of the model is larger and it is more promising to approach the global optimal dur- ing the training process. Another finding is that due to the noticeable difference in the gradient norm obtained by the ZO and the FO optimizer, the experimental results of LoHO- SGDinte−elr and LoHO-Adaminter−elr are unsatisfactory. In summary, the ablation study further indicates the effec- tiveness of the proposed method in this work. Intra-layer hybrid optimization In LoHO, we used ran- dom selection to obtain a subset of parameters to be updated by the FO optimizer. Here, we investigate other strategies to select the FO-optimized parameters, including absolute- min and absolute-max. Concretely, the absolute-min (resp. absolute-max) strategy leverages a ratio of parameters with minimal (resp. maximal) absolute values to be updated by the FO optimizer. As shown from Figure 3 (b), the three strategies achieve comparable performance on both the MNLI and RTE datasets. Therefore, we adopt the random selection strategy, instead of other heuristic strategies. Analysis on Different Settings for LoHO Effect of the Number of FO Layers We study the effect of different settings of FO layers in the inter-layer hybrid optimization. In particular, we investigate utilizing differ- ent numbers of FO layers and different positions of FO lay- ers for LoHO-Adaminter on the MNLI dataset (i.e., LoHO- Adaminter last n and LoHO-Adaminter first n). As shown in Figure 4 (a), when the FO layers are the deep layers, the model accuracy consistently improves on the MNLI dataset as the number of FO layers increases, which is sensible. However, placing FO layers in shallow layers significantly reduces predictive accuracy compared to using the FO opti- mizer for deep layers in most cases. One possible reason is that the deep layers have a greater impact than the shallow layers for this task. We leave this in future work to investi- gate the importance and effect of each layer in the PLM. Effect of the Ratio of FO-optimized Parameters We also study the effect of the ratio of FO-optimized parame- ters in the proposed intra-layer hybrid optimization. As pre- sented in Figure 4 (b), we performed this analysis on the 0 4 8 12 24 Number of FO layers 75.0 77.5 80.0 82.5 85.0 87.5 Performance LoHO-Adaminter_last_n LoHO-Adaminter_first_n (a) Effect of FO layers 0 0.1 0.2 0.3 0.4 0.5 0.6 1.0 Ratio of FO paramters 76 78 80 82 84 Performance 78.3 79.9 81.2 81.7 81.9 82.2 82.6 83.2 LoHO-Adamintra (b) Effect of FO parameter ratio Figure 4: Effect of FO layers and the ratio of FO parameters. Table 4: Memory usage comparison. The max sequence lengths of RTE and MultiRC datasets are 288 and 746 re- spectively. bz denotes batch size. Model Method Memory (GB) GPU #FO layers MeZO 4.4 RTX 4090 0 RoBERTa MeZO-Adam 9.3 RTX 4090 0 (RTE) LoHO-SGDinter 20.0 RTX 4090 12 (bz=64) LoHO-Adaminter 20.5 RTX 4090 12 LoHO-Adamintra 19.4 RTX 4090 12 OPT-13B MeZO 71.0 A800 0 (MultiRC) LoHO-SGDinter 76.9 A800 2 (bz=16) LoHO-Adaminter 78.9 A800 2 LoHO-Adamintra 76.6 A800 2 OPT-30B MeZO 65.1 A800 0 (RTE) LoHO-Adaminter 78.9 A800 4 (bz=8) LoHO-Adamintra 73.5 A800 4 MNLI dataset to demonstrate the model quality across var- ious ratios of FO-optimized parameters in each FO layer. With the ratio of FO-optimized parameters increasing, the model performance improves consistently. Moreover, using 60% of FO-optimized parameters can achieve comparable performance with that of using 100% FO-optimized param- eters, which is akin to inter-layer hybrid optimization. How- ever, improving the ratio of FO-optimized parameters also requires more memory consumption. Therefore, it is a trade- off between memory requirement and model performance. Analysis on Memory Usage Here, we show the memory profiling of fine-tuning using different backbones in Table 4. Specifically, we selected the RTE and MultiRC datasets to perform the memory profil- ing. The number of the FO layers depends on the sequence length of the specific dataset. For example, for the OPT- 30B model, the maximum number of FO layers is four us- ing a single A800 GPU. Following Malladi et al. (2024), we used Nvidia’s nvidia-smi command to monitor the peak GPU memory usage. From the results, we can see that although requiring additional memory compared with MeZO, our hy- brid optimization method still can be fine-tuned in a single GPU. The experimental results and the memory profiling re- sults demonstrate that trading off between performance and memory is effective. Analysis on Convergence Rate In this section, we demonstrate the convergence rate com- parison between MeZO and our hybrid optimizer (here we (a) MNLI (b) RTE (c) MNLI (d) RTE (e) MNLI (f) RTE Figure 5: Convergence rate comparison on RoBERTa-large. use inter-layer hybrid optimization). For a fair compari- son, we compare MeZO with LoHO-SGDinter since both of them use SGD optimizer. We show this comparison from two perspectives (loss vs. training step and loss vs. training time) on two datasets in Figure 5 (more results can be found in Appendix B). It can be observed that the convergence rate of LoHO-SGDinter is noticeably faster than that of MeZO on these two datasets. Although LoHO-SGDinter needs to perform three forward passes and a backward pass in a training step, while MeZO only needs to perform two for- ward passes, LoHO-SGDinter can reduce substantial train- ing steps compared with MeZO. As a result, the overall training time of LoHO-SGDinter is shorter than that of MeZO. This comparison demonstrates that LoHO-SGDinter has its advantages in terms of accuracy and time efficiency. We also compare the convergence rate between LoHO- SGDinter and LoHO-Adaminter, as shown in the lower part of Figure 5. The convergence rate of LoHO-Adaminter is faster than that of LoHO-SGDinter, which meets our expec- tations as Adam also converges faster than SGD. Comparison with First-order Fine-tuning Here, we compare LoHO with full fine-tuning using first- order optimizers. We present the performance comparison and memory usage in Table 5 and Table 6 respectively. Full FT (SGD) and Full FT (Adam) are full fine-tuning meth- ods using the SGD optimizer and the Adam optimizer re- spectively. LoHO achieves comparable performance with Table 5: Performance comparison between LoHO and fine- tuning with first-order optimizers on RoBERTa-large. Dataset SST-2 RTE MNLI SNLI Average Full FT (SGD) 95.1 81.1 80.5 85.5 85.6 Full FT (Adam) 94.9 83.0 83.6 87.2 87.2 LoHO-SGDinter 94.6 79.8 80.6 85.8 85.2 LoHO-Adaminter 95.0 81.1 83.2 87.6 86.7 Table 6: Memory usage comparison. Model Method Mem.(GB) GPU #FO layers RoBERTa Full FT (SGD) 43.1 A800 24 (RTE) Full FT (Adam) 45.8 A800 24 (bz=64) LoHO-SGDinter 20.0 RTX 4090 12 LoHO-Adaminter 20.5 RTX 4090 12 OPT-13B Full FT (Adam) (bz=1) 316.0 4 × A800 40 (MultiRC) LoHO-Adaminter(bz=16) 78.9 1 × A800 2 Full FT (Adam), with over 50% less memory usage on RoBERTa-large. On OPT-13B, it needs four A800 GPUs to run Full FT (Adam) with a batch size of one, while LoHO can be run with a single A800 GPU and a batch size of 16. Besides, LoHO can achieve comparable predictive ac- curacy or even outperform Full FT (Adam) on some tasks with OPT-13B backbone. For example, on the RTE dataset whose sequence length is shorter than 300 (so we can use more FO layers in an A800 GPU), LoHO achieves 77.6% while Full FT (Adam) achieves 70.8% in accuracy. How- ever, on datasets with longer sequence lengths, such as Mul- tiRC, the accuracy of LoHO and Full FT (Adam) is 67.2% and 71.1%, respectively, as we can only use two FO lay- ers within the memory budget. Nevertheless, if improving the memory budget, LoHO can approach the performance of Full FT (Adam) on datasets with longer sequence lengths. Related Work Here, we discuss the existing work on zeroth-order and first- order optimization and hybrid optimizers. Zeroth-order Optimization ZO optimization has been extensively studied in the realm of convex and strongly convex objectives (Jamieson, Nowak, and Recht 2012; Agarwal et al. 2012; Raginsky and Rakhlin 2011). As a kind of backpropagation-free optimization method, ZO optimization approximates gradients based on finite differences. It has long been studied and achieved re- markable success in solving various machine learning prob- lems, such as adversarial attack and defense (Tu et al. 2019; Ilyas et al. 2018; Shu et al. 2022; Zhao et al. 2019), auto- mated machine learning (Gu et al. 2021; Wang et al. 2022), policy search in reinforcement learning (Vemula, Sun, and Bagnell 2019), visual prompting for transfer learning (Tsai, Chen, and Ho 2020) and so on. Recently, Malladi et al. (2024) propose a memory- efficient zeroth-order optimizer (MeZO) that adapts the clas- sical ZO-SGD method to operate in place, making large language model fine-tuning as efficient as inference. De- spite its memory efficiency, MeZO still exhibits a signifi- cant performance drop in terms of predictive accuracy and slower convergence speed compared to full fine-tuning on various tasks. Several techniques have been developed to address these problems, including Sparse MeZO (Liu et al. 2024) which leverages gradient sparsity to improve both predictive accuracy and convergence speed of MeZO, ZO- AdaMU (Jiang et al. 2024) which adapts the simulated per- turbation with momentum in the stochastic approximation to improve convergence stability and rate in ZO-SGD, and HiZOO (Zhao et al. 2024) which leverages the diagonal Hes- sian to enhance the convergence and predictive accuracy of MeZO. Although efforts have been invested to improve the MeZO, there is still a performance gap between these meth- ods and first-order based methods on complex tasks. First-order Optimization There are many classical first-order optimization methods, such as SGD, Momentum, Adagrad (Duchi, Hazan, and Singer 2011) and ADADELTA (Zeiler 2012), which are fun- damental in many research areas such as computer vision and natural language processing. However, with the emer- gence of large-scale models, the effectiveness of such con- ventional first-order optimization methods has been chal- lenged. This is because their convergence rate needs to be improved to accelerate the training of large-scale models. Adam (Kingma and Ba 2014) has emerged and become a dominant optimizer for training and fine-tuning large-scale models due to its fast convergence speed. To alleviate the over-fitting problem, AdamW (Loshchilov and Hutter 2018) proposes to add a weight decay coefficient in Adam. To ac- celerate the training of large-scale models with large batch sizes, LAMB (You et al. 2019) proposes to employ the prin- cipled inter-layer adaptation strategy. Hybrid Optimization Hybrid optimization has not been extensively studied. Lan- dro, Gallo, and La Grassa (2020) combine SGD and Adam by using constant weights to balance the contributions of gradient estimates from each optimizer. Similar to the stan- dard fine-tuning with FO optimizers, this method requires significant memory. Ansaripour et al. (2022) propose hy- brid optimization at the model level under the decentralized optimization setting in a distributed system (some agents are optimized by ZO; others are optimized by FO), which is sig- nificantly different from ours. The hybrid optimization pro- posed in our work is layer level (i.e., inter-layer strategy) or weight level (i.e., intra-layer strategy)."
            },
            {
                "section": "Conclusion",
                "content": "In this work, we have proposed a Low-order Hybrid Optimizer (LoHO) for language model fine-tuning. In LoHO, we have proposed two hybrid optimization strategies namely inter-layer hybrid optimization and intra-layer hy- brid optimization. The two proposed strategies are designed to maximize memory usage to the fullest extent, by consid- ering the gradient back-propagation and leveraging the gra- dient sparsity respectively. We have experimentally demon- strated the effectiveness of LoHO both in predictive accu- racy and convergence rate on various datasets and different pre-trained backbones within a memory budget."
            }
        ]
    },
    "Paper_5": {
        "title": "Published as a conference paper at ICLR 2024 ADDAX: MEMORY-EFFICIENT FINE-TUNING OF LANGUAGE MODELS WITH A COMBINATION OF FORWARDBACKWARD AND FORWARD-ONLY PASSES",
        "sections": [
            {
                "section": "ABSTRACT",
                "content": "Fine-tuning language models (LMs) with ﬁrst-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We in- troduce a novel method named Addax that integrates the recently introduced Memory-Efﬁcient Zeroth-order Optimizer of Malladi et al. (2023) with Stochastic Gradient Descent (SGD). Addax obtains zeroth-order and ﬁrst-order gradient esti- mates and optimally combines them as the descent direction in each step. The ﬁrst- order updates are performed “in-place” to further save memory. Theoretically, we establish the convergence of Addax under mild assumptions, demonstrating less restrictive hyper-parameters and independence from model size. Our extensive ex- periments with diverse LMs and tasks show that Addax consistently outperforms zero-shot and MeZO in terms of accuracy. Moreover, Addax surpasses the per- formance of standard ﬁne-tuning approaches, such as SGD and Adam, in speciﬁc scenarios with signiﬁcantly less memory requirement. 1"
            },
            {
                "section": "INTRODUCTION",
                "content": "Fine-tuning pre-trained language models (LMs) is crucial for diverse natural language processing tasks, including text classiﬁcation and sentiment analysis (Devlin et al., 2019), as well as their use in different domains (Gururangan et al., 2020). However, standard ﬁne-tuning with Adam opti- mizer demands excessive memory usage due to gradient and/or optimizer state storage, presenting a challenge as LMs grow in scale (Brown et al., 2020; OpenAI, 2023). For instance, ﬁne-tuning a 13-billion-parameter model like OPT (Zhang et al., 2022) in mixed precision requires over 316 GB of memory, hindering accessibility for researchers and practitioners with limited resources and specialized hardware. This memory burden restricts innovation and experimentation. Recently, various memory-efﬁcient methods for ﬁne-tuning Large Language Models (LLMs) have been proposed. In-context learning (ICL) utilizes a single inference pass, incorporating label exam- ples in its context for prediction (Brown et al., 2020). Despite its limited success, ICL’s performance is constrained by the model’s context size and is shown to be less effective than traditional Adam ﬁne-tuning for medium-sized LMs (Brown et al., 2020). Parameter-Efﬁcient Fine-Tuning (PEFT) selectively tunes a fraction of the network while freezing the rest of the parameters, and signiﬁcantly reduces the parameters needed for ﬁne-tuning (Hu et al., 2022; Li & Liang, 2021; Lester et al., 2021). Despite its efﬁciency, ﬁne-tuning LMs with PEFT still requires more memory than model inference. For example, ﬁne-tuning OPT-13B with Adam with a batch size of 8 requires 4×A100 GPUs (316GB total), whereas utilizing PEFT decreases this to 2×A100 GPUs (158GB total) with a batch size of 16 (Brown et al., 2020). Nonetheless, this memory requirement is still 6× greater than what is needed for model inference, which is around 25GB. Memory-Efﬁcient Zeroth-order Optimizer (MeZO) proposed by Malladi et al. (2023) generates gra- dient estimators solely through forward passes with minimal memory overhead. Unlike classical zeroth-order optimization method ZO-SGD (Spall, 1992), MeZO allows in-place perturbation of model parameters to avoid storing the perturbation vector. Despite having a memory footprint equivalent to the inference phase, MeZO exhibits slower convergence compared to widely used 1 Published as a conference paper at ICLR 2024 100 200 300 GPU Memory (GB) 65 70 75 80 85 90 Accuracy (%) BS = 16 BS = 16 BS = 2 BS = 2 BS = 2 BS = 2 BS = 2 20 30 40 50 60 70 87 88 89 90 91 92 BS = 8 BS = 8 BS = 16 BS = 2 BS = 2 TASK : RTE TASK : CB TASK : COPA 16-bit SGD 16-bit SGD (IP) MeZO 32-bit Adam 20 40 60 80 GPU Memory (GB) 50 60 70 80 Accuracy (%) BS = 16 BS = 16 BS = 16 BS = 2 K1 = 2 K1 = 2 K1 = 2 K1 = 4 K1 = 6 K1 = 8 68 70 72 74 80 81 82 83 84 85 K1 = 10 K1 = 4 BS = 4 BS = 10 TASK : RTE TASK : BoolQ TASK : MultiRC Zero-shot MeZO 16-bit SGD 16-bit Addax Figure 1: Left: Fine-tuning OPT-13B using SGD with in-place (IP) gradient updates and small batch sizes (BS) can outperform MeZO and Adam, while maintaining similar memory consumption to MeZO. Right: Fine- tuning OPT-13B using Addax with different ﬁrst-order batch size K1 outperforms zero-shot, MeZO and SGD. The ﬁgure reports the memory consumption and accuracy on three different datasets with zeroth-order batch size K0 ﬁxed at 12 for Addax. ﬁrst-order ﬁne-tuning methods like Adam and AdamW (Kingma & Ba, 2015; Loshchilov & Hutter, 2019), both in theory and practice. Furthermore, the ﬁnal performance of models ﬁne-tuned with MeZO fails to match the ones ﬁne-tuned with ﬁrst-order methods. Although Adam and AdamW have achieved great success in training deep learning models, the re- quirement to store optimizer states signiﬁcantly burdens memory consumption. In contrast, SGD is simpler and more memory-efﬁcient, and our experimental results demonstrate that ﬁne-tuning OPT- 13B using SGD with 16-bit ﬂoating-point calculations can outperform conﬁgurations that utilize Adam with 32-bit ﬂoating-point calculations (See Figure 1). The success of using SGD in ﬁne- tuning LLMs may be attributed to the smoothness of the parameter space in LLMs and the favorable conditions already established by the loss function (Hao et al., 2019). While SGD signiﬁcantly reduces memory consumption compared with Adam by eliminating the use of optimizer states, it is still non-comparable to the memory usage of MeZO. This is because the straightforward imple- mentation of SGD necessitates additional storage for activations used for calculating the ﬁrst-order gradient in backward propagation, as well as the gradients themselves. To further reduce the memory footprint for SGD, several studies have explored the utilization of in-place (IP) gradient update during backward propagation (Zhao et al., 2024; Lv et al., 2023). Instead of separating the backward propagation and weight update steps, which requires storing the gradients for all layers, they combine the two steps by updating the weights in each layer as soon as the gradients are calculated. IP update does not require storage of the gradients of all layers and, thus, signiﬁcantly reduces the memory requirement, making it comparable to MeZO for certain tasks. Our experiments, as illustrated in Figure 1, demonstrate that ﬁne-tuning LLMs with SGD employing in-place gradient updates and small batch sizes can attain memory consumption levels comparable to those achieved when ﬁne-tuning with MeZO. Therefore, in certain low-memory settings where MeZO can be applied, ﬁrst-order gradient updates can also be feasible for small batch sizes. Given the availability of both ﬁrst- and zeroth-order updates, we can ask the following natural question: Question: Can we develop an optimizer for ﬁne-tuning language models (LMs) that requires signiﬁcantly less memory than standard ﬁrst-order methods but still enjoys relatively fast con- vergence and produces high-quality ﬁne-tuned models? In this work, we answer this question by proposing Addax (ADDition of grAdient estimates through memory-efﬁcient eXecution), a method that has the beneﬁts of both worlds: i) being memory efﬁcient algorithm, ii) having fast convergence speed and iii) achieving the best performance across different ﬁne-tuning methods. Speciﬁcally, our contributions are: 1. Addax integrates MeZO with SGD to enhance MeZO’s convergence rate and the resulting ﬁnal model performance, while still keeping its memory consumption signiﬁcantly smaller than that of standard ﬁrst-order optimizers. 2 Published as a conference paper at ICLR 2024 SST-2 RTE CB BoolQ WSC WIC MultiRC COPA ReCoRD SQuAD DROP 20 40 60 80 100 Accuracy/F1 (%) Zero-shot MeZO 16-bit SGD 16-bit Addax 32-bit Adam Figure 2: Accuracy/F-1 score resulted from ﬁne-tuning OPT-13B model with zero-shot, MeZO, SGD, Addax, and Adam. Addax consistently outperforms other methods with GPU memory consumption comparable to that of MeZO (See Figure 3). The exact numbers are provided in Table 1. SST-2 RTE CB BoolQ WSC WIC MultiRC COPA ReCoRD SQuAD DROP 0 100 200 300 GPU Memory (GB) Zero-shot MeZO 16-bit SGD 16-bit Addax 32-bit Adam Figure 3: GPU memory requirement of ﬁne-tuning OPT-13B with different methods on various tasks. The exact numbers are provided in Table 1. 2. We establish convergence of Addax under a set of mild assumptions. We show that Addax achieves O(1/ √ T) convergence rate for non-convex smooth problems and the hyper-parameters for Addax are less restrictive than the Zeroth-order algorithms. Moreover, Addax’s convergence rate is independent of the model size. 3. Our experiments include a broad range of model architectures (e.g. masked LM and autoregres- sive LM), scales (ranging from 350M to 13B parameter models), and tasks (e.g., classiﬁcation, multiple-choice questions, and content generation). In experiments with RoBERTa-large, Addax consistently outperforms the performance of both zero-shot and MeZO across all tasks, and even outperforms the standard ﬁne-tuning methods in several tasks (See Figure 4). With OPT-13B, Addax outperforms competing methods in 10 distinct tasks, while consuming up to 11× less memory than standard ﬁne-tuning and requiring only 1.04 to 2.1× more memory than MeZO (See Figure 2 and Figure 3). 4. Further investigation reveals that Addax offers a versatile trade-off between resource availabil- ity and performance. In scenarios where memory is severely constrained, Addax is capable of being as memory efﬁcient as MeZO by using small ﬁrst-order batch sizes and using the rest for zeroth-order gradient estimates, thereby conserving memory of activations. Our experiments have shown that, under the same memory constraint as running SGD, Addax can outperform SGD with similar memory consumption in speciﬁc scenarios. (See Figure 1). Notations. We are interested in the optimization of a smooth (possibly non-convex) loss function L with parameter θ ∈Rd. In other words, we are interested in solving min θ∈Rd L(θ) := Ex∈D[ℓ(θ; x)], (1) where D denotes the data distribution and x ∈D denotes the samples. Throughout the paper, we mark the values related to zeroth- and ﬁrst-order gradient with (·)0, (·)1, respectively, and denote the iteration and coordinate indices as (·)t, (·)i, where t ∈{0, . . . , T}, i ∈{1, . . . , d}, respectively. 2 ADDAX: ALGORITHM DESIGN Addax uses both ﬁrst- and zeroth-order update rules for ﬁne-tuning. It is known that ﬁrst-order update rules lead to faster algorithms (than zeroth-order ones), but they require more memory. To exploit and balance the beneﬁts of both algorithms, Addax ﬁrst draws a random batch B0 (with 3 Published as a conference paper at ICLR 2024 Algorithm 1 Addax: ADDition of grAdient estimates through memory-efﬁcient eXecution 1: Input: θ, T, L, K0, K1, learning rates {ηt}, perturbation scale ϵ, weight parameter α ∈[0, 1] 2: for t ∈{0, 1, · · · , T −1} do 3: Randomly draw mini-batches B0, B1 uniformly from D with K0, K1 samples. 4: (g0, s) ←ZerothGrad(θ, L, B0, ϵ) (Algorithm 2) # Estimate zeroth-order gradient 5: g1 ← 1 K1 P x∈B1 ∇L(θ, x) # Estimate ﬁrst-order gradient 6: Reset random number generator with seed s 7: for i ∈{1, . . . , d} do 8: z ∼N(0, 1) 9: θi ←θi −ηt \u0000αzg0 + (1 −α)g1 i \u0001 # Update model parameters 10: Output: θ |B0| = K0) of data and a random search direction z ∈Rd. Then, it uses the drawn samples to obtain a stochastic zeroth-order estimate of the directional derivative of the objective function in the direction z at the point θ based on (Spall, 1992; Malladi et al., 2023): g0 = 1 K0 X x∈B0 ℓ(θ + ϵz; x) −ℓ(θ −ϵz; x) 2ϵ , where ϵ is some small constant. Then, it draws a random batch B1 (with |B1| = K1) of data and computes g1 = 1 K1 P x∈B1 ∇ℓ(θ; x). Finally, it updates the parameters of the model by: θ ←θ −η \u0000αzg0 + (1 −α)g1\u0001 , where η is the step-size and α ∈[0, 1] is a mixing constant for combining the two gradient estimates. We present the key steps of Addax in Algorithm 1, and leave the detailed discussions to Appendix A. The choice of mini-batch sizes K0, K1 controls the memory usage of Addax. In the presence of devices with larger available memory, we can choose larger values of K1, and when having less memory, we can reduce K1 (and possibly increase K0). The parameter α controls the balance between the zeroth- and ﬁrst-order methods. When α is close to 1, the algorithm behaves more similar to ZO-SGD. However, when α is close to 0, Addax behaves similar to SGD. Speciﬁcally, SGD is an extreme case of Addax when α = 0, and MeZO is another extreme case when α = 1. 3 ADDAX: THEORETICAL ANALYSIS This section presents our theoretical analysis for Algorithm 1. We present the informal statement of the convergence result here and relegate the formal statement and the proof to Appendix E. Theorem 3.1 (Informal). Assume that the loss L is Lipschitz smooth and the stochastic gradients are unbiased and have bounded variance, then by running Algorithm 1 with η = O(T −1/2) and ϵ = O(d−1/2T −1/4)), the output of the algorithm satisﬁes Et[∥∇L(θt)∥2] = O 1 √ T · r (1 −α)2 K1 + α2d K0 ! . Note that α balances the importance of the zeroth- and ﬁrst-order gradient in Algorithm 1, and we can optimize it to achieve an optimal convergence rate. It can be shown that the optimal α is given by α⋆= K0 K0+dK1 , leading to the convergence rate O \u0010q d T (K0+dK1) \u0011 . Moreover, compared with existing ZO algorithms, the condition for Algorithm 1 is less restrictive. For example, ZO- SGD requires smaller stepsizes ϵ = O(d−1T −1/2) and η = O(1/ √ dT). In contrast, our stepsizes η and ϵ can be much larger, i.e., when K0 K1 ≪d, the convergence rate of Algorithm 1 further reduces to O(1/ √ TK1), independent of the model size d. Our experiment veriﬁes our reasoning (See Appendix C.4 for hyper-parameters details). 4 EXPERIMENTS In this section, we compare the performance of Addax with several baselines, including i) zero-shot, ii) MeZO (Malladi et al., 2023), iii) SGD, and iv) Adam (Kingma & Ba, 2015), where zero-shot 4 Published as a conference paper at ICLR 2024 Table 1: Experiments on OPT-13B (with 1000 examples). Addax outperforms zero-shot, MeZO, SGD and Adam across the board on 10 tasks. For the accuracy of MeZO and 32-bit Adam, we report the results from Malladi et al. (2023). Task SST-2 RTE CB BoolQ WSC WIC MultiRC COPA ReCoRD SQuAD DROP Metrics Task type classiﬁcation multiple choice generation Accuracy/F1 (%) Zero-shot 58.8 59.6 46.4 59.0 38.5 55.0 46.9 80.0 81.2 46.2 14.6 32-bit Adam 92.0 70.8 83.9 77.1 63.5 70.1 71.1 79.0 74.1 84.9 31.3 MeZO 91.4 66.1 67.9 67.6 63.5 61.1 60.1 88.0 81.7 84.7 30.9 16-bit SGD 94.2 83.8 92.8 80.7 63.5 67.5 76.5 85.0 79.0 89.4 30.2 16-bit Addax 95.1 85.2 92.9 83.0 64.4 70.7 77.0 91.0 81.9 89.3 34.7 Batch Size 32-bit Adam 8 MeZO 16 16-bit SGD 16 10 8 4 16 16 2 16 10 4 2 (K1, K0) 16-bit Addax (4, 12) (10, 12) (8, 12) (2, 12) (8, 12) (2, 12) (2, 6) Memory (GB) 32-bit Adam 322.4 344.5 330.9 316.4 299.7 319.4 349.0 296.3 291.5 316.4 318.4 MeZO 28.2 37.0 42.5 48.6 29.5 28.8 72.8 28.2 37.2 49.8 75.1 16-bit SGD 55.8 72.4 80.6 70.6 60.4 62.0 76.4 53.2 79.8 79.3 69.0 16-bit Addax 29.3 45.1 53.4 71.2 37.3 37.8 72.9 28.8 46.8 51.1 158.1 SST-2 SST-5 SNLI MNLI RTE TREC 20 40 60 80 Accuracy (%) Zero-shot MeZO 32-bit Addax 16-bit Addax 32-bit Adam Figure 4: Experiments on RoBERTa-large: 16/32-bit Addax outperform zero-shot and MeZO across all tasks and outperform Adam in four out of six tasks. Detailed numbers can be found in Table 6. evaluates on the pre-trained models without any ﬁne-tuning. We implement Addax in both 16-bit (FP16) and 32-bit (FP32) data types. Experiment Settings: We conduct two sets of experiments: ﬁrstly, ﬁne-tuning the masked LM RoBERTa-large of (Liu et al., 2019) (350M) using zero-shot, MeZO, 32-bit Adam, and 16/32-bit Addax. Secondly, ﬁne-tuning OPT-13B with different algorithms to assess their performance and memory usage. We also explore the impact of hyper-parameters α and K1 K0+K1 on Addax’s perfor- mance, detailed in Appendix D.2. Further details can be found in Appendix C. Empirical Observation: Addax outperforms other baseline methods while using substantially less memory than Adam. For RoBERTa-large experiments, 16/32-bit Addax outperforms zero-shot and MeZO across six different tasks and surpasses Adam in four out of six tasks (Figure 4). In the experiments with OPT-13B model in Figure 2, 16-bit Addax outperforms zero-shot, MeZO, 16-bit SGD and Adam across 10 distinct tasks with moderate memory consumption. For example, Ad- dax consumes up to 11× less memory than standard ﬁne-tuning and requires only 1.04 to 2.1× more memory than MeZO (Figure 3). The batch size details for different algorithms can be found in Table 1. Notably, ﬁne-tuning OPT-13B using Addax with a smaller ﬁrst-order batch size K1 surpasses the performance of SGD with larger batch sizes. This suggests that the zeroth-order gradi- ent estimate in Addax provides stability (and regularization of the gradient) when K1 is small and effectively reduces memory usage. For additional experimental results, refer to Appendix D. 5"
            },
            {
                "section": "CONCLUSION",
                "content": "This paper introduces Addax, a memory-efﬁcient ﬁne-tuning method for Large Language Models (LLMs). By leveraging both ﬁrst- and zeroth-order stochastic gradient estimates, Addax demon- strates improved memory efﬁciency without sacriﬁcing convergence speed or model performance, as validated by our extensive experiments across various models, tasks, and datasets. 5 Published as a conference paper at ICLR 2024"
            },
            {
                "section": "results",
                "content": "in high accuracy. This indicates that the zeroth-order gradient estimate in Addax offers sta- bility and acts as gradient regularization when K1 is small, thereby efﬁciently decreasing memory consumption. In scenarios where memory constraints are not a concern, Addax can utilize a greater number of ﬁrst-order samples to improve performance. This reveals that Addax could offer a versa- tile trade-off between resource availability and performance (See Figure 1). D.5 THE EFFECT OF DIFFERENT BATCH SIZE ON THE PERFORMANCE OF 16-BIT SGD ON OPT-13B In this section, we demonstrate that, in general, larger batch sizes result in higher GPU memory consumption as well as improved accuracy for ﬁne-tuning the OPT-13B model with 16-bit SGD. We evaluate the best accuracy that 16-bit SGD can achieve across different tasks under the constraints of a single A100 GPU (80GB) on the OPT-13B model. We searched three different learning rates η = {5e −3, 1e −2, 5e −2} and evaluate the performance across different tasks using the different batch size of {2, 4, 6, 8, 10, 12, 14, 16}. The results are presented in Table 8. It is observed that for certain tasks, ﬁne-tuning experiments with larger batch sizes encounter CUDA out of memory errors. D.6 CONVERGENCE SPEED OF DIFFERENT TUNING METHODS ON THE OPT-13B MODEL In this section, we demonstrate that 16-bit Addax reaches a convergence speed comparable to 16-bit SGD, despite SGD using 4× more ﬁrst-order samples for backward propagation. Meanwhile, Addax 2We ﬁx K0 = 6 and use two A100 GPUs for this dataset. 14 Published as a conference paper at ICLR 2024 Best α 90.6 89.9 90.1 89.9 89.9 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 90.6 89.9 90.0 89.4 89.4 88.4 89.3 89.4 89.6 88.3 88.4 85.0 89.1 89.4 89.6 88.4 89.4 89.1 88.9 89.7 88.8 88.5 89.3 89.9 89.9 88.8 89.7 88.6 88.5 89.9 88.5 81.8 89.1 88.0 89.4 85.1 89.7 90.1 89.4 88.2 82 84 86 88 90 Accuracy (%) (a) Task: SST-2 Best α 40.3 44.7 41.7 46.3 49.1 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 35.6 41.3 38.8 41.5 45.8 36.0 38.4 38.8 41.9 46.3 36.5 41.1 33.9 46.3 44.6 36.5 37.0 39.0 44.7 44.6 36.5 42.8 38.9 42.9 43.7 28.4 38.0 41.7 44.9 45.1 38.0 34.6 41.0 43.2 49.1 40.3 44.7 38.1 40.1 45.5 30 35 40 45 Accuracy (%) (b) Task: SST-5 Best α 74.4 74.5 79.3 77.3 78.5 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 61.0 73.1 79.2 72.8 73.7 63.7 73.6 79.3 73.4 74.5 53.9 73.9 75.8 75.2 78.1 35.7 73.4 68.0 77.3 74.1 34.7 74.5 69.8 76.7 74.2 74.4 69.4 70.2 75.9 74.7 71.1 71.7 74.0 72.3 78.5 68.6 68.1 74.6 75.4 75.9 40 50 60 70 Accuracy (%) (c) Task: SNLI Best α 68.0 69.9 68.3 69.9 68.7 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 61.5 67.8 65.1 68.9 68.3 65.1 69.9 64.9 59.3 68.3 46.5 63.7 66.6 67.2 68.2 68.0 68.1 65.6 68.4 68.2 57.5 67.3 68.3 67.4 67.0 65.9 65.1 65.9 66.5 68.7 65.8 64.7 64.7 69.9 64.4 63.9 68.6 66.2 65.0 68.6 50 55 60 65 Accuracy (%) (d) Task: MNLI Best α 61.7 63.5 60.6 64.3 62.5 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 57.8 58.5 60.6 58.5 62.5 60.3 62.8 56.7 58.1 59.9 57.4 56.7 59.6 57.0 62.5 61.7 63.5 59.2 61.0 60.6 59.9 62.8 59.6 59.6 59.6 53.4 58.1 58.8 58.8 61.0 58.5 58.1 57.4 58.5 59.6 61.0 58.1 57.0 64.3 59.6 55.0 57.5 60.0 62.5 Accuracy (%) (e) Task: RTE Best α 36.2 73.6 86.2 88.2 89.6 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 28.6 27.6 64.6 87.8 80.6 28.6 72.8 54.2 88.2 29.8 27.6 66.6 63.0 63.4 81.6 28.8 45.2 86.2 84.8 85.6 36.2 30.6 51.2 85.2 68.2 27.6 28.0 36.2 86.2 89.6 27.6 28.0 79.8 86.8 82.2 27.6 73.6 85.6 85.6 82.8 40 60 80 Accuracy (%) (f) Task: TREC Figure 5: The accuracy (%) of the 32-bit Addax across different tasks on the RoBERTa-large model, with variable combinations of α and K1 K1+K0 . requires only 1.04 to 2.1× more memory compared to MeZO. The comparison of convergence speeds across the three methods is illustrated in Figure 7. For MeZO and SGD, the batch size is set to 16, while for Addax, we conﬁgure (K1, K0) as (4, 12). The learning rate for Addax is set at η = 1e −4. For SGD, the learning rates are η = {5e −3, 1e −2, 5e −2}. For MeZO, we utilize learning rates of η = {1e −6, 1e −7}. We select the hyper-parameters that yield the best validation accuracy across three methods. We utilize a single A100 GPU (80GB total) for running both Addax and MeZO, whereas SGD requires two A100 GPUs (160GB total). MeZO requires signiﬁcantly more steps (20K steps) to converge compared to Addax and SGD (1K steps). Addax with smaller ﬁrst-order batch size K1 = 4 achieves a convergence speed similar to SGD with a batch size of 16, despite requiring signiﬁcantly less memory. 2Running 16-bit SGD in task BoolQ with batch size of 16 will encounter CUDA out of memory error. We report the training loss of 16-bit SGD with batch size of 4 here. 15 Published as a conference paper at ICLR 2024 Best α 84.4 89.6 90.7 89.1 91.4 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 83.6 88.5 88.5 87.7 90.0 82.8 87.6 89.2 89.0 90.5 83.1 88.9 89.1 87.5 90.7 84.4 88.8 89.8 87.7 90.7 80.5 88.5 89.3 87.8 90.8 83.0 89.0 89.9 88.6 90.6 82.0 88.2 89.1 89.0 90.9 83.3 89.6 90.7 89.1 91.4 82.5 85.0 87.5 90.0 Accuracy (%) (a) Task: SST-2 Best α 41.3 41.8 50.4 48.1 48.2 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 38.5 37.4 50.4 46.2 46.2 32.1 41.8 39.8 48.1 44.0 32.2 33.3 43.6 34.0 46.1 29.0 41.1 47.1 33.8 46.2 41.3 39.5 46.4 47.6 48.2 31.4 27.6 44.9 42.2 47.5 36.9 27.6 41.8 47.2 45.9 36.1 33.8 43.0 45.0 44.0 30 35 40 45 50 Accuracy (%) (b) Task: SST-5 Best α 79.3 74.9 76.3 78.2 76.7 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 44.3 73.3 74.5 78.0 72.6 73.9 74.1 75.9 78.2 75.5 57.4 74.9 76.3 68.1 75.6 67.8 73.2 74.9 77.8 76.1 79.3 71.4 75.6 76.1 76.7 44.3 73.7 71.8 76.1 74.4 76.7 72.4 72.8 77.7 75.7 74.2 69.5 75.3 75.7 75.0 50 60 70 Accuracy (%) (c) Task: SNLI Best α 61.7 66.9 66.1 67.7 68.2 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 47.7 61.8 63.5 66.9 65.8 49.8 66.0 60.5 66.3 62.1 61.7 63.8 65.9 63.9 68.2 58.3 65.6 66.1 66.7 61.8 55.8 64.9 65.4 67.4 65.8 48.4 66.1 65.7 67.7 65.1 57.6 66.9 65.8 65.0 66.2 61.0 64.3 65.5 66.9 66.6 50 55 60 65 Accuracy (%) (d) Task: MNLI Best α 67.1 65.7 66.8 66.8 66.4 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 64.3 65.7 63.9 62.8 66.1 62.1 61.7 64.6 63.2 63.5 62.5 61.0 65.3 66.8 66.4 67.1 63.5 62.8 66.1 60.6 62.8 65.3 63.2 63.9 63.9 63.9 65.3 61.4 62.8 66.1 57.8 58.1 62.1 60.3 65.0 63.2 61.4 66.8 64.6 65.7 58 60 62 64 66 Accuracy (%) (e) Task: RTE Best α 71.0 86.6 89.0 90.8 89.0 0.1 0.2 0.3 0.4 0.5 K1 K0+K1 0.0003 0.001 0.003 0.004 0.005 0.007 0.01 0.1 α 31.0 86.6 89.0 82.8 84.0 39.2 56.4 56.2 58.4 76.0 47.2 80.0 66.6 68.8 83.8 71.0 40.8 77.4 83.8 84.0 35.4 67.4 82.8 52.0 79.6 69.2 31.4 82.2 79.4 75.0 37.8 29.8 86.8 82.8 84.6 27.6 84.0 75.0 90.8 89.0 40 60 80 Accuracy (%) (f) Task: TREC Figure 6: The accuracy (%) of the 16-bit Addax across different tasks on the RoBERTa-large model, with variable combinations of α and K1 K1+K0 . E PROOF OF THEOREM 3.1 Assumption E.1. L(θ; x) is L-Lipschitz smooth, i.e., ∥∇L(θ; x) −∇L(θ′; x)∥≤L ∥θ −θ′∥, ∀θ, θ′ ∈Rd, x ∈D. Assumption E.2. The stochastic gradient is unbiased and has bounded variance, i.e., Ex[∇L(θ; x)] = ∇L(θ), Ex[∥L(θ; x) −L(θ)∥2] ≤σ2, ∀θ ∈Rd. Lemma E.3 (Gao et al., 2014, Lemma 4.1 (b)). Suppose Assumption E.1 holds, then the expected gradient estimated with SPSA is a biased estimation of ∇L(θ) and satisﬁes EB[ ˆ∇L(θ; B)] −∇L(θ) 2 ≤ϵ2L2d2 4 . 16 Published as a conference paper at ICLR 2024 Table 7: Fine-tuning results of 16-bit Addax using one A100 (80GB) with different K1 on different tasks with the OPT-13B model. We ﬁx K0 of 16-bit Addax at 12. * means the ﬁne-tuning task encounters CUDA out of memory error. Task Metric Values K1 2 4 6 8 10 12 14 16 SST-2 GPU Memory (MB) 28617 29341 30985 32493 34207 36175 38799 40645 Accuracy (%) 93.4 95.1 94.5 94.6 94.9 94.6 94.5 95.0 RTE GPU Memory (MB) 35285 45117 53969 63663 73481 * * * Accuracy (%) 79.2 85.2 84.8 85.2 84.5 * * * CB GPU Memory (MB) 37757 53407 69671 79521 * * * * Accuracy (%) 89.3 92.9 92.9 89.3 * * * * BoolQ GPU Memory (MB) 45937 71171 * * * * * * Accuracy (%) 81.6 83.0 * * * * * * WSC GPU Memory (MB) 29391 30251 32415 34919 37269 38737 41009 43085 Accuracy (%) 63.5 63.5 63.5 63.5 64.4 63.5 63.5 55.5 WIC GPU Memory (MB) 28885 30253 32493 34789 37929 40551 42739 46871 Accuracy (%) 67.4 66.5 68.7 70.7 68.3 69.0 70.5 68.0 MultiRC GPU Memory (MB) 72885 * * * * * * * Accuracy (%) 77.0 * * * * * * * COPA GPU Memory (MB) 28411 28615 28597 28849 28811 28835 29003 28803 Accuracy (%) 89.0 90.0 90.0 91.0 91.0 90.0 90.0 91.0 ReCoRD GPU Memory (MB) 35823 46777 57567 70199 79527 * * * Accuracy (%) 81.1 81.9 81.7 82.1 81.5 * * * SQuAD GPU Memory (MB) 51131 78981 * * * * * * F1 (%) 89.3 89.0 * * * * * * DROP2 GPU Memory (MB) 158180 * * * * * * * F1 (%) 34.7 * * * * * * * 0 1000 2000 3000 4000 5000 Steps 0.0 0.2 0.4 0.6 0.8 Loss MeZO (37.0 GB) 16-bit Addax (45.1 GB) 16-bit SGD (105.9 GB) (a) Task: RTE 0 500 1000 1500 2000 Steps 0.00 0.25 0.50 0.75 1.00 Loss MeZO (42.5 GB) 16-bit Addax (53.4 GB) 16-bit SGD (151.9 GB) (b) Task: CB Figure 7: Convergence speed of three ﬁne-tuning methods (16-bit Addax, MeZO, and 16-bit SGD) on two ﬁne-tuning datasets with the OPT-13B model. We set the batch size to 16 for MeZO and SGD and ﬁx (K1, K0) = (4, 12) for Addax. We utilize a single A100 GPU (80GB total) for running both Addax and MeZO, whereas SGD requires two A100 GPUs (160 GB total) to run with BS = 16. MeZO requires signiﬁcantly more steps to converge compared to Addax and SGD. Addax with 4× less ﬁrst-order samples achieves a convergence speed similar to SGD, despite requiring signiﬁcantly less memory. Lemma E.4. Suppose Assumption E.1 holds, then the variance of the gradient estimated with SPSA satisﬁes Var( ˆ∇L(θ; B)) = EB \u0014\r\r\rEB[ ˆ∇L(θ; B)] −ˆ∇L(θ; B) 2\u0015 ≤d K σ2. 17 Published as a conference paper at ICLR 2024 Table 8: Fine-tuning results of 16-bit SGD using one A100 (80GB) with different batch size on dif- ferent tasks with the OPT-13B model. We can see that in general SGD achieves better performance with more memory and larger batch size. * means the ﬁne-tuning task encounters CUDA out of memory error. Task Metric Batch Size Batch Size (BS) 2 4 6 8 10 12 14 16 SST-2 GPU Memory (MB) 51427 52113 52859 53115 53703 54671 54409 55771 Accuracy (%) 93.5 93.5 94.8 94.6 94.4 94.7 94.2 94.2 RTE GPU Memory (MB) 53901 62799 69189 79055 72389 * * * Accuracy (%) 79.8 80.8 81.2 84.5 83.8 * * * CB GPU Memory (MB) 56239 69359 77005 80595 * * * * Accuracy (%) 91.0 89.3 92.8 92.8 * * * * BoolQ GPU Memory (MB) 65387 70625 * * * * * * Accuracy (%) 78.8 80.7 * * * * * * WSC GPU Memory (MB) 52093 53391 54497 57211 58833 60719 62515 60413 Accuracy (%) 63.4 63.4 63.5 63.5 63.5 65.4 63.5 63.5 WIC GPU Memory (MB) 52297 53979 55689 58585 57185 58599 60513 61943 Accuracy (%) 60.5 64.9 67.5 63.3 66.6 68.5 68.0 67.5 MultiRC GPU Memory (MB) 79929 * * * * * * * Accuracy (%) 76.4 * * * * * * * COPA GPU Memory (MB) 51081 51367 51407 51885 52219 52615 52873 53157 Accuracy (%) 86.0 79.0 79.0 90.0 81.0 89.0 90.0 85.0 ReCoRD GPU Memory (MB) 55539 60063 69169 77007 79821 * * * Accuracy (%) 80.6 81.4 79.3 78.5 79.0 * * * SQuAD GPU Memory (MB) 60499 79275 * * * * * * F1 (%) 87.7 89.4 * * * * * * DROP GPU Memory (MB) 68964 * * * * * * * F1 (%) 30.2 * * * * * * * Theorem E.5. Under Assumptions E.1, E.2, by running Algorithm 1 for T iterations with ϵ ≤, ηt ≤η ≤, ∀t, the output satisﬁes E[∥∇L(θt)∥2] ≤4(L(θ0) −L⋆) ηT(2 −α) + α(1 + α −α2/2)ϵ2L2d2 2(2 −α) + 4ηL (2 −α) \u0012(1 −α)2 2K1 + α2d 2K0 \u0013 σ2. (2) Proof: By Assumption E.1: Et[L(θt+1)] ≤L(θt) + Et[⟨∇L(θt), θt+1 −θt⟩] + L 2 Et[∥θt+1 −θt∥2] (a) = L(θt) −ηt D ∇L(θt), (1 −α)∇L(θt) + αEB0[ ˆ∇L(θt; B0)] E + Lη2 t 2 (1 −α)∇L(θt) + αEB0[ ˆ∇L(θt; B0)] 2 + Lη2 t (1 −α)2 2 EB1[ ∇L(θt) −∇L(θt; B1) 2] + Lη2 t α2 2 Var( ˆ∇L(θt; B0)) (b) ≤L(θt) −(1 −α)ηt ∥∇L(θt)∥2 −αηt D ∇L(θt), EB0[ ˆ∇L(θt; B0)] E + Lη2 t 2 (1 −α)∇L(θt) + αEB0[ ˆ∇L(θt; B0)] 2 + Lη2 t (1 −α)2 2K1 σ2 + Lη2 t α2d 2K0 σ2, (3) 18 Published as a conference paper at ICLR 2024 where (a) substitutes the update of θ and takes expectations to g0, g1;(b) follows from the Lemma E.4. The third term on the Right-Hand-Side (RHS) can be further bounded as −αηt D ∇L(θt), EB0[ ˆ∇L(θt; B0)] E (a) = −αηt 2 ∥∇L(θt)∥2 −αηt 2 EB0[ ˆ∇L(θt; B0)] 2 + αηt 2 ∇L(θt) −EB0[ ˆ∇L(θt; B0)] 2 (b) ≤−αηt 2 ∥∇L(θt)∥2 −αηt 2 EB0[ ˆ∇L(θt; B0)] 2 + αηtϵ2L2d2 8 , (4) where (a) uses the fact that ∥u + v∥2 = ∥u∥2 + ∥v∥2 + 2 ⟨u, v⟩; (b) applies Lemma E.3 to the last term. The fourth term on the RHS of equation 3 can be bounded as Lη2 t 2 (1 −α)∇L(θt) + αEB0[ ˆ∇L(θt; B0)] 2 = Lη2 t 2 ∇L(θt) + α \u0010 EB0[ ˆ∇L(θt; B0)] −∇L(θt) \u0011 2 (a) ≤Lη2 t ∥∇L(θt)∥2 + α2Lη2 t \u0010 EB0[ ˆ∇L(θt; B0)] −∇L(θt) \u0011 2 (b) ≤Lη2 t ∥∇L(θt)∥2 + α2η2 t ϵ2L3d2 4 , (5) where (a) applies Cauchy-Schwarz inequality; (b) applies Lemma E.3 to the last term. Substitute equation 4, equation 5 back to equation 3, we have Et[L(θt+1)] ≤L(θt) −(1 −α 2 −Lηt)ηt ∥∇L(θt)∥2 −αηt 2 EB0[ ˆ∇L(θt; B0)] 2 + αηtϵ2L2d2(1 + 2αηtL) 8 + Lη2 t (1 −α)2 2K1 σ2 + Lη2 t α2 2 σ0. (6) Choose ηt ≤2−α 4L , we have 1 −α 2 −Lηt ≥2−α 4 > 0, 1 + 2αηtL ≤1 + α −α2 2 and (2 −α)ηt 4 ∥∇L(θt)∥2 + αηt 2 EB0[ ˆ∇L(θt; B0)] 2 ≤L(θt) −Et[L(θt+1)] + αηtϵ2L2d2(1 + α −α2/2) 8 + Lη2 t (1 −α)2 2K1 σ2 + Lη2 t α2 2 σ0. (7) Sum from t = 0 to T, we have T X t=0 \u0012(2 −α)ηt 4 E[∥∇L(θt)∥2] + αηt 2 E \u0014\r\r\rEB0[ ˆ∇L(θt; B0)] 2\u0015\u0013 ≤L(θ0) −E[L(θT +1)] + T X t=0 ηt · α(1 + α −α2/2)ϵ2L2d2 8 + T X t=0 η2 t · \u0012L(1 −α)2 2K1 σ2 + Lα2d 2K0 σ2 \u0013 . (8) Choose ηt = η ≤2−α 4L , ∀t, and divide both side by (2−α)ηT 4 , we have E[∥∇L(θt)∥2] + 2α 2 −αE \u0014\r\r\rEB0[ ˆ∇L(θt; B0)] 2\u0015 ≤4(L(θ0) −L⋆) ηT(2 −α) + α(1 + α −α2/2)ϵ2L2d2 2(2 −α) + 4ηL (2 −α) \u0012(1 −α)2 2K1 + α2d 2K0 \u0013 σ2, (9) which completes the proof. Corollary E.6. By choosing η = min ( 2−α 4L , r 2(L(θ0)−L⋆) T Lσ2 ( (1−α)2 K1 + α2d K0 ) ) and ϵ ≤ 2L(θ0) −L⋆)σ2 \u0000(1 −α)2/K1 + α2d/K0\u0001 T !1/4 · 1 L3/4d p α(1 + α −α2/2) , 19 Published as a conference paper at ICLR 2024 Convergence Memory Figure 8: An illustration for the memory-convergence trade-off of Addax. The x-axis represents the expected gradient size after a ﬁxed number of iterations. Algorithm 1 converges with rate E[∥∇L(θt)∥2] ≤5 √ 2L · q (1−α)2 K1 + α2d K0 2 −α · σ r L(θ0) −L⋆ T = O 1 √ T · r (1 −α)2 K1 + α2d K0 ! Remark 1. [Trade-off between convergence and memory] We obtain a trade-off between the conver- gence speed and the memory cost of Algorithm 1 as follows: For a model with parameters θ ∈Rd, the memory cost for estimating a zeroth-order gradient on one sample with SPSA is ≈d, and for estimating a ﬁrst-order gradient on one sample is ≈2d. Then we have the following trade-off shown in Figure 8 20"
            }
        ]
    },
    "Paper_6": {
        "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning Qitao Tan 1 Jun Liu 2 Zheng Zhan 2 Caiwen Ding 3 Yanzhi Wang 2 Jin Lu 1 Geng Yuan 1",
        "sections": [
            {
                "section": "Abstract",
                "content": "Large language models (LLMs) excel across vari- ous tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly lim- iting real-world deployment. Recently, zeroth- order (ZO) optimization stood out as a promis- ing memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both conver- gence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learn- ing capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude up- dates precisely scaled to layer-wise individual op- timization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48% on var- ious datasets. Moreover, DiZO consistently out- performs the representative ZO baselines in fine- tuning RoBERTa-large, OPT-series, and Llama- series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning."
            },
            {
                "section": "Introduction",
                "content": "Fine-tuning pre-trained large language models (LLMs) with backpropagation demonstrates superior performance for many natural language processing tasks (Yang et al., 2019; Liu et al., 2019; Talmor et al., 2018; Chowdhery et al., 2023; Zheng et al., 2020). However, the extensive parameteriza- 1University of Georgia 2Northeastern University 3University of Minnesota Twin Cities. Correspondence to: Geng Yuan <geng.yuan@uga.edu>. tion imposes a substantial memory burden, limiting their practicality for widespread downstream applications. In line with the neural scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020), next-generation LLMs continue to increase in parameter count. Specifically, model sizes are expanding at a rate of 410× every two years, dramatically outpacing the scaling of DRAM bandwidth (1.4× every two years) and DRAM capacity (2× every two years). This disparity leads to the memory wall challenge (Gholami et al., 2024), which becomes even more severe when deploying LLMs on memory-limited devices (Zeng et al., 2024; Chen et al., 2024; Hur et al., 2023). Recently, zeroth-order (ZO) optimization has emerged as a promising memory-efficient training paradigm for LLM fine-tuning, attracting significant attention (Zhang et al., 2024; Liu et al., 2024; Malladi et al., 2023; Zhao et al., 2024). By relying solely on forward passes (i.e., inference) to estimate gradients and update model parameters, ZO by- passes the need for backward propagation and significantly reduces extensive storage requirements for activations, gra- dients, and optimizer states. As reported in Malladi et al. (2023), fine-tuning LLMs via ZO optimization reduces up to 12× memory cost. Nevertheless, ZO optimization still exhibits a gap in convergence speed and accuracy compared to the conventional first-order (FO) method (i.e., compute gradient via backpropagation). As shown in Table 1, one can observe that the FO method substantially outperforms ZO method in both accuracy and GPU hours. Though ZO method achieves higher throughput due to its computational simplicity, it requires more than 10× iterations for conver- gence, dramatically increasing GPU hours. Previous studies typically attribute this gap to the fact that ZO optimization leverages random perturbation for gradient estimation, and thus results in unavoidable estimation error, but without fur- ther exploration of other underlying causes (Malladi et al., 2023; Gautam et al., 2024; Zhao et al., 2024). To bridge this gap, we begin by examining the distinct update patterns shown by ZO and FO methods during LLM fine-tuning. Our analysis reveals a substantial dif- ference in their layer-wise update magnitudes. Specifically, ZO method relies on high-dimensional random search and tends to apply uniform-magnitude updates without consid- 1 arXiv:2502.03304v1  [cs.LG]  5 Feb 2025 ering layer-wise individual characteristics. In contrast, FO method benefits from fine-grained gradient estimation and applies diverse-magnitude updates precisely scaled to the layer-wise individual optimization needs. Motivated by these observations, we are interested in investigating: if we can also provide ZO with diverse-magnitude updates, effectively achieving training acceleration and accuracy improvement. Drawing on these insights, we propose Divergence- driven Zeroth-Order optimization (DiZO). DiZO conducts divergence-driven layer adaptation by incorporating pro- jections, enabling layer-wise adaptive updates that closely resemble FO approaches. Notably, the projections can be optimized without gradients, ensuring that DiZO retains the appealing backpropagation-free features. Moreover, we vali- date DiZO on a variety of tasks, including classification and generation, using several LLMs such as RoBERTa-large, the OPT series, and the Llama series. Experimental results show that DiZO substantially decreases training iterations for con- vergence while maintaining throughput, cutting training GPU hours by up to 48% on diverse datasets. Furthermore, our method can be seamlessly integrated with parameter- efficient tuning techniques like low-rank adaptation (Hu et al., 2021) for additional speedups. DiZO also consistently outperforms the representative ZO baselines and, in some cases, surpasses memory-intensive FO fine-tuning. The summary of our contributions is as follows: • We introduce a novel layer-wise divergence analysis to uncover the fundamental differences in the updating patterns of FO and ZO methods. • We introduce DiZO, a novel ZO method using divergence-driven layer adaptation, achieving a learn- ing capacity closely resembling FO while maintaining the throughput benefit of ZO optimization. • DiZO consistently exceeds existing baselines in both accuracy and GPU hours, and it can be seamlessly integrated with LoRA for additional benefits. These advantages hold across diverse tasks and LLM archi- tectures. 2. Preliminaries and Pattern Analysis 2.1. Revisiting Zeroth-order Optimization Recently, ZO optimization has gained significant attention in machine learning (Verma et al., 2023; Dhurandhar et al., 2019; Wang et al., 2022; Gu et al., 2021). Unlike con- ventional FO optimization, which calculates gradients via backpropagation, ZO optimization estimates gradients us- ing only objective oracles via finite differences (Chen et al., 2023; Liu et al., 2018; Ye et al., 2018). This property can Table 1. Fine-tuning results on SST-2 datasets. Although ZO method shows advantages in memory saving, left behind FO method in terms of both accuracy and GPU hours. Model Type Acc. Memory #Train Iter. GPU Hours RoBERTa FO 91.9 9.2 GB 6.6% 12.3% ZO 90.5 4.5 GB 100.0% 100.0% OPT-2.7B FO 94.2 45.4 GB 7.5% 16.8% ZO 90.0 6.8 GB 100.0% 100.0% be leveraged for LLM fine-tuning to alleviate the extensive memory costs. Specifically, as ZO only needs two forward passes to obtain the estimated gradients, it avoids comput- ing and storing the most memory-consuming information needed in the conventional FO training, i.e., activations in the forward process, gradients in the backward process, and the optimizer state. The core idea of ZO optimization is to estimate gradients by applying random perturbations to the weights and comput- ing differences in the objective. For a mini-batch of data B, sampled from a labeled dataset D = {xi, yi}|D| i=1, a model with parameters θ ∈Rd, where d represents the dimension of the parameter space, and the corresponding loss function L(θ; B). The gradient is estimated as follows: ∇L(θ; B) = 1 q q X i=1 \u0014L (θ + ϵui; B) −L (θ −ϵui; B) 2ϵ ui \u0015 (1) where ui is a random vector with the same dimension as the model weights and is typically drawn from standard Gaus- sian distribution N(0, I) (Malladi et al., 2023; Zhang et al., 2024), or from Gaussian sampling over a unit sphere (Liu et al., 2018; Shamir, 2017), q is the number of objective queries, and ϵ > 0 is a small perturbation scalar for smooth- ing. Given the learning rate η and the mini-batch data Bt at t-th iteration, once the estimated gradient ∇L(θ; Bt) is obtained, then ZO-SGD updates the parameters with the following rule: θt+1 = θt −η∇L(θt; Bt) (2) 2.2. Layer-wise Divergence Analysis Drawing insight from the update formula of ZO optimiza- tion, we notice that ZO method applies uniform-magnitude updates across layers, e.g., the L2-norm of the updates is about the same for all layers in one iteration (see Appendix E for proof). This fact may be the root of the inferior perfor- mance of ZO optimization. To measure how the divergence of update magnitude affects the convergence speed and ac- curacy, we investigate the training dynamics of ZO and FO"
            },
            {
                "section": "methods",
                "content": ", respectively. 2 Divergence  Increases ZO layer-wise divergence FO layer-wise divergence Iter0: 58.8 (Acc) Iter1000: 85.0 Iter2000: 90.0 Iter0: 58.8 Iter20: 88.2 Iter40: 93.0 +1000× +1000× +20× +20× +50× +50× ZO Updates ZO Updates FO Updates FO Updates Iter0: 58.8 Iter50: 60.0 Iter100: 62.0 Figure 1. Comparison of the training dynamics of ZO and FO methods. The X-axis represents layer names, and the Y-axis represents the distance gap. Although they converge to different stable states, the divergence of the distance gap increases in both FO and ZO methods during training. FO accumulates divergence rapidly through diverse-magnitude updates, while ZO applies uniform-magnitude updates, requiring more iterations for an ideal divergence level. Results are obtained by fine-tuning OPT-2.7B on the SST-2 dataset, focusing on weights in the attention module: K (Key), V (Value), Q (Query), and O (Output projection). Analysis indicator. To quantify the effect of updates, we adapt the layer-wise L2-norm distance gap between the weights of the pre-trained and the fine-tuned model as an indicator. The layer-wise L2-norm distance gap is defined as: ∥∆θ(ℓ) t ∥= ∥θ(ℓ) t −θ(ℓ) 0 ∥2 (3) where t is t-th fine-tuning iteration, ℓis ℓ-th layer of the model, and θ(ℓ) 0 indicates the weights of ℓ-th layer of pre- trained model. Analysis result. Figure 1 compares the training dynam- ics of FO and ZO methods. Regardless of whether ZO or FO is used, the divergence of distance gap among layers grows during training, i.e., the line of distance gap gradually ’bends’. This pattern implies that different layers benefit from maintaining diverse distance gaps with the pre-trained model. However, FO and ZO diverge in how the distance gap divergence is accumulated. FO employs fine-grained gradient estimations, resulting in diverse-magnitude updates (FO updates in Figure 1). Therefore, it can rapidly reach the desired layer-wise distance gap in only a few iterations. In contrast, ZO relies on random search in high-dimensional parameter space and generates uniform-magnitude updates (ZO updates in Figure 1), resulting in thousands more iter- ations required for accumulating a meaningful layer-wise distance gap. With the above findings, we suspect the inferior performance of ZO stems from its inability to deliver layer-wise adaptive updates, a challenge that arises from its reliance on random perturbations for gradient estimation. Algorithm 1 Divergence-diven ZO Optimization (DiZO) Require: parameter of t-th iteration θt and pre-trained model θ0, loss function L, step budget T, perturbation scalar ϵ, mini-batch data Bt, learning rate η, projection at t-th iteration γt = {γi t}L i=1 for t = 1 to T do ∇L = GradEst(θt, ϵ, Bt) θt = θt−1 −η∇L γ∗ t = arg minγt L(θ0 + γt ∥∆θt∥∆θt; Bt) θt = ApplyProjection(θt, θ0, γ∗ t ) end Subroutine GradEst(θ, ϵ, B): Sample: u1, . . . , uq ∽N(0, I) Query: yi = L(θ + ϵui; B) −L(θ −ϵui; B) Estimator: ∇L = q 2ϵ Pq i=1 yiui return ∇L return Subroutine ApplyProjection(θt, θ0, γt): for ℓ= 1, 2, . . . , L do // Project l-th layer θ(ℓ) t = θ(ℓ) 0 + γ(ℓ) t ∥∆θ(ℓ) t ∥∆θ(ℓ) t end return θt return 3. Methodology We find that ZO applies uniform-magnitude updates for all layers, which could be the root of its inferior perfor- mance in accuracy and convergence speed. Consequently, we introduce a variant of ZO optimization which per- forms divergence-driven layer adaptation, thereby providing diverse-magnitude updates to enhance the overall learning capacity. 3 3.1. Design of the Divergence-driven Layer Adaptation To provide layer-wise adaptive updates for ZO optimization, we apply projections to the updates of different layers, gen- erating updates with diverse magnitudes. The pseudocode for the proposed method is shown in Algorithm 1. Specifically, We treat training iteration as a two-step process that iteratively updates the weights and the projection factor. Our approach involves two key steps performed in an alter- nating manner. First, we perform vanilla ZO optimization as defined in Eq. (2). Second, we identify the ideal projections for the weights and apply them, generating the projected weights. Formally, we define the ideal projection learning as solving the following minimization problem: min γt L(θ0 + γt ∥∆θt∥∆θt; Bt) (4) where γt = {γ(ℓ) t }L ℓ=1 is a projection vector at t-th iteration, and L is the number of layers. While searching for the ideal projection, we freeze the model weights and use the same mini-batch data Bt that is employed for the main ZO weight fine-tuning. After finding the ideal projection for the t-th ZO step, we project the weights as: θt = θ0 + γt ∥∆θt∥∆θt (5) where we get the new θt after projection, and then we use the projected one for the following fine-tuning. When the value of γt is larger than ∥∆θt∥, enlarges the distance gap between the fine-tuned model and the pre-trained model , and vice versa. 3.2. How to Learn the Projection? Although promising, finding the ideal projection (defined in Eq. (4)) remains challenging due to the high complexity of the objective. A straightforward solution is to also perform backpropagation for gradient computation and optimize the projection accordingly (FO-based method). For example, we use Adam optimizer to directly update γt. The results are shown in Table 2, one can observe that it significantly reduces 67.7% of the iteration and 58.5% of the training GPU hours, and increases by 3.4% in accuracy. These"
            },
            {
                "section": "results",
                "content": "underscore the effectiveness of incorporating our proposed divergence-driven layer adaptation. However, searching projection with the FO method makes DiZO only partially gradient-free. Specifically, while the model weights are updated via ZO, the per-layer projec- tion parameter γ(ℓ) t is updated via FO, which still requires the backward pass and storing memory-intensive activation. The only memory saving, compared to the vanilla FO fine- tuning, is the optimizer state. As a result, relying on FO to Table 2. Fine-tuning OPT-2.7B on SST-2 dataset. ●: partial gradient-free; DiZO†: learning projection by FO method; Task Type Gradient Free Acc. #Train Iter. GPU Hours MeZO ✓ 90.0 100% 100% DiZO† (w. FO) ● 93.4 33.3% 41.5% FT ✗ 94.2 9.3% 16.8% find the ideal projection, though it achieves faster conver- gence speed and better accuracy in ZO optimization, offers limited overall benefit. It is worth noting that the peak mem- ory usage during training of the FO-based DiZO is similar to that of low-rank adaptation (LoRA) (Hu et al., 2021). Is the projection-based method for enhancing layer-wise divergence in ZO a failed idea that seems promising at first glance but is actually not after deliberation? Fortunately, the answer is no. We develop a ZO projection learning algo- rithm, which retains the memory-efficient advantages and also achieves training acceleration and accuracy enhance- ment. 3.3. Projection Learning by Zeroth-order Optimization Our major goal is to find the ideal projection for adaptive updates while avoiding memory-intensive backpropagation. One potential promising solution is to also utilize the ZO method to update the projection. We estimate the gradient and update the projection as: ∇bL(γt; θt) = \" bL (γt + ϵu; θt) −bL (γt −ϵu; θt) 2ϵ u # (6) γt,j+1 = γt,j −η∇bL(γt; θt) (7) where u ∈RL is a random vector from N(0, I), bL is the objective defined in Eq. (4). However, naively applying vanilla ZO optimization for the sub-optimization (projection learning) results in unsatis- factory enhancement. More critically, it can lead to sub- optimization failure and undermine the main fine-tuning process (see Appendix C.2 for results). Two primary issues contribute to the failure. First, the values of projections are not only related to γt but also the distance gap ∥∆θt∥. Ignoring the distance gap when searching for projections causes uninformative optimization and yields sub-optimal solutions. Second, because the projection is derived through noisy ZO optimization over only a few iterations, there is a risk that the projection magnitude becomes inappropri- ately small or large. A small projection drives the fine-tuned model too close to the pre-trained model, nullifying many previous updates, while a large projection applies overly ag- 4 gressive weight updates, destabilizing the training process. To address the above issues, two strategies are devised. Re-initialization. To introduce the distance gap ∥∆θt∥into the projection learning process, the initial value γt,0 is reset to ∥∆θt∥each time the projection is optimized. This means that, initially, the projection magnitude γt ∥∆θt∥= 1. If pro- jection updates are not performed, DiZO reverts to standard ZO optimization. Projection clipping. To prevent drastic weight changes and maintain training stability, we introduce projection clipping. Specifically, given a clipping range τ > 0, if the projec- tion magnitude γt ∥∆θt∥/∈[1 −τ, 1 + τ], it is clipped to remain within this interval. This prevents aggressive model adjustments that could destabilize training. With the above two strategies, we enhance the learning process of projection, more analysis can be found in Ap- pendix C.2. We also provide a Pytorch-style implementa- tion, please refer to Appendix F for details."
            },
            {
                "section": "Discussion",
                "content": "and Overhead Analysis We have some discussion on our method and analyze the computational overhead here and elaborate further later. Would adjusting the learning rate be equally effective? As discussed in Section 2.2, our main objective is to pro- vide ZO optimization with diverse-magnitude updates. A seemingly straightforward alternative is to assign different learning rates to each layer. However, in practice, this ap- proach yields results that are similar to or even worse than vanilla ZO in terms of accuracy and GPU hours. We at- tribute this to the noisy gradient estimation of one single ZO step, which is likely to yield imprecise update directions. Therefore, using unrefined layer-wise learning rates can in- tensify this noise and further destabilize the optimization process. In contrast, DiZO enables the awareness of the pre-trained model during fine-tuning (see Eq. 5), robustifies the training process (Dong et al., 2021; Oh et al., 2023; Zhai et al., 2023; Wang et al., 2024), and mitigates the noise introduced by ZO’s random perturbations. More results and analysis are shown in Appendix C.3. Memory utilization. Our method requires additional mem- ory as it involves storing the pre-trained model and calculat- ing the weight distance gap with the fine-tuned model, which can become costly when scaling to large LLMs. However, in DiZO, we find that projecting only the weight updates of the Query and Value layers in the attention module, instead of updating all layers, not only reduces memory requirements but also delivers better performance. As a result, we only need to store the weights of these two types of layers from the pre-trained model, accounting for approximately 16.7% of the parameters in OPT-2.7B, which is a manageable over- head. Similarly, LoRA (Hu et al., 2021) also focuses on weight decomposition for Query and Value layers, which echoes our observation. Further analysis and results on projection layer selection are provided in Appendix C.1. Computational overhead. Our method introduces extra computational cost, as the projection is learned alongside the main optimization (fine-tuning). However, we observe that performing projection learning intermittently, only once every few training iterations, does not compromise perfor- mance and significantly reduces the added overhead. This strategy reduces the computational burden while maintain- ing efficiency, allowing DiZO to achieve throughput compa- rable to vanilla ZO fine-tuning. Additionally, the reduced frequency of projection updates ensures that DiZO remains scalable for larger models and datasets. Please refer to Sec- tion 5.4 and Appendix D.1 for more details on computational overhead. 5. Experiments 5.1. Experimental Settings Models and datasets. We evaluate DiZO with various models, including medium-sized masked models (Liu et al., 2019) (RoBERTa-large) and large-sized autoregressive mod- els (Zhang et al., 2022; Touvron et al., 2023) with differ- ent size, including OPT-2.7B, OPT-6.7B, Llama3-3B, and Llama3-8B. The total parameter size is ranging from 355M to 8B. Both classification and generation tasks are included. More details on datasets are shown in Appendix B.1. Baseline. We mainly compare with two ZO works, memory- efficient ZO optimization (MeZO) (Malladi et al., 2023) and Hessian-informed ZO optimization (HiZOO) (Zhao et al., 2024). MeZO is a fundamental and representative work in ZO LLM fine-tuning but suffers from slow convergence speed. HiZOO1 is a recently proposed ZO acceleration for LLM fine-tuning, which leverages the estimated second- order information to speed up. In addition, we also incor- porate the parameter-efficient fine-tuning (PEFT) technique LoRA (Hu et al., 2021), applying it on top of FO fine-tuning, MeZO, and HiZOO. Evaluation. For training and evaluation, we follow previous works (Gao et al., 2020; Malladi et al., 2023). We study few- shot and many-shot settings on RoBERTa-large, randomly sampling k samples per class for training and validation, and 1000 samples for testing. For RoBERTa models, we evaluate k = 16 and k = 512. For OPT and LLaMA, we sample 1000, 500, and 1000 samples for training, validation, and testing. All experiments are conducted on NVIDIA A100 and A6000 GPUs. 1We implement HiZOO ourselves, please refer to Appendix B.2 for details. 5 Table 3. Experiment results on RoBERTa-large (350M) on six classification datasets. Results of the baseline methods MeZO and MeZO LoRA are taken from Malladi et al. (2023). All reported numbers are averaged accuracy with standard deviation shown. Better results between MeZO and DiZO are highlighted in bold. Dataset Task Type SST-2 SST-5 SNLI MNLI RTE TREC ——-sentiment——- ———-language inference———- –topic– Zero-shot 79.0 35.5 50.2 48.8 51.4 32.0 Gradient-free methods: k = 16 MeZO 90.5 (1.2) 45.5 (2.0) 66.0 (2.7) 56.5 (2.5) 59.4 (5.3) 76.9 (2.7) MeZO LoRA 91.4 (0.9) 43.0 (1.6) 69.7 (6.0) 64.0 (2.5) 64.9 (3.6) 73.1 (6.5) DiZO 92.2 (0.9) 47.1 (1.3) 71.0 (3.1) 60.1 (3.5) 67.9 (4.7) 77.4 (2.4) DiZO LoRA 91.7 (0.8) 44.6 (1.7) 71.6 (3.8) 65.6 (2.8) 67.3 (3.9) 74.6 (4.3) Gradient-based methods: k = 16 FT 91.9 (1.8) 47.5 (1.9) 77.5 (2.6) 70.0 (2.3) 66.4 (7.2) 85.0 (2.5) FT LoRA 91.4 (1.7) 46.7 (1.1) 74.9 (4.3) 67.7 (1.4) 66.1 (3.5) 82.7 (4.1) Gradient-free methods: k = 512 MeZO 93.3 (0.7) 53.2 (1.4) 83.0 (1.0) 78.3 (0.5) 78.6 (2.0) 94.3 (1.3) MeZO LoRA 93.4 (0.4) 52.4 (0.8) 84.0 (0.8) 77.9 (0.6) 77.6 (1.3) 95.0 (0.7) DiZO 94.6 (0.1) 53.6 (1.7) 84.5 (0.6) 79.8 (0.9) 80.3 (1.8) 93.8 (1.5) DiZO LoRA 94.3 (0.3) 54.1 (1.4) 83.7 (1.1) 77.6 (0.4) 79.3 (1.4) 95.7 (0.9) Gradient-based methods: k = 512 FT 93.9 (0.7) 55.9 (0.9) 88.7 (0.8) 84.4 (0.8) 82.7 (1.4) 97.3 (0.2) FT LoRA 94.2 (0.2) 55.3 (0.7) 88.3 (0.5) 83.9 (0.6) 83.2 (1.3) 97.0 (0.3) Figure 2. Trajectory of training loss curves when using MeZO and DiZO to fine-tune Roberta-large on SST-2, MNLI, and RTE. 5.2. Medium-sized masked language models We conduct experiments on RoBERTa-large across three types of datasets and compare DiZO with two ZO base- lines. We also explore PEFT by integrating LoRA. Table 3 presents the results, while Figure 2 shows the trajectory of training loss curves, indicating the convergence speed of DiZO and MeZO. Our key findings are as follows: DiZO greatly increases the convergence speed over MeZO. By using divergence-driven layer adaptation, the loss curve of DiZO decreases much faster, cutting the re- quired iterations by over 50% on SST-2, MNLI, and RTE. In addition, DiZO improves accuracy by 1.7%, 3.6%, and 8.5% on these three datasets, respectively. DiZO outperforms MeZO and achieves results on par with full fine-tuning. From Table 3, DiZO consistently surpasses MeZO on all six datasets. Notably, on SST-2 and RTE datasets, DiZO even shows better performance than FO full-parameter fine-tuning, increasing by 0.3% and 1.5%, respectively. DiZO is effective for both full-parameter fine-tuning and PEFT. Although DiZO applies projections based on the distance with the pre-trained model, while such prior knowledge does not exist for the decomposed weights of LoRA, it still delivers some gains. 5.3. Large autoregressive language models To assess the broader applicability of DiZO, we run experi- ments on the OPT and Llama series autoregressive LLMs 6 Table 4. Experiments results of fine-tuning OPT-2.7B on seven classification datasets and two text generation datasets (with 1000 training samples). Better results between MeZO, HiZOO, and DiZO are highlighted in bold. Dataset Task Type SST-2 RTE CB BoolQ WSC WIC MultiRC SQuAD DROP ————————–classification————————– ——generation—— Zero-shot 56.3 54.2 50.0 47.6 36.5 52.7 44.4 29.8 10.0 FT 94.2 81.2 82.1 72.2 63.8 65.8 71.6 78.4 30.3 LoRA 94.6 80.8 82.7 77.7 59.8 64.0 72.8 77.9 31.1 MeZO 90.0 63.5 69.6 67.4 61.5 57.6 58.7 68.7 22.9 HiZOO 90.8 60.6 70.4 68.0 60.2 56.6 54.8 68.3 23.4 DiZO 92.5 68.2 71.4 67.0 63.4 57.9 56.4 69.0 24.3 MeZO LoRA 91.4 66.6 71.1 67.6 59.6 57.0 57.0 70.8 22.5 HiZOO LoRA 90.6 65.2 71.4 67.4 52.6 58.8 59.0 71.8 22.7 DiZO LoRA 91.5 68.4 71.8 70.0 61.6 58.4 56.2 74.4 23.3 Table 5. Experiment results on OPT-6.7B for four classification datasets and one text generation dataset (with 1000 training sam- ples). Better results are highlighted in bold. Dataset Task Type SST-2 RTE CB WSC SQuAD ———classification——— –generation– MeZO 90.2 73.2 71.4 62.2 76.0 HiZOO 90.7 74.2 71.8 62.1 77.3 DiZO 91.1 74.8 73.2 61.8 78.6 MeZO LoRA 91.6 71.2 71.4 61.8 76.3 HiZOO LoRA 91.3 71.3 71.4 62.1 76.1 DiZO LoRA 92.4 70.2 71.8 62.6 77.9 Figure 3. Experiment result on Llama3-3B and Llama3-8B for four classification datasets and one text generation dataset. More results and detailed numbers are shown in Appendix D.2. covering both classification and generation tasks. The over- all results are summarized in Table 4, Table 5, and Fig- ure 3 for OPT-2.7B, OPT-6.7B, and Llama series, respec- tively. We also compare the convergence speeds of DiZO and MeZO on OPT-2.7B across multiple datasets in Fig- ure 4. Below, we highlight the key observations from these experiments. DiZO dramatically reduces the training GPU hours com- pared with the representative baseline MeZO. By incor- porating divergence-driven layer adaptation, DiZO quickly establishes meaningful divergence across layers, whereas MeZO requires many more iterations to achieve the de- sired layer-wise divergence. As shown in Table 4, DiZO converges with far fewer iterations across nine datasets, re- sulting in up to a 48% reduction in training GPU hours. Moreover, unlike HiZOO, which reduces the number of iter- ations needed but slows the throughput of MeZO by more than 1.5× due to Hessian estimation, DiZO keeps its through- put nearly on par with MeZO. This efficiency is achieved because the additional projection learning procedure needs only two forward passes and is performed intermittently. DiZO outperforms baselines in both standard and parameter-efficient settings. From Table 4, DiZO sur- passes MeZO and HiZOO with or without the LoRA, achiev- ing results comparable to FO methods. Across seven classi- fication datasets, DiZO ranks first on five, and it also leads in both text generation tasks. Table 5 shows that these advan- tages persist even when scaling up to OPT-6.7B. Moreover, as illustrated in Figure 3, the fine-tuning process of Llama- series model also benefits from layer-wise adaptive updates. 5.4. Memory and Speed Analysis In this section, we examine the memory utilization and convergence speed of DiZO in comparison with both ZO baselines and FO fine-tuning approaches (with and without LoRA). Table 6 presents the results of fine-tuning OPT-2.7B on the RTE dataset, more results are shown in Appendix D.1. From the memory perspective, DiZO maintains the advan- tage of avoiding backpropagation, getting rid of the storage of memory-intensive data, and reducing memory usage by about 90% compared to FO fine-tuning. As explained in Sec- tion 4, the additional memory requirement of DiZO stems from storing a portion of the pre-trained weights, includ- ing the weight of the Query and Value, amounting to only 16.7% of the total parameters. In contrast, HiZOO needs to store Hessian information for all layers, with memory usage proportional to the size of the parameters. From the perspective of convergence speed, DiZO greatly reduces the required iterations while maintaining throughput 7 - 48% - 47% - 45% - 41% - 41% - 38% - 35% - 31% - 29% # DiZO on Datasets # MeZO Figure 4. Comparison of convergence iterations, forward pass, and training GPU hours between MeZO and DiZO across multiple datasets."
            },
            {
                "section": "Results",
                "content": "are presented as proportions, with the percentage of saved GPU hours highlighted for each dataset. Table 6. Memory utilization and speed test on OPT-2.7B on RTE dataset (180 tokens per example on average). ●: partial gradient-free; ✓: gradient-free; ✗: gradient-based. DiZO†: learning projection with Adam. For a fair comparison, the speed and memory are measured on the same machine with the same setting using the same batch size. Please refer to Appendix D.1 for results on more datasets. Task Type Gradient Free LoRA Added Peak Memory Averaged Memory Throughput #Train Iter. GPU Hours FT ✗ ✗ 62.2 GB 62.2 GB 1.05 it/s 10.0% 16.2% LoRA ✗ ✓ 42.5 GB 42.5 GB 2.12 it/s 8.3% 6.6% DiZO† ● ✗ 44.7 GB 10.1 GB 1.43 it/s 33.3% 39.6% DiZO LoRA† ● ✓ 40.1 GB 9.8 GB 2.40 it/s 26.6% 18.8% MeZO ✓ ✗ 7.8 GB 7.8 GB 1.70 it/s 100.0% 100.0% HiZOO ✓ ✗ 13.2 GB 13.2 GB 1.21 it/s 63.3% 88.9% DiZO ✓ ✗ 9.5 GB 9.5 GB 1.54 it/s 60.0% 62.3% MeZO LoRA ✓ ✓ 7.7 GB 7.7 GB 3.10 it/s 94.2% 51.6% HiZOO LoRA ✓ ✓ 13.0 GB 13.0 GB 2.07 it/s 80.0% 65.7% DiZO LoRA ✓ ✓ 9.4 GB 9.4 GB 2.87 it/s 66.7% 39.5% similar to MeZO, resulting in significantly fewer training GPU hours. In contrast, HiZOO does not achieve compa- rable iteration savings and lowers the throughput of MeZO by about 1.5× because it requires Hessian information esti- mation. As a result, it only shows a modest acceleration in training GPU hours, in some settings, such as HiZOO with LoRA on RTE, it even consumes more training GPU hours than MeZO with LoRA. A notable byproduct of our method is using a FO approach (e.g., with the Adam optimizer) to learn the projections. While this version has memory consumption comparable to LoRA and requires additional training GPU hours, it offers distinct advantages. Since DiZO does not update projections at every iteration, FO-based DiZO exhibits significantly lower average memory usage than FO-based LoRA, with an average memory overhead close to that of the ZO-based DiZO. Although average memory usage may seem less crit- ical in single-process, single-GPU setup, many real-world on-device training scenarios involve multi-process environ- ments (Li et al., 2024; Ye et al., 2024). In such cases, the FO-based DiZO can stagger memory usage phases across processes, enabling parallel operations that purely FO meth- ods cannot achieve. Furthermore, compared with ZO-based DiZO, the FO version reduces extra training GPU hours and delivers better performance. These qualities make it particularly appealing for specific on-device training cases."
            },
            {
                "section": "Conclusion",
                "content": "In this paper, we propose a novel layer-wise divergence analysis to reveal the distinct update pattern between FO and ZO methods. Building on these insights, we present DiZO, an enhanced ZO method using divergence-driven layer adaptation to resemble the learning capacity of the FO method. DiZO achieves significant training acceleration and superior performance across diverse tasks and architec- tures. Moreover, our method can be seamlessly integrated with PEFT techniques like LoRA for additional speedup. For future work, we plan to explore DiZO in other fields, particularly for fine-tuning large pre-trained vision models. 8 7. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
            }
        ]
    },
    "Paper_7": {
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Jiawei Zhao 1 Zhenyu Zhang 3 Beidi Chen 2 4 Zhangyang Wang 3 Anima Anandkumar * 1 Yuandong Tian * 2",
        "sections": [
            {
                "section": "Abstract",
                "content": "Training Large Language Models (LLMs) presents significant memory challenges, predom- inantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer. How- ever, such approaches typically underperform training with full-rank weights in both pre- training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full- parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) with- out model parallel, checkpointing, or offloading strategies. Code is provided in the link."
            },
            {
                "section": "Introduction",
                "content": "Large Language Models (LLMs) have shown impressive performance across multiple disciplines, including conver- sational AI and language translation. However, pre-training *Equal advising 1California Institute of Technology 2Meta AI 3University of Texas at Austin 4Carnegie Mellon University. Cor- respondence to: Jiawei Zhao <jiawei@caltech.edu>, Yuandong Tian <yuandong@meta.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). BF16 AdamW Adafactor 8-bit Adam 8-bit GaLore (no retaining grad) AdamW (no retaining grad) 8-bit Adam (no retaining grad) RTX 4090 Figure 1: Estimated memory consumption of pre-training a LLaMA 7B model with a token batch size of 256 on a single de- vice, without activation checkpointing and memory offloading2. Details refer to Section 5.5. Algorithm 1: GaLore, PyTorch-like for weight in model.parameters(): grad = weight.grad # original space -> compact space lor grad = project(grad) # update by Adam, Adafactor, etc. lor update = update(lor grad) # compact space -> original space update = project back(lor update) weight.data += update and fine-tuning LLMs require not only a huge amount of computation but is also memory intensive. The memory requirements include not only billions of trainable parame- ters, but also their gradients and optimizer states (e.g., gra- dient momentum and variance in Adam) that can be larger than parameter storage themselves (Raffel et al., 2020; Touvron et al., 2023; Chowdhery et al., 2023). For exam- ple, pre-training a LLaMA 7B model from scratch with a single batch size requires at least 58 GB memory (14GB for trainable parameters, 42GB for Adam optimizer states and weight gradients, and 2GB for activations1). This makes the training not feasible on consumer-level GPUs such as NVIDIA RTX 4090 with 24GB memory. In addition to engineering and system efforts, such as gra- dient checkpointing (Chen et al., 2016), memory offload- 1The calculation is based on LLaMA architecture, BF16 nu- merical format, and maximum sequence length of 2048. 2In the figure, “no retaining grad” denotes the application of per-layer weight update to reduce memory consumption of storing weight gradient (Lv et al., 2023b). 1 arXiv:2403.03507v2  [cs.LG]  2 Jun 2024 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection ing (Rajbhandari et al., 2020), etc., to achieve faster and more efficient distributed training, researchers also seek to develop various optimization techniques to reduce the memory usage during pre-training and fine-tuning. Parameter-efficient fine-tuning (PEFT) techniques allow for the efficient adaptation of pre-trained language mod- els (PLMs) to different downstream applications without the need to fine-tune all of the model’s parameters (Ding et al., 2022). Among them, the popular Low-Rank Adapta- tion (LoRA Hu et al. (2022)) reparameterizes weight ma- trix W ∈Rm×n into W = W0 + BA, where W0 is a frozen full-rank matrix and B ∈Rm×r, A ∈Rr×n are additive low-rank adaptors to be learned. Since the rank r ≪min(m, n), A and B contain fewer number of train- able parameters and thus smaller optimizer states. LoRA has been used extensively to reduce memory usage for fine- tuning in which W0 is the frozen pre-trained weight. Its variant ReLoRA is also used in pre-training, by periodi- cally updating W0 using previously learned low-rank adap- tors (Lialin et al., 2024). However, many recent works demonstrate the limitation of such a low-rank reparameterization. For fine-tuning, LoRA is not shown to reach a comparable performance as full- rank fine-tuning (Xia et al., 2024). For pre-training from scratch, it is shown to require a full-rank model training as a warmup (Lialin et al., 2024), before optimizing in the low-rank subspace. There are two possible reasons: (1) the optimal weight matrices may not be low-rank, and (2) the reparameterization changes the gradient training dynamics. Our approach: To address the above challenge, we pro- pose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation meth- ods, such as LoRA. Our key idea is to leverage the slow- changing low-rank structure of the gradient G ∈Rm×n of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank. We first show theoretically that the gradient matrix G be- comes low-rank during training. Then, we propose Ga- Lore that computes two projection matrices P ∈Rm×r and Q ∈Rn×r to project the gradient matrix G into a low- rank form P ⊤GQ. In this case, the memory cost of opti- mizer states, which rely on component-wise gradient statis- tics, can be substantially reduced. Occasional updates of P and Q (e.g., every 200 iterations) incur minimal amortized additional computational cost. GaLore is more memory- efficient than LoRA as shown in Table 1. In practice, this yields up to 30% memory reduction compared to LoRA during pre-training. We demonstrate that GaLore works well in both LLM pre- training and fine-tuning. When pre-training LLaMA 7B on C4 dataset, 8-bit GaLore, combined with 8-bit optimizers and layer-wise weight updates techniques, achieves com- parable performance to its full-rank counterpart, with less than 10% memory cost of optimizer states. Notably, for pre-training, GaLore keeps low memory throughout the entire training, without requiring full-rank training warmup like ReLoRA. Thanks to GaLore’s mem- ory efficiency, it is possible to train LLaMA 7B from scratch on a single GPU with 24GB memory (e.g., on NVIDIA RTX 4090), without any costly memory offload- ing techniques (Fig. 1). GaLore is also used to fine-tune pre-trained LLMs on GLUE benchmarks with comparable or better results than existing low-rank methods. When fine-tuning RoBERTa- Base on GLUE tasks with a rank of 4, GaLore achieves an average score of 85.89, outperforming LoRA, which achieves a score of 85.61. As a gradient projection method, GaLore is independent of the choice of optimizers and can be easily plugged into existing ones with only two lines of code, as shown in Al- gorithm 1. Our experiment (Fig. 3) shows that it works for popular optimizers such as AdamW, 8-bit Adam, and Adafactor. In addition, its performance is insensitive to very few hyper-parameters it introduces. We also provide theoretical justification on the low-rankness of gradient up- date, as well as the convergence analysis of GaLore. 2. Related Works Low-rank adaptation. Hu et al. (2022) proposed Low- Rank Adaptation (LoRA) to fine-tune pre-trained models with low-rank adaptors. This method reduces the mem- ory footprint by maintaining a low-rank weight adaptor for each layer. There are a few variants of LoRA proposed to enhance its performance (Renduchintala et al., 2023; Sheng et al., 2023; Zhang et al., 2023; Xia et al., 2024), sup- porting multi-task learning (Wang et al., 2023b), and fur- ther reducing the memory footprint (Dettmers et al., 2024). Lialin et al. (2024) proposed ReLoRA, a variant of LoRA designed for pre-training, but requires a full-rank training warmup to achieve comparable performance as the stan- dard baseline. Inspired by LoRA, Hao et al. (2024) also suggested that gradients can be compressed in a low-rank subspace, and they proposed to use random projections to compress the gradients. There have also been approaches that propose training networks with low-rank factorized weights from scratch (Kamalakara et al., 2022; Wang et al., 2023a; Zhao et al., 2023). Subspace learning. Recent studies have demonstrated that the learning primarily occurs within a significantly low-dimensional parameter subspace (Gur-Ari et al., 2018; 2 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Larsen et al., 2022). These findings promote a spe- cial type of learning called subspace learning, where the model weights are optimized within a low-rank subspace. This notion has been widely used in different domains of machine learning, including meta-learning and continual learning (Lee & Choi, 2018; Chaudhry et al., 2020). Projected gradient descent. GaLore is closely related to the traditional topic of projected gradient descent (PGD) (Chen & Wainwright, 2015; Chen et al., 2019). A key dif- ference is that, GaLore considers the specific gradient form that naturally appears in training multi-layer neural net- works (e.g., it is a matrix with specific structures), proving many of its properties (e.g., Lemma 3.3, Theorem 3.2, and Theorem 3.8). In contrast, traditional PGD mostly treats the objective as a general blackbox nonlinear function, and study the gradients in the vector space only. Low-rank gradient. Gradient is naturally low-rank dur- ing training of neural networks, and this property have been studied in both theory and practice (Zhao et al., 2022; Cos- son et al., 2023; Yang et al., 2023). It has been applied to reduce communication cost (Wang et al., 2018; Vogels et al., 2020), and memory footprint during training (Goon- eratne et al., 2020; Huang et al., 2023; Modoranu et al., 2023). Memory-efficient optimization. There have been some works trying to reduce the memory cost of gradient statis- tics for adaptive optimization algorithms (Shazeer & Stern, 2018; Anil et al., 2019; Dettmers et al., 2022). Quantiza- tion is widely used to reduce the memory cost of optimizer states (Dettmers et al., 2022; Li et al., 2024). Recent works have also proposed to reduce weight gradient memory by fusing the backward operation with the optimizer update (Lv et al., 2023a;b). 3. GaLore: Gradient Low-Rank Projection 3.1. Background Regular full-rank training. At time step t, Gt = −∇W φt(Wt) ∈Rm×n is the backpropagated (negative) gradient matrix. Then the regular pre-training weight up- date can be written down as follows (η is the learning rate): WT = W0 + η T −1 X t=0 ˜Gt = W0 + η T −1 X t=0 ρt(Gt) (1) where ˜Gt is the final processed gradient to be added to the weight matrix and ρt is an entry-wise stateful gradient regularizer (e.g., Adam). The state of ρt can be memory- intensive. For example, for Adam, we need M, V ∈Rm×n to regularize the gradient Gt into ˜Gt: Mt = β1Mt−1 + (1 −β1)Gt (2) Vt = β2Vt−1 + (1 −β2)G2 t (3) ˜Gt = Mt/ p Vt + ϵ (4) Here G2 t and Mt/√Vt + ϵ means element-wise multipli- cation and division. η is the learning rate. Together with W ∈Rm×n, this takes 3mn memory. Low-rank updates. For a linear layer W ∈Rm×n, LoRA and its variants utilize the low-rank structure of the update matrix by introducing a low-rank adaptor AB: WT = W0 + BT AT , (5) where B ∈Rm×r and A ∈Rr×n, and r ≪min(m, n). A and B are the learnable low-rank adaptors and W0 is a fixed weight matrix (e.g., pre-trained weight). 3.2. Low-Rank Property of Weight Gradient While low-rank updates are proposed to reduce memory usage, it remains an open question whether the weight ma- trix should be parameterized as low-rank. In many situa- tions, this may not be true. For example, in linear regres- sion y = Wx, if the optimal W ∗is high-rank, then impos- ing a low-rank assumption on W never leads to the optimal solution, regardless of what optimizers are used. Surprisingly, while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network architectures. Reversible networks. Obviously, for a general loss func- tion, its gradient can be arbitrary and is not necessarily low rank. Here we study the gradient structure for a gen- eral family of nonlinear networks known as “reversible net- works” (Tian et al., 2020), which includes not only simple linear networks but also deep ReLU/polynomial networks: Definition 3.1 (Reversiblity (Tian et al., 2020)). A network N that maps input x to output y = N(x) is reversible, if there exists L(x; W) so that y = L(x; W)x, and the backpropagated gradient gx satisfies gx = L⊤(x; W)gy, where gy is the backpropagated gradient at the output y. Here L(x; W) depends on the input x and weight W in the network N. Please check Appendix B.1 for its properties. For re- versible networks, the gradient takes a specific form. Theorem 3.2 (Gradient Form of reversible mod- els). Consider a chained reversible neural net- work N(x) := NL(NL−1(. . . N1(x))) and de- fine Jl := Jacobian(NL) . . . Jacobian(Nl+1) and 3 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection fl := Nl(. . . N1(x)). Then the weight matrix Wl at layer l has gradient Gl in the following form for batch size 1: (a) For ℓ2-objective φ := 1 2∥y −fL∥2 2: Gl = \u0000J⊤ l y −J⊤ l JlWlfl−1 \u0001 f ⊤ l−1 (6) (b) Left P ⊥ 1 := I − 1 K 11⊤be the zero-mean PSD pro- jection matrix. For K-way logsoftmax loss φ(y; fL) := −log \u0010 exp(y⊤fL) 1⊤exp(fL) \u0011 with small logits ∥P ⊥ 1 fL∥∞≪ √ K: Gl = \u0000JlP ⊥ 1 y −γK−1J⊤ l P ⊥ 1 JlWlfl−1 \u0001 f ⊤ l−1 (7) where γ ≈1 and y is a data label with y⊤1 = 1. From the theoretical analysis above, we can see that for batch size N, the gradient G has certain structures: G = 1 N PN i=1(Ai −BiWCi) for input-dependent matrix Ai, Positive Semi-definite (PSD) matrices Bi and Ci. In the following, we prove that such a gradient will become low- rank during training in certain conditions: Lemma 3.3 (Gradient becomes low-rank during training). Suppose the gradient follows the parametric form: Gt = 1 N N X i=1 (Ai −BiWtCi) (8) with constant Ai, PSD matrices Bi and Ci after t ≥t0. We study vanilla SGD weight update: Wt = Wt−1 + ηGt−1. Let S := 1 N PN i=1 Ci ⊗Bi and λ1 < λ2 its two smallest distinct eigenvalues. Then the stable rank sr(Gt) satisfies: sr(Gt) ≤sr(G∥ t0)+ \u00121−ηλ2 1−ηλ1 \u00132(t−t0) ∥G0−G∥ t0∥2 F ∥G∥ t0∥2 2 (9) where G∥ t0 is the projection of Gt0 onto the minimal eigenspace V1 of S corresponding to λ1. In practice, the constant assumption can approximately hold for some time, in which the second term in Eq. 9 goes to zero exponentially and the stable rank of Gt goes down, yielding low-rank gradient Gt. The final stable rank is de- termined by sr(G∥ t0), which is estimated to be low-rank by the following: Corollary 3.4 (Low-rank Gt). If the gradient takes the parametric form Gt = 1 N PN i=1(ai −BiWtfi)f ⊤ i with all Bi full-rank, and N ′ := rank({fi}) < n, then sr(G∥ t0) ≤ n −N ′ and thus sr(Gt) ≤n/2 for large t. Remarks. The gradient form is justified by Theorem 3.2. Intuitively, when N ′ is small, Gt is a summation of N ′ rank-1 update and is naturally low rank; on the other hand, when N ′ becomes larger and closer to n, then the training dynamics has smaller null space V1, which also makes Gt low-rank. The full-rank assumption of {Bi} is reasonable, e.g., in LLMs, the output dimensions of the networks (i.e., the vocabulary size) is often huge compared to matrix di- mensions. In general if the batch size N is large, then it becomes a bit tricky to characterize the minimal eigenspace V1 of S. On the other hand, if V1 has nice structure, then sr(Gt) can be bounded even further: Corollary 3.5 (Low-rank Gt with special structure of V1). If V1(S) is 1-dimensional with decomposable eigenvector v = y ⊗z, then sr(G∥ t0) = 1 and thus Gt becomes rank-1. One rare failure case of Lemma 3.3 is when G∥ t0 is precisely zero, in which sr(G∥ t0) becomes undefined. This happens to be true if t0 = 0, i.e., Ai, Bi and Ci are constant through- out the entire training process. Fortunately, for practical training, this does not happen. Transformers. For Transformers, we can also separately prove that the weight gradient of the lower layer (i.e., project-up) weight of feed forward network (FFN) becomes low rank over time, using the JoMA framework (Tian et al., 2024). Please check Appendix (Sec. B.3) for details. 3.3. Gradient Low-rank Projection (GaLore) Since the gradient G may have a low-rank structure, if we can keep the gradient statistics of a small “core” of gradient G in optimizer states, rather than G itself, then the memory consumption can be reduced substantially. This leads to our proposed GaLore strategy: Definition 3.6 (Gradient Low-rank Projection (GaLore)). Gradient low-rank projection (GaLore) denotes the follow- ing gradient update rules (η is the learning rate): WT = W0 + η T −1 X t=0 ˜Gt, ˜Gt = Ptρt(P ⊤ t GtQt)Q⊤ t (10) where Pt ∈Rm×r and Qt ∈Rn×r are projection matrices. Different from LoRA, GaLore explicitly utilizes the low- rank updates instead of introducing additional low-rank adaptors and hence does not alter the training dynamics. In the following, we show that GaLore converges under a similar (but more general) form of gradient update rule (Eqn. 8). This form corresponds to Eqn. 6 but with a larger batch size. Definition 3.7 (L-continuity). A function h(W) has (Lip- schitz) L-continuity, if for any W1 and W2, ∥h(W1) − h(W2)∥F ≤L∥W1 −W2∥F . 4 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Theorem 3.8 (Convergence of GaLore with fixed projec- tions). Suppose the gradient has the form of Eqn. 8 and Ai, Bi and Ci have LA, LB and LC continuity with re- spect to W and ∥Wt∥≤D. Let Rt := P ⊤ t GtQt, ˆBit := P ⊤ t Bi(Wt)Pt, ˆCit := Q⊤ t Ci(Wt)Qt and κt := 1 N P i λmin( ˆBit)λmin( ˆCit). If we choose constant Pt = P and Qt = Q, then GaLore with ρt ≡1 satisfies: ∥Rt∥F ≤ \u0002 1−η(κt−1−LA−LBLCD2) \u0003 ∥Rt−1∥F (11) As a result, if mint κt > LA +LBLCD2, Rt →0 and thus GaLore converges with fixed Pt and Qt. Setting P and Q. The theorem tells that P and Q should project into the subspaces corresponding to the first few largest eigenvectors of ˆBit and ˆCit for faster convergence (large κt). While all eigenvalues of the positive semidefi- nite (PSD) matrix B and C are non-negative, some of them can be very small and hinder convergence (i.e., it takes a long time for Gt to become 0). With the projection P and Q, P ⊤BitP and Q⊤CitQ only contain the largest eigen subspaces of B and C, improving the convergence of Rt and at the same time, reduces the memory usage. While it is tricky to obtain the eigenstructure of ˆBit and ˆCit (they are parts of Jacobian), one way is to instead use the spectrum of Gt via Singular Value Decomposition (SVD): Gt = USV ⊤≈ r X i=1 siuiv⊤ i (12) Pt = [u1, u2, ..., ur], Qt = [v1, v2, ..., vr] (13) Difference between GaLore and LoRA. While both Ga- Lore and LoRA have “low-rank” in their names, they fol- low very different training trajectories. For example, when r = min(m, n), GaLore with ρt ≡1 follows the ex- act training trajectory of the original model, as ˜Gt = PtP ⊤ t GtQtQ⊤ t = Gt. On the other hand, when BA reaches full rank (i.e., B ∈Rm×m and A ∈Rm×n), op- timizing B and A simultaneously follows a very different training trajectory compared to the original model. 4. GaLore for Memory-Efficient Training For a complex optimization problem such as LLM pre- training, it may be difficult to capture the entire gradient trajectory with a single low-rank subspace. One reason is that the principal subspaces of Bt and Ct (and thus Gt) may change over time. In fact, if we keep the same projection P and Q, then the learned weights will only grow along these subspaces, which is not longer full-parameter train- ing. Fortunately, for this, GaLore can switch subspaces during training and learn full-rank weights without increas- ing the memory footprint. 𝑾𝟎 𝑾𝟎+ ∆𝑾𝑻𝟏 𝑾𝟎+ ∆𝑾𝑻𝟏+ ∆𝑾𝑻𝟐 !𝑮𝒕𝟏 !𝑮𝒕𝟐 Figure 2: Learning through low-rank subspaces ∆WT1 and ∆WT2 using GaLore. For t1 ∈[0, T1 −1], W are updated by projected gradients ˜Gt1 in a subspace determined by fixed Pt1 and Qt1. After T1 steps, the subspace is changed by recomputing Pt2 and Qt2 for t2 ∈[T1, T2 −1], and the process repeats until convergence. 4.1. Composition of Low-Rank Subspaces We allow GaLore to switch across low-rank subspaces: Wt = W0 + ∆WT1 + ∆WT2 + . . . + ∆WTn, (14) where t ∈ hPn−1 i=1 Ti, Pn i=1 Ti i and ∆WTi = η PTi−1 t=0 ˜ Gt is the summation of all Ti updates within the i-th subspace. When switching to i-th subspace at step t = Ti, we re-initialize the projector Pt and Qt by perform- ing SVD on the current gradient Gt by Equation 12. We il- lustrate how the trajectory of ˜ Gt traverses through multiple low-rank subspaces in Fig. 2. In the experiment section, we show that allowing multiple low-rank subspaces is the key to achieving the successful pre-training of LLMs. Following the above procedure, the switching frequency T becomes a hyperparameter. The ablation study (Fig. 5) shows a sweet spot exists. A very frequent subspace change increases the overhead (since new Pt and Qt need to be computed) and breaks the condition of constant projection in Theorem 3.8. In practice, it may also impact the fidelity of the optimizer states, which accumulate over multiple training steps. On the other hand, a less frequent change may make the algorithm stuck into a region that is no longer important to optimize (convergence proof in Theorem 3.8 only means good progress in the designated subspace, but does not mean good overall performance). While optimal T depends on the total training iterations and task com- plexity, we find that a value between T = 50 to T = 1000 makes no much difference. Thus, the total computational overhead induced by SVD is negligible (< 10%) compared to other memory-efficient training techniques such as mem- ory offloading (Rajbhandari et al., 2020). 4.2. Memory-Efficient Optimization Reducing memory footprint of gradient statistics. Ga- 5 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Algorithm 2: Adam with GaLore Input: A layer weight matrix W ∈Rm×n with m ≤n. Step size η, scale factor α, decay rates β1, β2, rank r, subspace change frequency T. Initialize first-order moment M0 ∈Rn×r ←0 Initialize second-order moment V0 ∈Rn×r ←0 Initialize step t ←0 repeat Gt ∈Rm×n ←−∇W φt(Wt) if t mod T = 0 then U, S, V ←SVD(Gt) Pt ←U[:, : r] {Initialize left projector as m ≤n} else Pt ←Pt−1 {Reuse the previous projector} end if Rt ←P ⊤ t Gt {Project gradient into compact space} UPDATE(Rt) by Adam Mt ←β1 · Mt−1 + (1 −β1) · Rt Vt ←β2 · Vt−1 + (1 −β2) · R2 t Mt ←Mt/(1 −βt 1) Vt ←Vt/(1 −βt 2) Nt ←Mt/(√Vt + ϵ) ˜Gt ←α · PNt {Project back to original space} Wt ←Wt−1 + η · ˜Gt t ←t + 1 until convergence criteria met return Wt Lore significantly reduces the memory cost of optimizer that heavily rely on component-wise gradient statistics, such as Adam (Kingma & Ba, 2015). When ρt ≡Adam, by projecting Gt into its low-rank form Rt, Adam’s gra- dient regularizer ρt(Rt) only needs to track low-rank gra- dient statistics. where Mt and Vt are the first-order and second-order momentum, respectively. GaLore computes the low-rank normalized gradient Nt as follows: Nt = ρt(Rt) = Mt/( p Vt + ϵ). (15) GaLore can also apply to other optimizers (e.g., Adafactor) that have similar update rules and require a large amount of memory to store gradient statistics. Reducing memory usage of projection matrices. To achieve the best memory-performance trade-off, we only use one project matrix P or Q, projecting the gradient G into P ⊤G if m ≤n and GQ otherwise. We present the algorithm applying GaLore to Adam in Algorithm 2. With this setting, GaLore requires less memory than LoRA during training. As GaLore can always merge ∆Wt to W0 during weight updates, it does not need to store a separate low-rank factorization BA. In total, GaLore re- quires (mn + mr + 2nr) memory, while LoRA requires (mn + 3mr + 3nr) memory. A comparison between Ga- Lore and LoRA is shown in Table 1. As Theorem 3.8 does not require the projection matrix to be carefully calibrated, we can further reduce the memory Table 1: Comparison between GaLore and LoRA. Assume W ∈ Rm×n (m ≤n), rank r. GaLore LoRA Weights mn mn + mr + nr Optim States mr + 2nr 2mr + 2nr Multi-Subspace ✓ ✗ Pre-Training ✓ ✗ Fine-Tuning ✓ ✓ cost of projection matrices by quantization and efficient pa- rameterization, which we leave for future work. 4.3. Combining with Existing Techniques GaLore is compatible with existing memory-efficient opti- mization techniques. In our work, we mainly consider ap- plying GaLore with 8-bit optimizers and per-layer weight updates. 8-bit optimizers. Dettmers et al. (2022) proposed 8-bit Adam optimizer that maintains 32-bit optimizer perfor- mance at a fraction of the memory footprint. We apply Ga- Lore directly to the existing implementation of 8-bit Adam. Per-layer weight updates. In practice, the optimizer typ- ically performs a single weight update for all layers after backpropagation. This is done by storing the entire weight gradients in memory. To further reduce the memory foot- print during training, we adopt per-layer weight updates to GaLore, which performs the weight updates during back- propagation. This is the same technique proposed in recent works to reduce memory requirement (Lv et al., 2023a;b). 4.4. Hyperparameters of GaLore In addition to Adam’s original hyperparameters, GaLore only introduces very few additional hyperparameters: the rank r which is also present in LoRA, the subspace change frequency T (see Sec. 4.1), and the scale factor α. Scale factor α controls the strength of the low-rank update, which is similar to the scale factor α/r appended to the low-rank adaptor in Hu et al. (2022). We note that the α does not depend on the rank r in our case. This is because, when r is small during pre-training, α/r significantly af- fects the convergence rate, unlike fine-tuning. 5. Experiments We evaluate GaLore on both pre-training and fine-tuning of LLMs. All experiments run on NVIDIA A100 GPUs. Pre-training on C4. To evaluate its performance, we ap- ply GaLore to train LLaMA-based large language models on the C4 dataset. C4 dataset is a colossal, cleaned version 6 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Table 2: Comparison with low-rank algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with a memory estimate of the total of parameters and optimizer states based on BF16 format. The actual memory footprint of GaLore is reported in Fig. 4. 60M 130M 350M 1B Full-Rank 34.06 (0.36G) 25.08 (0.76G) 18.80 (2.06G) 15.56 (7.80G) GaLore 34.88 (0.24G) 25.36 (0.52G) 18.95 (1.22G) 15.64 (4.38G) Low-Rank 78.18 (0.26G) 45.51 (0.54G) 37.41 (1.08G) 142.53 (3.57G) LoRA 34.99 (0.36G) 33.92 (0.80G) 25.58 (1.76G) 19.21 (6.17G) ReLoRA 37.04 (0.36G) 29.37 (0.80G) 29.08 (1.76G) 18.33 (6.17G) r/dmodel 128 / 256 256 / 768 256 / 1024 512 / 2048 Training Tokens 1.1B 2.2B 6.4B 13.1B AdamW 8-Bit Adam Adafactor Figure 3: Applying GaLore to different optimizers for pre-training LLaMA 1B on C4 dataset for 10K steps. Validation perplexity over training steps is reported. We apply GaLore to each optimizer with the rank of 512 and 1024, where the 1B model dimension is 2048. Table 3: Pre-training LLaMA 7B on C4 dataset for 150K steps. Validation perplexity and memory estimate are reported. Mem 40K 80K 120K 150K 8-bit GaLore 18G 17.94 15.39 14.95 14.65 8-bit Adam 26G 18.09 15.47 14.83 14.61 Tokens (B) 5.2 10.5 15.7 19.7 of Common Crawl’s web crawl corpus, which is mainly intended to pre-train language models and word represen- tations (Raffel et al., 2020). To best simulate the practical pre-training scenario, we train without data repetition over a sufficiently large amount of data, across a range of model sizes up to 7 Billion parameters. Architecture and hyperparameters. We follow the ex- periment setup from Lialin et al. (2024), which adopts a LLaMA-based3 architecture with RMSNorm and SwiGLU activations (Zhang & Sennrich, 2019; Shazeer, 2020; Tou- vron et al., 2023). For each model size, we use the same set of hyperparameters across methods, except the learn- ing rate. We run all experiments with BF16 format to re- duce memory usage, and we tune the learning rate for each method under the same amount of computational budget and report the best performance. The details of our task setups and hyperparameters are provided in the appendix. 3LLaMA materials in our paper are subject to LLaMA com- munity license. Fine-tuning on GLUE tasks. GLUE is a benchmark for evaluating the performance of NLP models on a variety of tasks, including sentiment analysis, question answering, and textual entailment (Wang et al., 2019). We use GLUE tasks to benchmark GaLore against LoRA for memory- efficient fine-tuning. 5.1. Comparison with Existing Low-Rank Methods We first compare GaLore with existing low-rank methods using Adam optimizer across a range of model sizes. Full-Rank Our baseline method that applies Adam opti- mizer with full-rank weights and optimizer states. Low-Rank We also evaluate a traditional low-rank ap- proach that represents the weights by learnable low-rank factorization: W = BA (Kamalakara et al., 2022). LoRA Hu et al. (2022) proposed LoRA to fine-tune pre- trained models with low-rank adaptors: W = W0 + BA, where W0 is fixed initial weights and BA is a learnable low-rank adaptor. In the case of pre-training, W0 is the full-rank initialization matrix. We set LoRA alpha to 32 and LoRA dropout to 0.05 as their default settings. ReLoRA Lialin et al. (2024) proposed ReLoRA, a vari- ant of LoRA designed for pre-training, which periodically merges BA into W, and initializes new BA with a reset on 7 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection optimizer states and learning rate. ReLoRA requires care- ful tuning of merging frequency, learning rate reset, and optimizer states reset. We evaluate ReLoRA without a full- rank training warmup for a fair comparison. For GaLore, we set subspace frequency T to 200 and scale factor α to 0.25 across all model sizes in Table 2. For each model size, we pick the same rank r for all low-rank meth- ods, and we apply them to all multi-head attention layers and feed-forward layers in the models. We train all mod- els using Adam optimizer with the default hyperparame- ters (e.g., β1 = 0.9, β2 = 0.999, ϵ = 10−8). We also estimate the memory usage based on BF16 format, includ- ing the memory for weight parameters and optimizer states. As shown in Table 2, GaLore outperforms other low-rank"
            },
            {
                "section": "methods",
                "content": "and achieves comparable performance to full-rank training. We note that for 1B model size, GaLore even outperforms full-rank baseline when r = 1024 instead of r = 512. Compared to LoRA and ReLoRA, GaLore re- quires less memory for storing model parameters and opti- mizer states. A detailed training setting of each model and memory estimation for each method are in the appendix. 5.2. GaLore with Memory-Efficient Optimizers We demonstrate that GaLore can be applied to various learning algorithms, especially memory-efficient optimiz- ers, to further reduce the memory footprint. We apply GaLore to AdamW, 8-bit Adam, and Adafactor optimiz- ers (Shazeer & Stern, 2018; Loshchilov & Hutter, 2019; Dettmers et al., 2022). We consider Adafactor with first- order statistics to avoid performance degradation. We evaluate them on LLaMA 1B architecture with 10K training steps, and we tune the learning rate for each set- ting and report the best performance. As shown in Fig. 3, applying GaLore does not significantly affect their conver- gence. By using GaLore with a rank of 512, the memory footprint is reduced by up to 62.5%, on top of the mem- ory savings from using 8-bit Adam or Adafactor optimizer. Since 8-bit Adam requires less memory than others, we de- note 8-bit GaLore as GaLore with 8-bit Adam, and use it as the default method for the following experiments on 7B model pre-training and memory measurement. 5.3. Scaling up to LLaMA 7B Architecture Scaling ability to 7B models is a key factor for demonstrat- ing if GaLore is effective for practical LLM pre-training scenarios. We evaluate GaLore on an LLaMA 7B architec- ture with an embedding size of 4096 and total layers of 32. We train the model for 150K steps with 19.7B tokens, using 8-node training in parallel with a total of 64 A100 GPUs. Due to computational constraints, we compare 8-bit Ga- Lore (r = 1024) with 8-bit Adam with a single trial with- out tuning the hyperparameters. As shown in Table 3, after 350M 1B 3B 7B Model Size 0 10 20 30 40 50 60 Memory cost (GB) RTX 4090 Memory Comparsion BF16 Adafactor 8-bit Adam 8-bit GaLore (retaining grad) 8-bit GaLore Figure 4: Memory usage for different methods at various model sizes, evaluated with a token batch size of 256. 8-bit GaLore (re- taining grad) disables per-layer weight updates but stores weight gradients during training. 150K steps, 8-bit GaLore achieves a perplexity of 14.65, comparable to 8-bit Adam with a perplexity of 14.61. 5.4. Memory-Efficient Fine-Tuning GaLore not only achieves memory-efficient pre-training but also can be used for memory-efficient fine-tuning. We fine-tune pre-trained RoBERTa models on GLUE tasks us- ing GaLore and compare its performance with a full fine- tuning baseline and LoRA. We use hyperparameters from Hu et al. (2022) for LoRA and tune the learning rate and scale factor for GaLore. As shown in Table 4, GaLore achieves better performance than LoRA on most tasks with less memory footprint. This demonstrates that GaLore can serve as a full-stack memory-efficient training strategy for both LLM pre-training and fine-tuning. 5.5. Measurement of Memory and Throughput While Table 2 gives the theoretical benefit of GaLore com- pared to other methods in terms of memory usage, we also measure the actual memory footprint of training LLaMA models by various methods, with a token batch size of 256. The training is conducted on a single device setup with- out activation checkpointing, memory offloading, and opti- mizer states partitioning (Rajbhandari et al., 2020). Training 7B models on consumer GPUs with 24G mem- ory. As shown in Fig. 4, 8-bit GaLore requires signifi- cantly less memory than BF16 baseline and 8-bit Adam, and only requires 22.0G memory to pre-train LLaMA 7B with a small per-GPU token batch size (up to 500 tokens). This memory footprint is within 24GB VRAM capacity of a single GPU such as NVIDIA RTX 4090. In addition, when activation checkpointing is enabled, per-GPU token batch size can be increased up to 4096. While the batch size is small per GPU, it can be scaled up with data parallelism, which requires much lower bandwidth for inter-GPU com- munication, compared to model parallelism. Therefore, it 8 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Table 4: Evaluating GaLore for memory-efficient fine-tuning on GLUE benchmark using pre-trained RoBERTa-Base. We report the average score of all tasks. Memory CoLA STS-B MRPC RTE SST2 MNLI QNLI QQP Avg Full Fine-Tuning 747M 62.24 90.92 91.30 79.42 94.57 87.18 92.33 92.28 86.28 GaLore (rank=4) 253M 60.35 90.73 92.25 79.42 94.04 87.00 92.24 91.06 85.89 LoRA (rank=4) 257M 61.38 90.57 91.07 78.70 92.89 86.82 92.18 91.29 85.61 GaLore (rank=8) 257M 60.06 90.82 92.01 79.78 94.38 87.17 92.20 91.11 85.94 LoRA (rank=8) 264M 61.83 90.80 91.90 79.06 93.46 86.94 92.25 91.22 85.93 is possible that GaLore can be used for elastic training (Lin et al., 2019) 7B models on consumer GPUs such as RTX 4090s. Specifically, we present the memory breakdown in Fig. 1. It shows that 8-bit GaLore reduces 37.92G (63.3%) and 24.5G (52.3%) total memory compared to BF16 Adam baseline and 8-bit Adam, respectively. Compared to 8- bit Adam, 8-bit GaLore mainly reduces the memory in two parts: (1) low-rank gradient projection reduces 9.6G (65.5%) memory of storing optimizer states, and (2) using per-layer weight updates reduces 13.5G memory of storing weight gradients. Throughput overhead of GaLore. We also measure the throughput of the pre-training LLaMA 1B model with 8-bit GaLore and other methods, where the results can be found in the appendix. Particularly, the current implementation of 8-bit GaLore achieves 1019.63 tokens/second, which induces 17% overhead compared to 8-bit Adam imple- mentation. Disabling per-layer weight updates for GaLore achieves 1109.38 tokens/second, improving the throughput by 8.8%. We note that our results do not require offloading strategies or checkpointing, which can significantly impact training throughput. We leave optimizing the efficiency of GaLore implementation for future work. 6. Ablation Study How many subspaces are needed during pre-training? We observe that both too frequent and too slow changes of subspaces hurt the convergence, as shown in Fig. 5 (left). The reason has been discussed in Sec. 4.1. In general, for small r, the subspace switching should happen more to avoid wasting optimization steps in the wrong subspace, while for large r the gradient updates cover more sub- spaces, providing more cushion. How does the rank of subspace affect the convergence? Within a certain range of rank values, decreasing the rank only slightly affects the convergence rate, causing a slow- down with a nearly linear trend. As shown in Fig. 5 (right), training with a rank of 128 using 80K steps achieves a lower loss than training with a rank of 512 using 20K steps. Figure 5: Ablation study of GaLore on 130M models. Left: varying subspace update frequency T. Right: varying subspace rank and training iterations. This shows that GaLore can be used to trade-off between memory and computational cost. In a memory-constrained scenario, reducing the rank allows us to stay within the memory budget while training for more steps to preserve the performance."
            },
            {
                "section": "Conclusion",
                "content": "We propose GaLore, a memory-efficient pre-training and fine-tuning strategy for large language models. GaLore sig- nificantly reduces memory usage by up to 65.5% in opti- mizer states while maintaining both efficiency and perfor- mance for large-scale LLM pre-training and fine-tuning. We identify several open problems for GaLore, which in- clude (1) applying GaLore on training of various mod- els such as vision transformers (Dosovitskiy et al., 2021) and diffusion models (Ho et al., 2020), (2) further enhanc- ing memory efficiency by employing low-memory projec- tion matrices, and (3) exploring the feasibility of elastic data distributed training on low-bandwidth consumer-grade hardware. We hope that our work will inspire future research on memory-efficient training from the perspective of gradi- ent low-rank projection. We believe that GaLore will be a valuable tool for the community, enabling the training of large-scale models on consumer-grade hardware with lim- ited resources. 9 GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Impact Statement This paper aims to improve the memory efficiency of train- ing LLMs in order to reduce the environmental impact of LLM pre-training and fine-tuning. By enabling the training of larger models on hardware with lower memory, our ap- proach helps to minimize energy consumption and carbon footprint associated with training LLMs."
            },
            {
                "section": "Methods",
                "content": "Token Batch Size Memory Cost Throughput #Tokens / s #Samples / s 1B ✘ AdamW 256 13.60 1256.98 6.33 Adafactor 256 13.15 581.02 2.92 Adam8bit 256 9.54 1569.89 7.90 8-bit GaLore 256 7.95 1109.38 5.59 1B ✔ AdamW 256 9.63 1354.37 6.81 Adafactor 256 10.32 613.90 3.09 Adam8bit 256 6.93 1205.31 6.07 8-bit GaLore 256 5.63 1019.63 5.13 23"
            }
        ]
    },
    "Paper_8": {
        "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models Zhen Zhang1, Yifan Yang1, Kai Zhen2, Nathan Susanj2, Athanasios Mouchtaris2, Siegfried Kunzmann2, Zheng Zhang1 1University of California, Santa Barbara",
        "sections": [
            {
                "section": "Abstract",
                "content": "Large language models have demonstrated ex- ceptional capabilities across diverse tasks, but their fine-tuning demands significant mem- ory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimiza- tion provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve gen- eralization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these chal- lenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first- order optimization. 1"
            },
            {
                "section": "Introduction",
                "content": "Large language models (LLMs) have revolution- ized natural language processing, enabling break- throughs in various applications (Anthropic, 2024; DeepMind, 2024; OpenAI, 2024; Bai et al., 2023). However, the large sizes of LLMs pose signif- icant memory challenges during training. Tra- ditional first-order (FO) optimization uses back- propagation, which requires substantial memory to store intermediate activations and gradients (Ros- tam et al., 2024; Kundu et al., 2024). This is- sue is especially pronounced in fine-tuning tasks on resource-constrained platforms (e.g. low-end GPUs or edge devices) (Zhuang et al., 2024). More- over, certain hardware platforms lack software sup- port (e.g. automatic differentiation) for backpropa- gation (Bergholm et al., 2018), further restricting FO methods. Although parameter-efficient fine- tuning methods have alleviated some of these chal- lenges, they still require multiple times the memory of inference (Bai et al., 2024a; Zhang et al., 2024b). Zeroth-order (ZO) optimization provides a memory-efficient alternative by estimating gradi- ents via forward passes only. Recent advances, such as MeZO (Malladi et al., 2023), have reduced memory usage to inference levels while achieving strong performance in LLM fine-tuning. However, the gradient variance in ZO methods is proportional to the number of perturbed parameters, which makes ZO methods struggle with high-dimensional parameter spaces, leading to slower convergence, increased gradient estimation variance, and hard to scale up (Chen et al., 2024b). Although recent work (Liu et al., 2024b; Yang et al., 2024b; Chen et al., 2023; Liu et al., 2024b; Yu et al., 2024) has addressed some of these issues, most ZO methods focus on single-task learning, leaving their applica- tion to multi-task learning largely unexplored. Multi-task learning is a key paradigm in LLMs to enable shared representations across diverse down- stream tasks. This approach improves generaliza- tion, reduces the need for task-specific models, and improves performance in a wide range of applica- tions (Zhang et al., 2023; Radford et al., 2019). De- spite its advantages, multi-task learning also intro- duces inherent challenges, particularly when tasks exhibit conflicting objectives. These conflicts arise when the optimization signals from different tasks are misaligned, leading to competing gradients that prevent the model from learning effectively across all tasks (Sener and Koltun, 2018; Mahapatra and Rajan, 2020; Crawshaw, 2020; Zhou et al., 2022; Shi et al., 2023). 1 arXiv:2502.11513v1  [cs.LG]  17 Feb 2025 SST2 BoolQ RTE WSC WiC MultiRC COPA SQuAD 20 40 60 80 100 LLaMA-2-7B SST2 BoolQ RTE WSC WiC MultiRC COPA SQuAD 20 40 60 80 100 Mistral-7B Zero-shot Single-task ZO (shared model) Multi-task ZO MaZO (Ours) Figure 1: Radar chart comparing the performance of our MaZO method with other methods on LLaMA-2-7B and Mistral-7B. Larger is better. Shared model means we train the model on one task and test it on all tasks. The issue of conflicting gradients is further exacerbated in scenarios involving ZO optimiza- tion (Liu et al., 2020; Malladi et al., 2023). The high gradient variance in ZO methods can amplify inter-task conflicts and make it even more diffi- cult to balance competing objectives (Zhang et al., 2024a). Furthermore, ZO methods suffer from collinearity in gradient estimates (see Section 2.2), where aggregated gradient directions lack diversity, and higher rank in Hessian matrix (see Section 3.1), where slower decay of eigenvalues in multi-task learning makes the convergence slow. A primary experiment demonstrated in Figure 1 shows that vanilla multi-task ZO optimization is only slightly better than zero-shot on average and is even worse on many tasks. To address these challenges, we propose Masked Zeroth-Order Optimization (MaZO), a novel frame- work designed for multi-task fine-tuning under ZO settings. MaZO tackles the problem at parameter level, which introduces two key innovations: (1) a weight importance metric that identifies critical pa- rameters for each task, and (2) a multi-task weight update mask that selectively updates these parame- ters while freezing others. By focusing on the most important parameters, MaZO reduces the dimen- sion of parameter space, mitigating the high vari- ance of ZO fine-tuning while preserving the model capacity. Moreover, unlike traditional approaches dynamic weighting (Chen et al., 2018; Liu et al., 2024a; Aghajanzadeh et al., 2023), which are triv- ial in ZO settings because of collinearity, MaZO balances multi-task learning conflicts from the per- spective of weight. It activates distinct parameter subsets for different tasks based on their impor- tance scores, allowing MaZO to allocate more ca- pacity to tasks that require more updates. Paper Contributions. This paper makes the fol- lowing novel contributions: • First ZO-based multi-task fine-tuning frame- work: We propose Masked Zeroth-Order Op- timization (MaZO), the first framework specif- ically designed for multi-task LLM fine-tuning under ZO optimization. • Task conflict resolution at the parameter level: MaZO addresses inter-task conflicts by selec- tively activating critical parameters for each task. This parameter-level approach ensures balanced optimization across tasks under ZO settings. • State-of-the-art performance: Comprehensive experiments on LLaMA-2-7B and Mistral-7B demonstrate that MaZO achieves state-of-the-art"
            },
            {
                "section": "results",
                "content": "in multi-task fine-tuning under ZO set- tings, outperforming multi-task learning methods designed for first-order (FO) optimization. 2 Preliminaries and Related Work 2.1 Zeroth-Order Optimization Zeroth-order (ZO) optimization estimates gradients using forward passes only. A common approach for ZO gradient estimation is the simultaneous per- turbation stochastic approximation (Spall, 1992), which serves as a randomized gradient estimator. Consider a model with parameters θ ∈Rd and a loss function L(θ). Using Taylor expansion, the randomized gradient can be estimated by perturb- ing θ with random noise z ∼N(0, Id) and com- puting forward and reverse losses: 2 b∇L(θ) = L(θ + ϵz) −L(θ −ϵz) 2ϵ z, (1) where ϵ is a small scalar. The expectation of b∇L(θ) matches the smoothed version of the true gradient. During training, zeroth-order stochastic gradient descent (ZO-SGD) updates parameters as: θ = θ −η b∇L(θ), (2) where η is the learning rate. Recent advances have improved ZO optimiza- tion for large-scale applications. For example, MeZO (Malladi et al., 2023) reduces memory us- age by regenerating random perturbations z using random seeds instead of storing them. ZO optimiza- tion offers significant advantages for fine-tuning LLMs, as it avoids memory-intensive backpropaga- tion (Liu et al., 2020; Zhang et al., 2024b). Despite these advantages, the gradient variance of ZO opti- mization increases linearly with the dimensionality of the parameter space. This leads to slower conver- gence and difficulties in large-scale training (Chen et al., 2024b). To address these challenges, various"
            },
            {
                "section": "methods",
                "content": "have been proposed. These include the de- sign of advanced ZO optimizers (Zhao et al., 2024; Jiang et al., 2024; Chen et al., 2019); dimensional- ity reduction techniques (Liu et al., 2024b; Wang et al., 2024; Yang et al., 2024b; Guo et al., 2024); hybrid approaches like Addax (Li et al., 2024); full- batch gradient estimation (Gautam et al., 2024); exploiting low-rank structures (Zhao et al., 2023; Yu et al., 2024), and using orthogonal random di- rections (Kozak et al., 2023). While these methods have advanced ZO in var- ious ways, they do not specifically address the unique challenges of multi-task learning. 2.2 Multi-task Learning Multi-task learning aims to improve generalization performance by jointly learning T related tasks through shared parameters (Chen et al., 2024a). Classical multi-task learning minimizes a weighted combination of task-specific losses: L(θ) = T X t=1 wtLt(θ), s.t. T X t=1 wt = 1, wt ≥0, (3) where Lt(θ) represents the learning loss for a sin- gle task t. Parameter updates are performed using gradient descent. Multi-task learning under FO optimization has been widely studied, with different technical routes: (1) dynamic weight, which adjusts the weight of different tasks by gradients (Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2022), loss (Liu et al., 2019, 2024a; Kongyoung et al., 2020; Gong et al., 2024) or uncertainty (Aghajanzadeh et al., 2023); (2) gradient manipulation (Désidéri, 2012; Liu et al., 2021; Yu et al., 2020); (3) data mixing and scheduling (Bai et al., 2024b; Ahma- dian et al., 2024); (4) learning shared and specific knowledge with model architecture based on LoRA (Feng et al., 2024; Yang et al., 2024a; Wang et al., 2023) or MoE (Liu et al., 2023; Gupta et al., 2022); (5) model merging (Yang et al., 2023). 3 The MaZO Framework 3.1 Challenges in ZO Multi-Task Fine Tuning Under ZO optimization, multi-task learning faces unique challenges. Specifically, task-specific ZO gradient estimates exhibit fundamental collinear- ity, as the aggregated multi-task learning gradient aligns with the shared random perturbation z: g = T X t=1 wtgt = T X t=1 wt Lt(θ + ϵz) −Lt(θ −ϵz) 2ϵ ! z. (4) Here g and gt are gradients of multi-task learning and of task t, respectively. This collinearity results in a lack of directional diversity, limiting optimiza- tion efficacy. Further discussion can be found in Appendix H. As explained in (Malladi et al., 2023), the sur- prising success of ZO optimization in LLM fine- tuning is due to the low-rank property of the Hes- sian matrix. Based on (3), the Hessian matrix in multi-task fine-tuning can be written as H = T X t=1 wtHt, (5) where Ht is the Hessian associated with single-task learning loss Lt. Although Ht has a low rank in the fine-tuning process, H can have a much higher rank due to the weighted sum of T task-specific Hessian matrices. Figure 3 empirically verifies our theoretical claim: the Hessian in multi-task learn- ing exhibits a broader eigenvalue spectrum than single-task learning, leading to a higher effective rank. This further slows down the convergence of ZO in multi-task LLM fine-tuning. To address the above challenges, we propose Masked Zeroth-Order Optimization (MaZO). 3 Weight Importance S Row-wise Input x Weight W & Task 1…T Weight Importance Multi-task Norm & Rank 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 Mask M Row-wise Figure 2: Diagram of our MaZO method. The weight importance scoring and weight update mask is calculated row-wise. The weight importance for each task is calculated independently, and only from the input and weight. 0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 Eigenvalue Index (sorted descending) 0.0 0.2 0.4 0.6 0.8 1.0 Eigenvalue Top-K Eigenvalue Distribution Comparison Single-Task Learning Multi-Task Learning Figure 3: Top-K eigenvalue distribution of the Hessian matrices in multi-task learning and single-task learn- ing. These eigenvalue are normalized by dividing by the maximum value. The slower decay of eigenvalues in multi-task learning suggests a higher effective rank, which contributes to the slower convergence of ZO fine- tuning in multi-task scenarios. Our approach introduces a novel framework that solves multi-task learning at parameter level. MaZO combines weight importance metrics and a multi-task weight update mask. The weight impor- tance is derived using two complementary metrics: (1) a global score which evaluates the theoretical minimum loss when freezing a parameter, (2) a greedy score which quantifies the immediate loss change during a single optimization step. Using these scores, we construct a weight update mask that identifies a subset of critical parameters, en- abling effective optimization by reducing dimen- sionality and variance while balancing the perfor- mance among potentially conflicting tasks. 3.2 Multi-Task Weight Update Mask We first introduce the multi-task weight update mask, assuming the weight importance scores are precomputed. We defer the computation of weight importance scores to the next subsection. In ZO optimization, the variance of an estimated gradient increases with the number of training parameters. Therefore, it is crucial to identify and focus on critical parameters for effective optimization while freezing others (Liu et al., 2024b; Guo et al., 2024). Suppose that we have a weight importance score matrix St for each task t and a sparsity level ρ. We unfreeze the top k = ⌈(1 −ρ) · N⌉parameters in each row, where N is the total number of param- eters in that row. The importance scores are com- pared row-wise due to the approximations involved in gradient and Hessian estimation following Sun et al. (2023), which will be detailed in Section 3.4. Since importance scores across tasks are not di- rectly comparable due to differing scales, we nor- malize the scores row-wise for each task: ˆSt ij = St ij −min(St i) max(St i) −min(St i), (6) where St i denotes the i-th row of St; ˆSt ij is the normalized score for parameter j in row i for task t. The overall score across tasks is computed as: S = T X t=1 ˆSt. (7) We select the top k parameters based on S in each row to fine-tune, while freezing the others. This selection is represented by a binary mask ma- trix M, where Mij = 1 indicates that parameter j in row i is unfrozen. The final parameter update is computed as: ∆Wmasked = ∆W ⊙M, (8) where ⊙denotes element-wise multiplication. When applied to LoRA (Hu et al., 2021), this be- comes: ∆Wmasked = (A · B) ⊙M, (9) where A and B are the decomposed matrices of LoRA. 4 3.3 Weight Importance The overall importance score for task t combines the normalized global and greedy scores with a weight regularization term: St = St global + αSt greedy + β|W|, (10) where α and β are hyperparameters controlling the contributions of each component and |W| is the absolute value of weight. We now describe the computation of two complementary metrics: the global score and the greedy score. 3.3.1 Global Score The global score is inspired by the Optimal Brain Surgeon method (Frantar and Alistarh, 2023; Sun et al., 2023; Das et al., 2023). Unlike pruning, which sets the parameters to zero, our approach freezes certain parameters while updating others via perturbation. Consider the Taylor expansion of the loss function of task t: δLt = (gt)⊤· δθ + 1 2δθ⊤· Ht · δθ + O(∥δθ∥3), where Ht is the Hessian matrix of task t and gt = ∂Lt STL ∂θ . Freezing a parameter at position m imposes the constraint I⊤ mδθ = 0, where Im is an indicator function. The optimization problem becomes: min m \u001a min δθ \u0012 \u0000gt\u0001⊤· δθ + 1 2δθ⊤· Ht · δθ \u0013 I⊤ m · δw = 0 \u001b . (11) This formulation seeks to find the parameter posi- tion m that, when frozen, results in the maximal decrease in the loss function while allowing other parameters to adjust optimally. The inner optimiza- tion determines the best possible parameter updates given the constraint, while the outer optimization identifies the least impactful parameter to fix. Using Lagrange multipliers, the optimal loss change (global score) is derived as: (St global)m = δLt m = \u0010 I⊤ m · \u0000Ht\u0001−1 · gt\u00112 2 \u0010 (Ht)−1\u0011 mm , (12) This expression quantifies the theoretical maximum decrease in loss when parameter m is fixed, pro- viding a measure of its importance to the overall optimization process. Smaller values indicate less important parameters, which should be frozen. 3.3.2 Greedy Score Although the global score provides a theoretical measure of parameter importance, it may not suf- fice because the model may not converge to the optimal situation due to the large variance in the ZO gradient. Therefore, we also introduce a greedy score as a practical complement, which considers the immediate impact of freezing a parameter in a single optimization step. For a gradient descent update with learning rate η and random direction z, the parameter update of task t is approximated as: δθ ≈−ηzzT gt. (13) Substituting δθ and taking the expectation over random directions z, we obtain the expected change in loss: E(δLt) = −(gt)T gt · η + M X i=0 (gt i)2Ht ii + 2(gt i)T Htgt ! η2 where M is the number of parameters in a LLM. When we freeze a parameter at position m, the change of loss (greedy score) will increase by: (St greedy)m = δLt m = (gt m)2η + Ht mm(gt m)2η2 −4 M X j=0 Ht mj(gt m)(gt j)η2 (14) Parameters with lower St greedy values are consid- ered less important for the current optimization step and are better candidates for freezing during multi-task learning. 3.4 Implementation To avoid the huge cost of computing the full gradi- ent and Hessian, we adopt a row-wise approxima- tion strategy. For a linear layer y = Wx, focusing on a single row wi, the output is yi = wix. Per- forming a Taylor expansion of the loss L with re- spect to yi, we find that both the first-order gradient ∇L(yi) and second-order derivative ∇2L(yi) are scalars. Substituting ∆yi = ∆wix, the gradient and Hessian with respect to wi are: gt = ∂L ∂wi = ∇L(yi)x, (15) Ht = ∂2L ∂w2 i = ∇2L(yi)(xx⊤). (16) 5 Here, we replace the gradient with x, and the Hes- sian with xx⊤since we only care about the relative value in a row. This row-wise approximation signif- icantly reduces computational cost, while still cap- turing the relative importance of parameters within each row. However, it also restricts the weight- importance comparison to the row direction. Overall Algorithm Flow. The pseudo-code of the whole MaZO fine-tuning framework is shown as Algorithm 1 in Appendix B. 4 Experiments 4.1 Experimental Setup We perform multi-task fine-tuning on two widely used decoder-only pretrained language models: LLaMA-2-7B (Touvron et al., 2023) and Mistral- 7B (Jiang et al., 2023). Tasks. We evaluate our approach on a diverse set of natural language understanding (NLU) and natural language generation (NLG) tasks from the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks. Specifically, for NLU, we include SST-2, BoolQ, RTE, WSC, WiC, Mul- tiRC, and COPA, covering various classification and reasoning tasks. For NLG, we use SQuAD for question answering. To ensure computational feasibility, we sample a subset of each dataset with fixed training, validation, and test splits. Details on datasets and evaluation metrics are in Appendix G. Baselines. We compare MaZO with several baselines. First, we include vanilla ZO optimiza- tion combined with traditional multi-task learning (MTL-ZO) techniques as a direct comparison to MaZO in the ZO setting. Second, we evaluate single-task learning (STL-ZO), where models are trained individually on each task to provide an upper bound for task-specific performance with- out multi-task conflicts, as well as a single-task transfer baseline, where the model is trained on a single task (SST-2) using vanilla ZO optimiza- tion and evaluated across all tasks to highlight the limitations of single-task training in multi-task sce- narios. Third, we include LoRA fine-tuning (Hu et al., 2021), a parameter-efficient approach, and extend MaZO to update LoRA matrices under ZO settings, demonstrating its flexibility. Finally, we compare MaZO against state-of-the-art first-order (FO) multi-task learning methods, including CoBa (Gong et al., 2024), FAMO (Liu et al., 2024a), and MTL-LoRA (Yang et al., 2024a), to its compatibil- ity with ZO optimization. These baselines provide 0 2000 4000 6000 8000 10000 Training Steps 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 Eval Loss Convergence Curve MTL-ZOLoRA MaZOLoRA Figure 4: The convergence curve of (1) vanilla multi- task ZO fine-tuning with LoRA, (2) MaZO with LoRA. a comprehensive comparison for assessing MaZO’s effectiveness and robustness in addressing the chal- lenges of ZO-based multi-task learning. 4.2"
            },
            {
                "section": "Results",
                "content": "on LLaMA-2-7B MaZO Outperforms Competitors. The results for LLaMA-2-7B are presented in Table 1. Vanilla multi-task ZO optimization shows only slight im- provements over the zero-shot baseline, highlight- ing its inability to effectively address multi-task conflicts under ZO settings. Similarly, vanilla single-task ZO optimization with a shared model fails to generalize effectively across multiple tasks, underscoring the inherent challenges of ZO opti- mization in multi-task scenarios. In contrast, our proposed MaZO framework achieves the highest average performance across all tasks and demonstrates a balanced performance profile. These results validate MaZO’s ability to mitigate inter-task conflicts and optimize multi-task learning by selectively focusing on critical parame- ters. The effectiveness of MaZO is further evident in its superior performance in both full-model ZO fine-tuning and LoRA-based fine-tuning, with par- ticularly pronounced gains in the latter. This un- derscores MaZO’s flexibility and its compatibility with parameter-efficient fine-tuning techniques. Dimensionality Reduction Enhances Multi-Task Learning. The application of LoRA to ZO fine- tuning significantly improves the performance of multi-task learning. This improvement can be at- tributed to LoRA’s ability to reduce the dimension- ality of the parameter space, thereby lowering the variance of gradient estimates. These findings re- inforce the validity of MaZO’s masking strategy, which optimizes multi-task learning by focusing on a reduced set of critical parameters. 6 Task SST-2 BoolQ RTE WSC WiC MultiRC COPA SQuAD Avg Task Type ———————- Classification ———————- – Multiple Choice – – Generation – STL-ZO (one model per task) 93.8 83.0 73.5 51.3 62.1 61.0 86.0 79.6 73.8 Zero-Shot 83.8 75.8 57.0 37.5 52.6 46.6 79.0 56.4 61.1 ICL 93.7 78.7 61.2 47.2 59.9 54.3 80 57.7 66.6 STL-ZO(shared model) 93.8 75.8 58.8 43.3 50.6 46.0 75.0 52.2 61.9 MTL-ZO 80.6 74.2 55.6 58.6 51.0 53.8 80.0 62.3 64.5 MTL-ZOLoRA 90.2 80.0 61.0 54.8 56.6 58.0 76.0 74.8 68.9 MTL-ZOMTL-LoRA 88.4 76.8 60.2 60.6 57.6 61.6 79.0 59.4 68.0 MTL-ZOCoBa 82.8 75.3 56.8 60.6 53.4 55.8 77.0 57.6 64.9 MTL-ZOFAMO 91.0 77.6 59.4 56.5 53.4 50.6 78.0 53.9 65.1 MaZO 90.6 76 62.8 56.7 52.6 58.6 82.0 55.5 66.9 MaZOLoRA 91.2 80.0 62.8 61.5 56.6 60.4 80.0 77.7 71.3 Table 1: Performance comparison across tasks using different methods on LLaMA-2-7B. The average score (Avg) is computed across all tasks. Metrics for these tasks are consistent with MeZO (Malladi et al., 2023). Shared model indicates training on a single task (SST-2) and testing on all tasks. ICL refers to in-context learning. STL represents single-task learning and MTL represents multi-task learning. FO multi-task learning methods do not apply to ZO. Multi-task learning methods originally de- veloped for first-order (FO) optimization, such as CoBa and FAMO, do not achieve effective perfor- mance in the ZO setting. This can be attributed to their inability to resolve multi-task conflicts due to the collinearity problem in ZO gradient estimates. Under the ZO framework, FO methods can only adjust the magnitude of the approximated gradient, but not its direction, resulting in performance degra- dation. Additionally, MTL-LoRA, the multi-task version of LoRA fine-tuning, does not significantly enhance performance in the ZO setting. This may be due to the sensitivity of task-specific weights and the diagonal transformation matrix to noise. Perturbation-based optimization, as used in ZO, in- troduces excessive variance, which undermines the effectiveness of these FO-based methods. 4.3"
            },
            {
                "section": "Results",
                "content": "on Mistral-7B The results for Mistral-7B in Table 2 reveal trends similar to those observed with LLaMA-2-7B. De- spite the relatively low zero-shot performance of Mistral-7B, vanilla multi-task learning ZO fails to deliver substantial improvements. This under- scores the inherent challenges of ZO-based multi- task learning. In contrast, MaZO consistently out- performs all other methods. Its ability to mitigate ZO-specific challenges is evident in its superior performance, further validating MaZO as a state-of- the-art solution for ZO-based multi-task learning. 4.4 Computational Performance Figure 4 shows that MaZO converges faster and achieves a significantly lower loss compared to tra- ditional multi-task ZO fine-tuning methods. This holds true both with and without LoRA. This im- provement can be attributed to the mask mecha- nism in MaZO, which focuses on optimizing the most critical parameters, thereby reducing gradi- ent noise, balancing the inter-task conflicts, and accelerating convergence. The significantly better convergence and accu- racy of MaZO is obtained at marginal comput- ing and memory overhead. Specifically, the mask search time introduced by MaZO is negligible com- pared to the overall training time. When evaluated in the LlaMMA-7B model, MaZO incurs a slight increase in memory usage (approximately 10%) compared to baseline multi-task learning ZO meth- ods. This is primarily due to the additional storage required for the weight update mask. However, this increase does not significantly impact the over- all memory efficiency, especially when combined with LoRA, where the parameter space is already reduced. Details are provided in Appendix E. 4.5 Various Weight Importance Metrics To further validate the effectiveness of MaZO, we compare its performance with three alterna- tive weight scoring methods: random selection, magnitude-based scoring, and Wanda scoring. De- tailed implementation of these methods is de- scribed in Appendix F. For a fair comparison, we fix the sparsity level at 50%, consistent with the sparsity used in the Wanda score. Table 3 summa- rizes the results of this comparison. The findings indicate that while both the magnitude-based and Wanda-based scoring can im- prove average performance, their improvements 7 Task SST-2 BoolQ RTE WSC WiC MultiRC COPA SQuAD Avg Task Type ———————- Classification ———————- – Multiple Choice – – Generation – STL-ZO (one model per task) 93.6 77.8 74.2 55.3 62.1 62.7 88.0 76.5 73.8 Zero-Shot 56.7 42.4 50.5 52.8 50.3 43.6 79.0 57.2 54.1 ICL 62.3 46.1 56.0 53.2 61.4 53.4 79.0 62.3 59.2 MTL-ZO 58.7 47.2 55.0 53.2 59.8 54.4 79.0 56.3 58.0 MTL-ZOLoRA 89.3 73.2 71.5 51.3 58.1 53.4 80.0 73.5 68.7 MaZO 83.4 56.3 60.2 54.8 58.1 55.8 79 59.4 63.4 MaZOLoRA 90.2 72.4 74.2 54.8 62.1 57.3 82.0 73.5 70.8 Table 2: Performance comparison across tasks using different methods on Mistral-7B. The setting and notation are the same as Table 1. We exclude the FO MTL methods as they do not have significant improvement. Task SST-2 BoolQ Copa SQuAD Avg No Mask 85.4 72.2 80.0 66.0 75.9 Random 86.6 73.0 80.0 63.4 75.8 Magnitude 87.4 75.6 79.0 65.6 76.9 Wanda 88.4 77.8 80.0 62.4 77.2 MaZO 90.2 78.0 81.0 72.3 80.4 Table 3: Comparison of different weight importance metrics. The sparsity is set to 50% except for No Mask. Random and Magnitude are done weight-wise while Wanda and MaZO are selected row-wise. are less pronounced and less balanced across tasks compared to MaZO. This is because these meth- ods evaluate the weight importance statically, with- out considering training dynamics or perturbation- based insights. In contrast, MaZO dynamically identifies critical parameters during training, en- abling more effective optimization and better multi- task balance under the ZO framework. These re- sults underscore the superiority of MaZO in lever- aging weight importance to achieve state-of-the-art performance in multi-task fine-tuning. 4.6 Ablation Study Finally, we explore the optimal hyperparameter set- tings for MaZO, includeing α, β, sparsity level, and the LoRA rank. To streamline the process, we per- form grid searches for each hyperparameter while keeping the others constant. For most experiments, we fine-tune the model on SST-2, BoolQ, COPA, and SQuAD, encompassing binary classification, multiple-choice, and generation tasks, providing di- verse evaluation scenarios. However, for the LoRA rank, we evaluate performance across all tasks. α and β. To optimize α and β, we fix the sparsity level at 50% and perform full-model fine- tuning (without LoRA). The search is conducted in two stages. First, β is set to zero, and α is tuned, resulting in an optimal value of α = 10. Next, with α fixed, β is tuned, yielding an optimal value of β = 1. These values strike a balance between the global and greedy weight importance metrics, ensuring effective parameter selection. LoRA rank. We examine the impact of LoRA rank and provide detailed results in Appendix D. In summary, the results reveal a U-shaped relationship between rank and performance, reflecting a trade- off between model capacity and dimensionality. The optimal rank of 16 minimizes loss and is used as the default setting for LoRA-based baseline. Sparsity. We perform a grid search of the spar- sity level ρ from 0.1 to 0.99. For full-model fine- tuning, the performance first improves with increas- ing sparsity and then sharply declines. The peak performance is achieved at ρ = 0.9. For LoRA fine-tuning, we jointly optimize sparsity levels and LoRA ranks. The optimal result is found at a LoRA rank of 64 and a sparsity level of 0.8. Notably, the effective number of parameters is equivalent to 64 × (1 −0.8) = 12.8, which is less than the best- performing rank of LoRA baseline. This highlights that MaZO can further reduce the dimension while maintaining the model capacity. 5"
            },
            {
                "section": "Conclusion",
                "content": "In this work, we have presented MaZO, a novel framework that harnesses masked zeroth-order op- timization for the multi-task fine-tuning of LLMs. By incorporating weight importance score along- side a multi-task weight update mask, MaZO ef- fectively reduces gradient variance and mitigates conflicts among tasks. Our experimental results demonstrate that MaZO not only surpasses cur- rent zeroth-order optimization methods but also outperforms leading multi-task learning methods designed for first-order optimization across a range of NLP tasks. Furthermore, our parameter-level approach is not limited solely to zeroth-order op- timization, offering potential integrations with a variety of other optimization strategies. 8 6 Limitations While MaZO demonstrates strong empirical per- formance, several limitations warrant discussion. First, the computation of weight importance intro- duces additional computational overhead compared to vanilla ZO methods. However, this cost remains negligible relative to the memory and computa- tional demands of model weights and activations. Second, the effectiveness of MaZO is partially con- tingent on the quality of gradient and Hessian ap- proximations. While our current approximations are effective, they could be further refined through more sophisticated estimation techniques to en- hance performance. Third, our experiments are lim- ited to medium-scale models (7B parameters) due to computational constraints. Although the method is theoretically applicable to larger models, the interplay between mask sparsity and model scale has not been systematically studied and represents an avenue for future research. Finally, we do not provide a theoretical convergence analysis for the masking approach. However, Sparse MeZO (Liu et al., 2024b) has already conducted a comprehen- sive and rigorous analysis of general masking sce- narios in zeroth-order optimization. We refer inter- ested readers to their work for detailed theoretical insights, and therefore do not duplicate these efforts here."
            },
            {
                "section": "methods",
                "content": "for fine-tuning language models. arXiv preprint arXiv:2404.08080. Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, and Jianguo Li. 2024. Coba: Convergence balancer for multitask finetuning of large language models. arXiv preprint arXiv:2410.06741. Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R Gardner, Osbert Bas- tani, Christopher De Sa, Xiaodong Yu, et al. 2024. Zeroth-order fine-tuning of llms with extreme spar- sity. arXiv preprint arXiv:2406.02913. Shashank Gupta, Subhabrata Mukherjee, Krishan Sub- udhi, Eduardo Gonzalez, Damien Jose, Ahmed H Awadallah, and Jianfeng Gao. 2022. Sparsely acti- vated mixture-of-experts are robust multi-task learn- ers. arXiv preprint arXiv:2204.07689. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap- tation of large language models. arXiv preprint arXiv:2106.09685. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Shuoran Jiang, Qingcai Chen, Youcheng Pan, Yang Xi- ang, Yukang Lin, Xiangping Wu, Chuanyi Liu, and Xiaobao Song. 2024. Zo-adamu optimizer: Adapt- ing perturbation by the momentum and uncertainty in zeroth-order optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 38, pages 18363–18371. Sarawoot Kongyoung, Craig Macdonald, and Iadh Ou- nis. 2020. Multi-task learning using dynamic task weighting for conversational question answering. In Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI), pages 17–26, Online. Association for Computational Lin- guistics. David Kozak, Cesare Molinari, Lorenzo Rosasco, Luis Tenorio, and Silvia Villa. 2023. Zeroth-order opti- mization with orthogonal random directions. Mathe- matical Programming, 199(1):1179–1219. Joyjit Kundu, Wenzhe Guo, Ali BanaGozar, Udari De Alwis, Sourav Sengupta, Puneet Gupta, and Arindam Mallik. 2024. Performance modeling and workload analysis of distributed large language model training and inference. In 2024 IEEE Inter- national Symposium on Workload Characterization (IISWC), pages 57–67. IEEE. Zeman Li, Xinwei Zhang, Peilin Zhong, Yuan Deng, Meisam Razaviyayn, and Vahab Mirrokni. 2024. Addax: Utilizing zeroth-order gradients to im- prove memory efficiency and performance of sgd for fine-tuning language models. arXiv preprint arXiv:2410.06441. Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. 2024a. Famo: Fast adaptive multitask optimization. Ad- vances in Neural Information Processing Systems, 36. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. 2021. Conflict-averse gradient descent for multi-task learning. Advances in Neural Information Processing Systems, 34:18878–18890. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2023. Moelora: An moe-based parameter efficient fine- tuning method for multi-task medical applications. arXiv preprint arXiv:2310.18339. Shengchao Liu, Yingyu Liang, and Anthony Gitter. 2019. Loss-balanced task weighting to reduce nega- tive transfer in multi-task learning. Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. 2020. A primer on zeroth-order optimization in sig- nal processing and machine learning: Principals, re- cent advances, and applications. IEEE Signal Pro- cessing Magazine, 37(5):43–54. Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, and Yang You. 2024b. Sparse mezo: Less parameters for better performance in zeroth-order llm fine-tuning. arXiv preprint arXiv:2402.15751. Debabrata Mahapatra and Vaibhav Rajan. 2020. Multi- task learning with user preferences: Gradient descent with controlled ascent in pareto optimization. In International Conference on Machine Learning. Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. 2023. Fine-tuning large language models with just forward passes. Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, and Pengtao Xie. 2022. MetaWeighting: Learning to weight tasks in multi-task learning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3436–3448, Dublin, Ireland. Association for Computational Linguistics. OpenAI. 2024. Gpt-4o. Available at https://www. openai.com/gpt-4o. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. P Rajpurkar. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. 10 Zhyar Rzgar K Rostam, Sándor Szénási, and Gábor Kertész. 2024. Achieving peak performance for large language models: A systematic review. IEEE Access. Ozan Sener and Vladlen Koltun. 2018. Multi-task learn- ing as multi-objective optimization. Advances in neural information processing systems, 31. Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen, and Xiao-Ming Wu. 2023. Recon: Reducing conflict- ing gradients from the root for multi-task learning. arXiv preprint arXiv:2302.11289. James C Spall. 1992. Multivariate stochastic approx- imation using a simultaneous perturbation gradient approximation. IEEE transactions on automatic con- trol, 37(3):332–341. Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A simple and effective pruning ap- proach for large language models. arXiv preprint arXiv:2306.11695. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman- preet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stick- ier benchmark for general-purpose language under- standing systems. Advances in neural information processing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for nat- ural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium. Association for Com- putational Linguistics. Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, and Changxing Ding. 2024. Simultaneous computa- tion and memory efficient zeroth-order optimizer for fine-tuning large language models. arXiv preprint arXiv:2410.09823. Yiming Wang, Yu Lin, Xiaodong Zeng, and Guan- nan Zhang. 2023. Multilora: Democratizing lora for better multi-task learning. arXiv preprint arXiv:2311.11501. Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guib- ing Guo, Xingwei Wang, and Dacheng Tao. 2023. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575. Yaming Yang, Dilxat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Denvy Deng, Feng Sun, Qi Zhang, et al. 2024a. Mtl-lora: Low-rank adaptation for multi-task learning. arXiv preprint arXiv:2410.09437. Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, and Zheng Zhang. 2024b. Adazeta: Adaptive zeroth-order tensor-train adaption for memory-efficient large language models fine-tuning. arXiv preprint arXiv:2406.18060. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782. Ziming Yu, Pan Zhou, Sike Wang, Jia Li, and Hua Huang. 2024. Subzero: Random subspace zeroth- order optimization for memory-efficient llm fine- tuning. arXiv preprint arXiv:2410.08989. Qi Zhang, Peiyao Xiao, Kaiyi Ji, and Shaofeng Zou. 2024a. On the convergence of multi-objective op- timization under generalized smoothness. arXiv preprint arXiv:2405.19440. Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Ja- son D Lee, Wotao Yin, Mingyi Hong, et al. 2024b. Revisiting zeroth-order optimization for memory- efficient llm fine-tuning: A benchmark. arXiv preprint arXiv:2402.11592. Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, and Meng Jiang. 2023. A survey of multi-task learn- ing in natural language processing: Regarding task relatedness and training methods. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 943–956, Dubrovnik, Croatia. Association for Com- putational Linguistics. Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, and Ivor W Tsang. 2024. Second-order fine-tuning without pain for llms: A hessian in- formed zeroth-order optimizer. arXiv preprint arXiv:2402.15173. Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, and Zheng Zhang. 2023. Tensor- compressed back-propagation-free training for (physics-informed) neural networks. arXiv preprint arXiv:2308.09858. Xiaojun Zhou, Yuan Gao, Chaojie Li, and Zhaoke Huang. 2022. A multiple gradient descent design for multi-task learning on edge computing: Multi- objective machine learning approach. IEEE Transac- tions on Network Science and Engineering, 9(1):121– 133. Yan Zhuang, Zhenzhe Zheng, Fan Wu, and Guihai Chen. 2024. Litemoe: Customizing on-device llm serving via proxy submodel tuning. In Proceedings of the 22nd ACM Conference on Embedded Networked Sen- sor Systems, SenSys ’24, page 521–534, New York, NY, USA. Association for Computing Machinery. 11 A Additional Explanation on Hessian and Gradient Approximation Consider a linear layer in an LLM that computes: y = Wx, (17) where W ∈Rm×n, x ∈Rn, and y ∈Rm. Fo- cusing on one particular linear component, let us analyze a single row wi ∈Rn of W. The corre- sponding output is given by: yi = wi x, (18) which is a scalar. To analyze the sensitivity of the loss L with re- spect to wi, we perform a second-order Taylor ex- pansion of L with respect to yi: L(yi + ∆yi) ≈L(yi) + ∇L(yi) ∆yi + 1 2∇2L(yi) (∆yi)2. (19) Since yi is a scalar, its second derivative ∇2L(yi) is also a scalar. Now, the change in yi due to a change in the weights is ∆yi = ∆wi x. (20) Substituting this into the second-order term yields: ∂2L ∂w2 i ≈∇2L(yi) (xx⊤). (21) Since we are primarily interested in comparing weight importance along the row direction, the ab- solute scale of the Hessian is not crucial. In prac- tice, we can drop the multiplicative factor ∇2L(yi) (or, equivalently, assume it to be a constant) and write: ∂2L ∂w2 i ∝xx⊤. (22) Similarly, one can derive a first-order approx- imation for the gradient. By retaining only the first-order term of the Taylor expansion, we have: L(yi + ∆yi) ≈L(yi) + ∇L(yi) ∆yi. (23) With ∆yi = ∆wi x, the gradient with respect to wi becomes: ∂L ∂wi ≈∇L(yi) x. (24) Similarly, since we are only interested in the rela- tive value, the factor is dropped: g ∝x. (25) Algorithm 1 MaZO LLM Fine-Tuning Framework Input: Pre-trained LLM parameters θ, train- ing data, tasks t = 1, . . . , T, sparsity level ρ, hyperparameters α, β, learning rate η Output: Updated parameters θ∗ for each task t = 1 to T do Collect evaluation data Compute St global with eq. (12) Compute St greedy with eq. (14) Combine scores: St = St global + α St greedy + β|W| Normalize St and get ˆSt with eq. (6) end for Aggregate: Sum row-wise normalized scores across tasks: S = T X t=1 ˆSt for each weight W in LLM do for each row i in W do Select top k parameters in each row according to corresponding S and construct weight update mask M end for end for for each training step do Compute weight update ∆W using ZO optimization Apply mask: ∆Wmasked = ∆W ⊙M Update parameters: θ ←θ + η∆Wmasked end for This derivation shows that, by considering each row independently (row-wise), we avoid the im- mense complexity involved in computing the full Hessian matrix (which is high-dimensional and dif- ficult to characterize even under diagonalization assumptions). In other words, computing the Hes- sian row-wise allows us to circumvent the problem of determining the eigenvalues or even a reliable diagonal approximation of the full Hessian. 12 B Pseudo-code of MaZO The pseudo-code of the whole MaZO LLM fine- tuning framework is shown as Algorithm 1. C Baseline C.1 CoBa: Convergence Balancer for Multitask Finetuning CoBa (Convergence Balancer) (Gong et al., 2024) is a novel multi-task learning (MTL) method de- signed for large language models (LLMs). It dy- namically adjusts task weights during training to ensure balanced convergence across tasks, while maintaining computational efficiency. Consider an LLM parameterized by θ ∈Rm, trained on T ≥2 tasks. The loss function for task t at iteration i is denoted as Lt(θ; i) : Rm →R≥0. The overall optimization objective is: min θ∈Rm L(θ; i) = T X t=1 ωt(i)Lt(θ; i), (26) where ωt(i) is the weight of task t at iteration i. A uniform weight assignment ωt(i) = 1 T ensures equal attention to all tasks but often leads to varying convergence rates. CoBa dynamically adjusts ωt(i) to balance these rates, prioritizing generalization by deriving weights from validation losses instead of training losses. CoBa is built upon three main components: Relative Convergence Score (RCS) dynam- ically allocates smaller weights to tasks that converge faster and larger weights to slower- converging tasks. It is computed as: RCSt(i) = softmaxt T αt(i) PT t′=1 |αt′(i)| ! , (27) where αt(i) is the convergence slope of task t, de- rived from the normalized validation loss ratio over a sliding window of N iterations. The softmax op- eration ensures differentiation across tasks, with faster-converging tasks receiving lower weights. Absolute Convergence Score (ACS) addresses task divergence by reducing weights for diverging tasks and increasing weights for converging tasks. It is computed as: ACSt(i) = softmaxt −N αt(i) Pi j=i−N+1 |αt(j)| ! , (28) where normalization is performed along the his- torical iteration dimension, isolating a task’s own trajectory. ACS ensures tasks with consistent con- vergence receive higher weights while diverging tasks are penalized. Divergence Factor (DF) determines the relative influence of RCS and ACS on the final task weights. It is defined as: DF(i) = min softmaxi i · αmax(i) Pi j=1 αmax(j) ! , 1 ! , (29) where αmax(i) is the largest convergence slope across all tasks at iteration i. DF ensures RCS dom- inates when all tasks are converging, while ACS takes precedence when divergence is detected. The final task weights ωt(i) are computed as: ωt(i) = DF(i) · RCSt(i) + (1 −DF(i)) · ACSt(i), (30) allowing a seamless transition between RCS and ACS dominance based on task convergence trends. The convergence slope αt(i) for task t is calcu- lated based on the normalized validation loss ratio ¯Lval t (θ; i). Specifically, we fit a linear model to the validation loss ratios over a sliding window of N iterations. The observations are defined as: xt(i) = [i, 1]⊤, Xt(N; i) = [xt(s0), . . . , xt(i)]⊤, (31) yt(N; i) = [ ¯Lval t (θ; s0), . . . , ¯Lval t (θ; i)]⊤, (32) where s0 = max(0, i −N + 1) is the starting step of the sliding window. The goal is to compute the coefficient vector ct(N; i) = [αt(N; i), βt(N; i)]⊤ that minimizes the mean squared error (MSE) be- tween the predicted and actual validation loss ra- tios: ct = arg min ct 1 2(Xtct −yt)⊤(Xtct −yt). (33) The closed-form solution for ct is given by: ct = (X⊤ t Xt)−1X⊤ t yt. (34) Algorithm The CoBa algorithm is summarized in Algorithm 2, We use M = 4 with batchsize = 16 C.2 FAMO: Fast Adaptive Multitask Optimization Fast Adaptive Multitask Optimization (FAMO) is a dynamic weighting method designed to address 13 Algorithm 2 CoBa Algorithm Require: Initial parameters θ0, M batches of val- idation set, history window length N = 5M, warm-up steps W = M, number of tasks T, initial weights ωi(0) = 1 T . Ensure: Trained parameters θ. 1: for t = 0 to T do 2: Compute L(θ; i) with training batch xi. 3: Compute ¯Lval t (θ; t) with validation batch vi. 4: Update validation loss history yt(N; i). 5: Compute αt(i). 6: if i > W then 7: Compute RCS(i), ACS(i), and DF(i) using Eqs. (27), (28), and (29). 8: Update task weights ωt(i) using Eq. (30). 9: else 10: Set ωt(i) = 1 T . 11: end if 12: Update model parameters θ using weighted loss L(θ; i). 13: end for the challenges of multitask learning (MTL), where directly optimizing the average loss across tasks often leads to under-optimization of certain tasks. FAMO ensures balanced task loss reduction using only O(1) space and time per iteration, making it computationally efficient and scalable. The complete FAMO algorithm is summarized in Algorithm 4. C.3 MTL-LoRA MTL-LoRA (Multi-Task Learning LoRA) is a parameter-efficient fine-tuning method designed to enhance the multi-task learning (MTL) capabil- ities of large language models (LLMs). It builds upon the Low-Rank Adaptation (LoRA) framework by addressing the challenges of task interference and suboptimal information sharing in multi-task scenarios. LoRA is a parameter-efficient fine-tuning method that freezes the majority of a pre-trained model’s parameters and introduces trainable low- rank matrices to approximate gradient updates. For a weight matrix W ∈Rd×k in the original model, LoRA decomposes the gradient update ∆W into two low-rank matrices B ∈Rd×r and A ∈Rr×k, where r ≪min(d, k). The updated weight matrix Algorithm 3 PyTorch Implementation of Wanda Input: Weight matrix W ∈RCout×Cin, input activations X ∈R(N·L)×Cin, sparsity ratio s ∈ [0, 1] Output: Pruned weight matrix W Compute importance scores: metric = W.abs() · X.norm(p = 2, dim = 0) Sort scores within each row: _, sorted_idx = torch.sort(metric, dim = 1) Identify indices to prune: pruned_idx = sorted_idx[:, : ⌊Cin · s⌋] Set pruned weights to zero: W.scatter_(dim = 1, index = pruned_idx, src = 0) Return W is expressed as: W′ = W + ∆W = W + BA. The output of the updated layer for an input x is: h = (W + BA)x. MTL-LoRA enhances LoRA by introduc- ing task-specific transformations and dynamic information-sharing strategies. Task-Specific Transformation. MTL-LoRA in- troduces a learnable task-specific transformation matrix Λt ∈Rr×r for each task t. For an input xt corresponding to task t, the low-rank projection is modified as: zt = ΛtAxt, where A ∈Rr×k is the shared low-rank matrix. Dynamic Information Sharing. To improve cross-task information sharing, MTL-LoRA em- ploys multiple up-projection matrices Bi ∈Rd×r (i = 1, . . . , n) and combines their outputs using a weighted averaging strategy. The final output for task t is computed as: ht = Wxt + n X i=1 exp(wt i/τ) Pn j=1 exp(wt j/τ)Bizt, where wt i are learnable weights for task t, and τ is a temperature hyperparameter controlling the sharpness of the weight distribution. We set number of up-projection matrices n to 3, rank to 16 and temperature τ to 0.5 14 0 5000 10000 15000 20000 25000 30000 Training Steps 0.72 0.74 0.76 0.78 0.80 0.82 0.84 Eval Loss 2, 0.800 4, 0.792 8, 0.771 16, 0.765 32, 0.766 64, 0.785 128, 0.812 256, 0.826 512, 0.842 Evaluation Loss vs Steps for Different LoRA Ranks with Minimum Points and Ideal Trend Line Minimum Points Trend Ideal Trend Figure 5: The loss curve with different LoRA rank. D LoRA Rank We investigate the influence of LoRA rank on the model’s final performance. Initially, we exclude weight masking and fine-tune the model with dif- ferent LoRA ranks. The evaluation loss curves for ranks ranging from 1 to 512 are plotted in Figure 5. As the rank increases, the loss forms a U-shaped curve, with the lowest point occurring at a rank of 16. Ideally, the trend of the lowest point in first- order (FO) optimization should follow the green dashed line in Figure 5. However, in the zeroth- order (ZO) setting, the larger parameter optimiza- tion space as rank increases leads to a deviation from this ideal trend. This U-shaped curve highlights a critical trade- off: while increasing the rank improves the model’s capacity, it simultaneously introduces challenges in optimizing a larger parameter space under ZO settings. This observation directly motivates our exploration of sparsity and mask selection strate- gies, which aim to reduce the number of parameters being optimized while retaining the most impor- tant ones. By identifying and focusing on the most critical parameters, we can mitigate the challenges posed by ZO optimization and achieve better per- formance, as demonstrated by our MaZO approach. E Memory Usage and Search Time To evaluate the efficiency of MaZO, we compare its memory usage, search time, and training time against baseline vanilla multi-task learning ZO"
            },
            {
                "section": "methods",
                "content": ", both with and without LoRA. Table 4 summarizes the results. The search time introduced by MaZO is negligi- ble compared to the overall training time. MaZO Method Memory (GB) Search Time (min) Training Time (h) MTL-ZO 29.0 - 14.3 MaZO 33.3 42 16.6 MTL-ZOLoRA 31.2 - 13.7 MaZOLoRA 33.9 8.5 14.1 Table 4: Comparison of memory usage, search time, and training time between MTL-ZO and MaZO, with and without LoRA. MTL refers to multi-task learning. While MaZO introduces marginal memory and runtime overhead due to the mask storage and search process, it achieves significantly better accuracy as shown in Tables 1 and 2, demonstrating its effectiveness and prac- ticality. Note that the memory requirement exceeds the model size (7B) because we use a batch size of 16 and a maximum token length of 600. incurs a slight increase in memory usage (approxi- mately 10%) compared to baseline multi-task learn- ing ZO methods. This is primarily due to the addi- tional storage required for the weight update mask. However, this increase is marginal and does not significantly impact the overall memory efficiency, especially when combined with LoRA, where the parameter space is already reduced. While MaZO introduces a small memory overhead, its benefits in terms of faster convergence and reduced gradient variance outweigh this cost, making it an effective and practical solution for multi-task fine-tuning under ZO optimization. F Details of Different Weight Score Metrics F.1 Wanda: Pruning by Weights and Activations In this section, we introduce Wanda (Pruning by Weights and Activations), a simple yet effective method for pruning large language models (LLMs). Wanda can induce high sparsity in pretrained LLMs without requiring retraining or weight updates, making it computationally efficient and easy to implement. The key idea of Wanda is to evaluate the impor- tance of each weight based on both its magnitude and the corresponding input activation. Specifi- cally, for a linear layer with weight matrix W ∈ RCout×Cin and input activations X ∈R(N·L)×Cin, the importance score Sij of weight Wij is defined as: Sij = |Wij| · ∥Xj∥2, (35) where |Wij| is the absolute value of the weight, and ∥Xj∥2 is the L2 norm of the j-th column of X, aggregated across all tokens in the batch and sequence dimensions. This metric effectively com- 15 bines weight magnitude and input activation infor- mation to determine the importance of each weight. Unlike traditional pruning methods that compare weights globally or layer-wise, Wanda adopts a per- output comparison strategy (the same as our row- wise comparison). For a weight Wij connecting input j to output i, its comparison group is defined as all weights connected to the same output i: Gij = {Wuv | u = i}. (36) Within each comparison group, weights are ranked by their importance scores Sij, and a predefined sparsity ratio s% is applied to prune the lowest- ranked weights. F.2 Other Metrics In this section, we introduce two additional heuris- tic weight importance metrics: random and magni- tude. For the random metric, we randomly select 50% of the weights. It is important to note that the comparison group is the entire set of weights, rather than a single row. For the magnitude metric, we select weights with the smallest values in a weight, following the ap- proach described by Liu et al. (2024b). G Task Details We consider a diverse set of natural language under- standing (NLU) and natural language generation (NLG) tasks. G.1 Natural Language Understanding Tasks We select tasks from the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks: • SST-2 (Stanford Sentiment Treebank): A bi- nary sentiment classification task. • BoolQ: A yes/no question-answering task. • RTE (Recognizing Textual Entailment): A bi- nary classification task for textual entailment. • WSC (Winograd Schema Challenge): A pro- noun resolution task. • WiC (Word-in-Context): A word sense disam- biguation task. • MultiRC (Multi-Sentence Reading Compre- hension): A question-answering task where each question has multiple correct answers. • COPA (Choice of Plausible Alternatives): A multiple-choice task for causal reasoning. G.2 Natural Language Generation Task For natural language generation, we include: • SQuAD (Rajpurkar, 2016): A question- answering dataset where the model generates text-based answers from a given passage. G.3 Dataset Splits and Evaluation Metrics To ensure computational feasibility, we randomly sample 500 instances for training, 250 for valida- tion, and 500 for testing for each task. Performance is measured using F1 score or accuracy, depending on the task. H"
            },
            {
                "section": "Discussion",
                "content": "of Collinearity Task-specific ZO gradients: For each task t ∈ {1, . . . , T}, the zeroth-order gradient estimate is given by gt = Lt(θ + ϵz) −Lt(θ −ϵz) 2ϵ z ≡αtz, (37) where αt is a scalar. Thus, every gt is a scalar multiple of the same random direction z. Span of all task gradients: The space spanned by the set of all task gradients is span{g1, g2, . . . , gT } = span{z}. (38) Therefore, the dimension of this span is dim \u0000span{g1, g2, . . . , gT } \u0001 = 1. (39) Aggregated gradient: The combined gradient used for the update is g = T X t=1 wtgt = T X t=1 wtαt ! z, (40) which clearly lies in the one-dimensional subspace spanned by z. Gradient covariance matrix: Define the covari- ance matrix of the task gradients as C = T X t=1 πt \u0000gt −¯g \u0001 \u0000gt −¯g \u0001⊤, (41) where πt are probability weights (or simply 1/T for uniform weighting) and the mean gradient is ¯g = T X t=1 πtgt. (42) 16 Since gt = αtz, we have gt −¯g = (αt −¯α)z, with ¯α = T X t=1 πtαt. (43) Thus, the covariance matrix becomes C = T X t=1 πt(αt −¯α)2 ! zz⊤. (44) Since zz⊤is an outer product of a vector with itself, it has rank 1. Hence, rank(C) = 1. (45)"
            },
            {
                "section": "Conclusion",
                "content": "The lack of directional diversity in the task gradients is mathematically captured by the fact that all task-specific gradients lie in a one- dimensional subspace, and the covariance matrix of these gradients has rank 1. This indicates that no matter how many tasks are aggregated, the update direction remains confined to a single direction z in the parameter space. Algorithm 4 Fast Adaptive Multitask Optimization (FAMO) Require: Initial model parameters θ0, task losses {Lt,i}T t=1, learning rates α and β, decay factor γ. 1: Initialize logits: ξ1 ←0. 2: for i = 1 to T do 3: Compute task weights: zi = Softmax(ξt), where for each i, zt,i = exp(ξt,i) PT t′=1 exp(ξt′,i) . 4: Update model parameters: θt+1 = θt −α T X t=1 \u0012 ct zt,i Lt,i \u0013 ∇Lt,i, with ci = k X i=1 zt,i Lt,i !−1 . 5: Compute the vector of log-loss differences: di =   log L1,i −log L1,i+1 ... log LT,i −log LT,i+1  . 6: Compute the Jacobian of the softmax func- tion: (Ji)tt′ = ∂zt,i ∂ξt′,i = zt,i(δtt′ −zt′,i). 7: Aggregate the gradient by the chain rule: δi = J⊤ i di. 8: Update logits: ξi+1 = ξi −β \u0000δi + γ ξi \u0001 . 9: end for 17"
            }
        ]
    },
    "Paper_9": {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning Rui Pan♠∗, Xiang Liu♣∗, Shizhe Diao♦, Renjie Pi♡, Jipeng Zhang♡, Chi Han♠, Tong Zhang♠",
        "sections": [
            {
                "section": "Abstract",
                "content": "The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory con- sumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter efficient fine-tuning techniques such as Low-Rank Adap- tation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank sub- space. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to differ- ent layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains. 1"
            },
            {
                "section": "Introduction",
                "content": "Large language models (LLMs) like ChatGPT excel in tasks such as writing documents, generating complex code, answering questions, and conducting human-like conversations [1]. With LLMs being increasingly applied in diverse task domains, domain-specific fine-tuning has emerged as a critical strategy to enhance their downstream capabilities [2, 3, 4, 5]. Nevertheless, these methods are typically time-intensive and consume substantial computational resources, posing significant challenges to the development of large-scale models [6]. For example, continual pre-training typically *Equal Contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). requires several weeks even with multiple 80 GB GPUs. To reduce costs, Parameter-Efficient Fine-Tuning (PEFT) techniques have been proposed to minimize the number of trainable parameters. 0 20 40 60 80 100 120 step 1.0 1.1 1.2 1.3 1.4 loss LISA GaLore LoRA GaLore LoRA LISA FT Figure 1: Training loss of LLaMA-2- 7B on Alpaca GPT-4. These techniques include adapter weights [7], prompt weights [8], and LoRA [9]. Among these, LoRA stands out as one of the most widely adopted due to its unique ability to merge the adaptor back into the base model parameters, significantly enhancing efficiency. However, LoRA’s su- perior performance in fine-tuning tasks has yet to reach a point that universally surpasses full parameter fine-tuning in all settings [10, 11]. In particular, it has been observed that LoRA tends to falter on large-scale datasets during continual pre-training [12], which raises doubts about the effectiveness of LoRA under those circumstances. We at- tribute this to LoRA’s much fewer trainable parameters compared to the base model, which limits the representa- tion power of LoRA training. To overcome this shortcoming, we delve into LoRA’s training statistics in each layer, aspiring to bridge the difference between LoRA and full-parameter fine-tuning. Surprisingly, we discover that LoRA’s layerwise weight norms have an uncommonly skewed distribution, where the bottom layer and/or the top layer occupy the majority of weights during the update. In contrast, the other self-attention layers only account for a small amount, which means different layers have different importance when updating. This key observation inspires us to “sample” different layers by their importance, which matches the idea of importance sampling [13, 14]. As a natural consequence, this strategy brings forth our Layerwise Importance Sampled Adam (LISA) algorithm, where by selectively updating only essential LLM layers and leaving others untouched, LISA enables training large-scale language models (≥65B parameters) with less or similar memory consumption as LoRA. Furthermore, fine-tuned on downstream tasks, LISA outperformed both LoRA and conventional full-parameter fine-tuning approaches by a large margin, indicating the large potential of LISA as a promising alternative to LoRA. We summarize our key contributions as follows, • We discover the phenomenon of skewed weight-norm distribution across layers in LoRA, which implies the varied importance of different layers in large-scale LLM training. • We propose the Layerwise Importance Sampled AdamW (LISA), a simple optimization method capable of scaling up to over 70B LLMs with less or similar memory cost as LoRA. • We demonstrate LISA’s effectiveness in fine-tuning tasks for modern LLMs, where it outperforms LoRA by 10%-35% in MT-Bench and achieves better performance in multiple benchmarks. In addition, LISA exhibits much better convergence behaviors than LoRA. LISA even outperforms full parameters training under certain settings. Similar performance gain is observed across different sized models (7B-70B) and tasks, including instruction following, medical QA, and math problems. 2 Related Work 2.1 Large Language Models In the realm of natural language processing (NLP), the Transformer architecture has been a revolu- tionary technique, initially known for its effectiveness in machine translation tasks [15]. With the inception of models like BERT [16] and GPT-2 [17], the approach shifted towards pre-training on extensive corpora, which led to significant performance enhancements in downstream fine-tuning tasks [2, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]. However, the growing number of parameters in these models results in a huge GPU memory consumption, rendering the fine-tuning of large scale models (≥65B) infeasible under low resource scenarios. This has prompted a shift towards more efficient training of LLMs. 2 2.2 Parameter-Effieient Fine-Tuning Parameter-efficient fine-tuning (PEFT) methods adapt pre-trained models by fine-tuning only a subset of parameters. In general, PEFT methods can be grouped into three classes: 1) Prompt Learning methods [8, 28, 29, 30, 31, 32, 33], 2) Adapter methods [7, 9, 34, 35, 36, 37, 38], and 3) Selective methods [39, 39, 40, 41]. Prompt learning methods emphasize optimizing the input token or input embedding with frozen model parameters, which generally has the least training cost among all three types. Adapter methods normally introduce an auxiliary module with much fewer parameters than the original model, and updates are only applied to the adapter module during training. Compared with them, selective methods are more closely related to LISA, which focuses on optimizing a fraction of the model’s parameters without appending extra modules. Recent advances in this domain have introduced several notable techniques through layer freezing. AutoFreeze [39] offers an adaptive mechanism to identify layers for freezing automatically and accelerates the training process. FreezeOut [42] progressively freezes intermediate layers, significantly reducing training time without notably affecting accuracy. The SmartFRZ [40] framework utilizes an attention-based predictor for layer selection, substantially cutting computation and training time while maintaining accuracy. However, none of these layer-freezing strategies has been widely adopted in the context of Large Language Models due to their inherent complexity or non-compatibility with modern memory reduction techniques [43, 44, 27] for LLMs. 2.3 Low-Rank Adaptation (LoRA) In contrast, the Low-Rank Adaptation (LoRA) technique is much more prevalent in common LLM training [9]. LoRA reduces the number of trainable parameters by employing low-rank matrices, thereby lessening the computational burden and memory cost. One key strength of LoRA is its compatibility with models featuring linear layers, where the decomposed low-rank matrices can be merged back into the original model. This allows for efficient deployment without changing the model architecture. As a result, LoRA can be seamlessly combined with other techniques, such as quantization [11] or Mixture of Experts [45]. Despite these advantages, LoRA’s performance is not universally comparable with full parameter fine-tuning. There have been tasks in [10] that LoRA performs much worse than full parameter training on. This phenomenon is especially evident in large-scale pre-training settings [12], where to the best of our knowledge, only full parameter training was adopted for successful open-source LLMs [21, 22, 23, 46, 47, 26, 27]. 2.4 Large-scale Optimization Algorithms In addition to approaches that change model architectures, there have also been efforts to improve the efficiency of optimization algorithms for LLMs. One such approach is layerwise optimization, a concept with roots extending back several decades. Notably, [48] introduced an effective layer-by- layer pre-training method for Deep Belief Networks (DBN), demonstrating the benefits of sequential layer optimization. This idea was expanded by researchers like [49, 50], who illustrated the advantages of a greedy, unsupervised approach to pre-training each layer of deep networks. In the context of large batch training, [51, 52] developed LARS and LAMB to improve generalization and mitigate the performance declines associated with large batch sizes. Despite these innovations, Adam [53, 54, 55, 27] and AdamW [56] continue to be the predominant optimization methods used in most LLM settings. Recently, other attempts have also been made to reduce the training cost of LLMs. For example, MeZO [57] adopted zeroth order optimization, bringing significant memory savings during training. However, it also incurred a considerable performance drop in multiple benchmarks, particularly in complex fine-tuning scenarios. Regarding acceleration, Sophia [58] incorporates clipped second-order information into the optimization, obtaining non-trivial speedup on LLM training. The significant downsides are its intrinsic complexity of Hessian estimation and unverified empirical performance in large-size models (e.g., ≥65B). In parallel to our work, [59] proposed GaLore, a memory-efficient training strategy that reduces memory cost by projecting gradients into a low-rank compact space. Yet the performance has still not surpassed full-parameter training in fine-tuning settings. To sum up, LoRA-variant methods [9, 11, 59] with AdamW [56] is still the dominant paradigm for large-size LLM fine-tuning, the performance of which still demands further improvements. 3 3 Method 3.1 Motivation To understand how LoRA achieves effective training with only a few parameters, we conducted empirical studies on multiple models, especially observing the weight norms across various layers. We fine-tune it on the Alpaca-GPT4 dataset [60]. During the training, we meticulously recorded the mean weight norms of each layer ℓat every step t after updates, i.e. w(ℓ) ≜mean-weight-norm(ℓ) = 1 T T X t=1 ∥θ(ℓ) t ∥2 Figure 2 presents these findings, with the x-axis representing the layer id, from embedding weights to the final layer, and the y-axis quantifying the weight norm. The visualization reveals one key trend: • The embedding layer or the language modeling (LM) head layer exhibits significantly larger weight norms than intermediary layers in LoRA, often by a factor of hundreds. This phenomenon, however, was not salient under full-parameter training settings. token embedding position embedding h.0 h.1 h.2 h.3 h.4 h.5 h.6 h.7 h.8 h.9 h.10 h.11 lm head Model Layers 10 1 10 2 10 3 Weight Norm Weight Norm of Each Layer For GPT2 GPT2 with Full Parameters GPT2 with LORA token embedding layers.0 layers.5 layers.10 layers.15 layers.20 layers.25 layers.30 lm head Model Layers 10 0 10 1 10 2 10 3 Weight Norm Weight Norm of Each Layer For LLaMA-2-7B LLaMA-2-7B with Full Parameters LLaMA-2-7B with LORA Figure 2: Layer-wise weight norms during training of GPT2 and LLaMA-2-7B Model with LoRA and Full Parameters training. This observation indicates that the update emphasis of LoRA and full parameter training differ significantly, which can be attributed to the difference in their learned knowledge. For example, in embedding layers, tokens with similar meanings, i.e., synonyms, can be projected into the same embedding space and converted to similar embeddings. LoRA may capture this similarity in language and “group” them in the low-dimension space, allowing frequent features of language meanings to be promptly identified and optimized. The price is LoRA’s limited representation power restricted by its intrinsic low-rank space, as we can see from the comparison with LISA in image generation tasks (Appendix A.1), where LoRA memorizes and learns details much slower than LISA. Other possible explanations can also justify this phenomenon. Despite various interpretations of this observation, one fact remains clear: LoRA values layerwise importance differently from full parameter tuning. 3.2 Layerwise Importance Sampled AdamW (LISA) To exploit the discovery above, we aspire to simulate LoRA’s updating pattern via sampling different layers to freeze. This way, we can avoid LoRA’s inherent deficiency of limited low-rank representation ability and emulate its fast learning process. Intuitively, given the same global learning rates across layers, layers with small weight norms in LoRA should also have small sampling probabilities to unfreeze in full-parameter settings so the expected learning rates across iterations can stay the same. This is exactly the idea of importance sampling [13, 14], where instead of applying layerwise different learning rates {ηt} in full-parameter settings to emulate LoRA’s updates {˜ηt}, we apply sampling and instead get the same expected parameter update η(ℓ) t = ˜η(ℓ) t · ˜w(ℓ) w(ℓ) ⇒ η(ℓ) t = η(ℓ), p(ℓ) = ˜w(ℓ) w(ℓ) 4 This gives rise to our Layerwise Importance Sampling AdamW method, as illustrated in Algorithm 1. In practice, since all layers except the bottom and top layer have small weight norms in LoRA, we adopt {pℓ}NL ℓ=1 = {1.0, γ/NL, γ/NL, . . . , γ/NL, 1.0} in practice, where γ controls the expected number of unfreeze layers during optimization, and the embedding layer E and head layer H remain active. Intuitively, γ serves as a compensation factor to bridge the difference between LoRA and full parameter tuning, letting LISA emulate a similar layerwise update pattern as LoRA. To further control the memory consumption in practical settings, we instead randomly sample γ layers every time to upper-bound the maximum number of unfrozen layers during training. Algorithm 1 Layerwise Importance Sampling AdamW (LISA) Require: number of layers NL, number of iterations T, sampling period K, number of sampled layers γ, initial learning rate η0 1: for i ←0 to T/K −1 do 2: Freeze all layers except the embedding and language modeling head layer 3: Randomly sample γ intermediate layers to unfreeze 4: Run AdamW for K iterations with {ηt}ik+k−1 t=ik 5: end for 4 Experimental Results Table 1: The chart illustrates peak GPU memory consumption for various model architectures and configurations, highlighting differences across models. The LISA configuration is specifically labeled in the table: “E” denotes the embedding layer, “H” represents the language modeling head layer, and “2L” indicates two additional intermediate layers. *: Model parallelism is applied for the 70B model. VANILLA LORA RANK LISA ACTIVATE LAYERS MODEL - 128 256 512 E+H E+H+2L E+H+4L GPT2-SMALL 3.8G 3.3G 3.5G 3.7G 3.3G 3.3G 3.4G TINYLLAMA 13G 7.9G 8.6G 10G 7.4G 8.0G 8.3G MISTRAL-7B 59G 23G 26G 28G 21G 23G 24G LLAMA-2-7B 59G 23G 26G 28G 21G 23G 24G LLAMA-2-70B* OOM 79G OOM OOM 71G 75G 79G 4.1 Memory Efficiency We conducted peak GPU memory experiments to demonstrate LISA’s memory efficiency and show- case its comparable or lower memory cost than LoRA. Figure 3: GPU memory consumption of LLaMA- 2-7B with different methods and batch size 1. Settings To reasonably estimate the memory cost, we randomly sample prompts from the Alpaca dataset [61] and limit the maximum out- put token length to 1024. We focus on two key hyperparameters: LoRA’s rank and LISA’s number of activation layers. For other hyper- parameters, a mini-batch size of 1 was consis- tently used across five LLMs from 120M to 70B parameters, deliberately excluding other GPU memory-saving techniques such as gra- dient checkpointing [62], offloading [63], and flash attention [64, 65]. All memory-efficiency experiments are conducted on 4× NVIDIA Am- pere Architecture GPUs with 80G memory."
            },
            {
                "section": "Results",
                "content": "Upon examining Table 1, it is evident that the LISA configuration, particularly when enhanced with both the embedding layer (E) and two additional layers (E+H+2L), demonstrates a considerable reduction in GPU memory usage when fine-tuning the LLaMA-2-70B model, as compared to the LoRA method. Specifically, the LISA E+H+2L configuration shows a decrease to 75G of peak GPU memory from the 79G required by the 5 LoRA Rank 128 configuration. This efficiency gain is not an isolated incident; a systematic memory usage decrease is observed across various model architectures, suggesting that LISA’s method of activating layers is inherently more memory-efficient. In Figure 3, it is worth noticing that the memory reduction in LISA allows LLaMA-2-7B to be trained on a single RTX4090 (24GB) GPU, which makes high-quality fine-tuning affordable even on a laptop computer. In particular, LISA requires much less activation memory consumption than LoRA since it does not introduce additional parameters brought by the adaptor. LISA’s activation memory is even slightly less than full parameter training since pytorch [66] with deepspeed [44] allows deletion of redundant activations before backpropagation. Figure 4: Single-iteration time cost of LLaMA-2- 7B with different methods and batch size 1. On top of that, a reduction in memory foot- print from LISA also leads to an acceleration in speed. As shown in Figure 4, LISA provides almost 2.9× speedup when compared with full- parameter training, and ∼1.5× speedup against LoRA, partially due to the removal of adaptor structures. It is worth noticing that the reduc- tion of memory footprint in both LoRA and LISA leads to a significant acceleration of for- ward propagation, emphasizing the importance of memory-efficient training. 4.2 Moderate Scale Fine-Tuning LISA can achieve this significant memory sav- ing while still obtaining competitive perfor- mance under the fine-tuning setting. Table 2: Results of different methods on MMLU, AGIEval, and WinoGrande, measured by accuracy. MODEL METHOD MMLU (5-SHOT) AGIEVAL (3-SHOT) WINOGRANDE (5-SHOT) TINYLLAMA VANILLA 25.50 19.55 59.91 LORA 25.81 ± 0.07 19.82 ± 0.11 61.33 ± 0.09 GALORE 25.21 ± 0.06 21.19 ± 0.07 61.09 ± 0.12 LISA 26.02 ± 0.13 21.71 ± 0.09 61.48 ± 0.08 FT 25.62 ± 0.10 21.28 ± 0.07 62.12 ± 0.15 MISTRAL-7B VANILLA 60.12 26.79 79.24 LORA 61.78 ± 0.09 27.56 ± 0.07 78.85 ± 0.11 GALORE 57.87 ± 0.08 26.23 ± 0.05 75.85 ± 0.13 LISA 62.09 ± 0.10 29.76 ± 0.09 78.93 ± 0.08 FT 61.70 ± 0.13 28.07 ± 0.12 78.85 ± 0.12 LLAMA-2-7B VANILLA 45.87 25.69 74.11 LORA 45.50 ± 0.07 24.73 ±0.04 74.74 ± 0.09 GALORE 45.56 ± 0.05 24.39 ± 0.11 73.32 ± 0.12 LISA 46.21 ± 0.12 26.06 ± 0.08 75.30 ± 0.11 FT 45.66 ± 0.09 27.02 ± 0.10 75.06 ± 0.13 Settings To demonstrate the superiority of LISA over LoRA, we evaluate them on the instruction- following fine-tuning task with the Alpaca GPT-4 dataset [61], which consists of 52k conversation pairs generated by GPT-4 [5]. The effectiveness of fine-tuning was evaluated on multiple benchmarks: MT-Bench [67] features 80 high-quality, multi-turn questions designed to assess LLMs on multiple aspects; MMLU [68] includes a total of 57 tasks with 14,079 questions covering a broad spectrum of world knowledge; AGIEval [69] serves as a human-centric benchmark for general abilities, comprising 9,316 instances; WinoGrande [70] is a large-scale dataset for commonsense reasoning, consisting of 44,000 instances designed to challenge models’ understanding of the context and commonsense knowledge. In our experiments, we assessed three baseline models: TinyLlama [47], Mistral-7B [46], and LLaMA-2-7B [23]. These models, varying in size ranging from 1B to 7B parameters, provide a 6 diverse representation of decoder-only models. For hyper-parameters, we adopt a rank of 128 for LoRA and E+H+2L for LISA in this section, with full details available in Appendix B. Table 3: Different methods on MT-Bench. MODEL METHOD MT-BENCH ↑ TINYLLAMA VANILLA 1.25 LORA 1.90 ± 0.14 GALORE 2.61 ± 0.17 LISA 2.57 ± 0.25 FT 2.21 ± 0.16 MISTRAL-7B VANILLA 4.32 LORA 4.41 ± 0.09 GALORE 4.36 ± 0.16 LISA 4.85 ± 0.14 FT 4.64 ± 0.12 LLAMA-2-7B VANILLA 3.29 LORA 4.45 ± 0.15 GALORE 4.63 ± 0.09 LISA 4.94 ± 0.14 FT 4.75 ± 0.16"
            },
            {
                "section": "Results",
                "content": "Table2 and 3 present a detailed compari- son on moderate-scale LLMs. The baselines include Full-parameter Training (FT), Low-Rank Adaptation (LoRA) [9] and Gradient Low-Rank Projection (Ga- Lore) [59]. The results demonstrate that LISA consis- tently outperforms other fine-tuning methods in most evaluation tracks, indicating its robustness and effec- tiveness across diverse tasks and model architectures. LISA is particularly effective in instruction follow- ing tasks, where a large gap is observed when com- pared with other baseline methods. LISA even out- performs Full-parameter Training, suggesting that an implicit regularization effect is present when the num- ber of unfrozen layers is restricted, which is similar to dropout [71]. According to more results in stable diffusion and detailed MT-Bench scores, we found that LISA outperforms LoRA mostly in memorization tasks, such as depicting high-resolution image details in image generation, or Writing or Humanities tasks in instruction following. This implies that LISA’s performance improvement may majorly come from the ability to memorize long-tailed patterns, while LoRA is better at multi-hop reasoning with limited knowledge. For more details, please refer to Appendix A.1 and A.2. 4.3 Moderate Scale Continual Pre-training Table 4: Comparison of Moderate Scale Model Continual Pre-training on OpenWebMath Dataset. MODEL METHOD GSM8K ↑ MEM. ↓ TINYLLAMA VANILLA 2.26 - LISA 3.56 8G FT 3.26 13G LLAMA-2-7B VANILLA 14.40 - LISA 22.21 26G FT 22.21 59G Continual pre-training is crucial for enabling models to adapt to new data and domains. To evaluate LISA’s efficacy in the continual pre-training scenario, we experiment on the mathematics domain in comparison with Full- parameter Training. Settings We adopt the mathematics corpus OpenWebMath [72] for constructing the con- tinual pre-training dataset. Specifically, we ex- tracted a high-quality subset from it which con- tains 1.5 billion tokens. Full details are ex- plained in Appendix B.2. After continual pre- trainig, we then apply the same fine-tuning procedure on the GSM8K [73] training set, which comprises 7473 instances."
            },
            {
                "section": "Results",
                "content": "Table 4 shows that LISA is capable of achieving on-par or even better performance than full-parameter training with much less memory consumption. Specifically, LISA requires only half of the memory cost compared to full-parameter training. This indicates a better balance between computational efficiency and model performance is achieved by LISA. According to our experience, reducing the number of unfrozen layers to half the original size leads to no worse or even better performance during continual pretraining, while requiring much less memory consumption. 4.4 Large Scale Fine-Tuning To further demonstrate LISA’s scalability on large-sized LLMs, we conduct additional fine-tuning experiments on LLaMA-2-70B [23]. Settings On top of the aforementioned instruction-following tasks in Section 4.2, we use extra domain-specific fine-tuning tasks on mathematics and medical QA benchmarks. The GSM8K dataset [73], comprising 7473 training instances and 1319 test instances, is used for the mathematics 7 domain. For the medical domain, we select the PubMedQA dataset [74], which includes 211.3K artificially generated QA training instances and 1K test instances. Table 5: Different methods on MT-Bench, GSM8K, and PubMedQA score for LLaMA-2-70B. METHOD MT-BENCH↑ GSM8K↑ PUBMEDQA↑ VANILLA 5.19 54.8 83.0 LORA 6.10 59.4 90.8 LISA 6.72 61.1 91.6 FT 6.25 67.1 90.8 Evaluation on the PubMedQA dataset [74] is conducted in a 5-shot prompt setting, while the GSM8K dataset [73] assessment was con- ducted using Chain-of-Thought (CoT) prompt- ing, following recent studies [75, 76, 77]. Re- garding hyperparameters, as detailed in the section 4.1, we utilize the rank 256 for LoRA and the configuration E+H+4L for LISA. Fur- ther information is available in Appendix B."
            },
            {
                "section": "Results",
                "content": "As shown in Table 5, LISA consis- tently produces better or on-par performance when compared with LoRA. Furthermore, LISA again surpasses full-parameter training in instruction-tuning tasks, providing strong evidence to support LISA’s scalability under large-scale training scenarios. More results are available in Appendix A.2. 4.5 Ablation Studies Table 6: Different LISA hyperparameters combina- tions. All settings adopt learning rate η0 = 10−5. Here γ stands for sampling layers, K stands for sampling period. MODELS γ K MT-BENCH SCORE TINYLLAMA 2 ⌈T/125⌉ 2.44 ⌈T/25⌉ 2.73 ⌈T/5⌉ 2.64 T 2.26 8 ⌈T/125⌉ 2.59 ⌈T/25⌉ 2.81 ⌈T/5⌉ 2.74 T 2.53 LLAMA-2-7B 2 ⌈T/125⌉ 4.86 ⌈T/25⌉ 4.91 ⌈T/5⌉ 4.88 T 4.64 8 ⌈T/125⌉ 4.94 ⌈T/25⌉ 5.11 ⌈T/5⌉ 5.01 T 4.73 Hyperparameters of LISA The two key hy- perparameters of LISA are the number of sam- pling layers γ and sampling period K. To obtain intuitive and empirical guidance of those hyper- parameter choices, we conduct ablation studies using TinyLlama [47] and LLaMA-2-7B [23] models with the Alpaca-GPT4 dataset. The con- figurations for γ, such as E+H+2L, E+H+8L, were denoted as γ = 2 and γ = 8. As for the sampling period K = T/n, T = 122 represent- ing the maximum training step within our exper- imental framework. The findings, presented in Table 6, reveal that both γ and K markedly af- fect the LISA algorithm’s performance. Specif- ically, a higher γ value increases the quantity of trainable parameters, albeit with higher memory costs. On the other hand, an optimal K value fa- cilitates more frequent layer switching, thereby improving performance to a certain threshold, beyond which the performance may deteriorate. Generally, the rule of thumb is: More sampling layers and higher sampling period lead to bet- ter performance. For a detailed examination of loss curves and MT-Bench results, refer to Appendix A.4. Table 7: The MT-Bench scores derived from vary- ing random seeds for layer selection. MODEL SEED 1 SEED 2 SEED 3 TINYLLAMA 2.57 2.55 2.60 MISTRAL-7B 4.85 4.82 4.82 LLAMA-2-7B 4.94 4.92 4.89 Sensitiveness of LISA As LISA is algorith- mically dependent on the sampling sequence of layers, it is intriguing to see how stable LISA’s performance is under the effect of randomness. For this purpose, we further investigate LISA’s performance variance over three distinct runs, each with a different random seed for layer selec- tion. Here, we adopt TinyLlama, LLaMA-2-7B, and Mistral-7B models with the Alpaca-GPT4 dataset while keeping all other hyperparameters consistent with those used in the instruction follow- ing experiments in section 4.2. As shown in Table 7, LISA is quite resilient to different random seeds, where the performance gap across three runs is within 0.13, a small value compared to the 8 performance gains over baseline methods. For more ablation experiment on LISA hyperparameters, please refer to Appendix A.4. 5"
            },
            {
                "section": "Discussion",
                "content": "Theoretical Properties of LISA Compared with LoRA, which introduces additional parameters and leads to changes in loss objectives, layerwise importance sampling methods enjoy nice conver- gence guarantees in the original loss. For layerwise importance sampled SGD, similar to gradient sparsification [78, 55], the convergence can still be guaranteed for unbiased estimation of gradients with increased variance. The convergence behavior can be further improved by reducing the variance with appropriately defined importance sampling strategy [14]. For layerwise importance sampled Adam, theoretical results in [79, 55] prove its convergence in convex objectives. If we denote f as the loss function and assume that the stochastic gradients are bounded, then based on [56], we know that AdamW optimizing f aligns with Adam optimizing f with a scaled regularizer, which can be written as f reg(w) ≜f(w) + 1 2w⊤Sw, where S is a finite positive semidefinite diagonal matrix. Following existing convergence results of RBC-Adam (Corollary 1 in [79]), we have the convergence guarantee of LISA in Theorem 1. Theorem 1 Let the loss function f be convex and smooth. If the algorithm runs in a bounded convex set and the stochastic gradients are bounded, the sequence {wt}T t=1 generated by LISA admits the following convergence rate: 1 T T X t=1 f reg(wt) −f reg ∗ ≤O \u0012 1 √ T \u0013 , where f reg ∗ denotes the optimum value of f reg. Memorization and Reasoning In our instruction following experiments in Appendix A.1 and A.2, we observe that LISA is much better than LoRA at memorization-centered tasks, such as Writing or depicting image details, while this gap is much smaller in reasoning-centered tasks like Code or Math. It is an intriguing observation since LISA emphasizes more on layer-wise width and restricts the depth of learned parameters, while LoRA focuses more on depth and restricts the representation space in each layer. It may suggest that width is crucial for memorization, while depth is important for reasoning, a similar phenomenon that echos the intuition of [80]. Based on the same intuition, it may be possible to combine the benefits of both and bring forth an even better PEFT method. 6"
            },
            {
                "section": "Conclusion",
                "content": "In this paper, we propose Layerwise Importance Sampled AdamW (LISA), an optimization algorithm that randomly freezes layers of LLM based on a given probability. Inspired by observations of LoRA’s skewed weight norm distribution, a simple and memory-efficient freezing paradigm is introduced for LLM training. This paradigm achieves significant performance improvements over LoRA on downstream fine-tuning tasks with various models, including LLaMA-2-70B. Further experiments on domain-specific training also demonstrate its effectiveness, showing LISA’s huge potential as a promising alternative to LoRA for LLM training. Limitations The major bottleneck of LISA is the same as LoRA, where during optimization, the forward pass still requires the model to be presented in the memory, leading to significant memory consumption. This limitation shall be compensated by approaches similar to QLoRA [11], where we intend to conduct further experiments to verify its performance. In addition, as suggested by the theoretical intuition, the strategy of E+H+2L in Section 4.2 and E+H+4L in Section 4.4 may not be the optimal importance sampling strategy, given it still sampled 9 intermediate layers in a uniformly random fashion. We anticipate the optimizer’s efficiency will be further improved when considering data sources and model architecture in the importance sampling procedure."
            },
            {
                "section": "results",
                "content": "in distinct training trajectories, their conver- gence points are remarkably similar. This finding im- plies that for a 7B model trained on a 52K instruction conversation pair dataset, a sampling period of K = 13 is optimal for achieving the best loss curve and corresponding MT-Bench score radar graph. A.4.3 Sensitiveness to Randomness LLaMA-2-7B on Alpaca-GPT4 with update step per sampling period K = 13, and sampling layers γ = 2, run three times with different random layer pick. Figure 10 shows that different random selections of layers slightly affect the training process but converge similarly. Despite initial fluctuations, the loss trends of three runs—distinguished by blue, green, and red lines—demonstrate that the model consistently reaches a stable state, underscoring the robustness of the training against the randomness in layer selection. Figure 10: Comparison of loss curves for random variance ablation experiment, indicating the loss metric over steps. Table 12: Compare LISA with fixed lay- ers on LLaMA-2-7B, evaluate on MT- Bench. METHOD MT-BENCH ↑ LISA 4.94 LISA-FIX-1 4.62 LISA-FIX-2 4.60 LISA-FIX-3 4.67 Additionally, we also conduct experiments that analyze the impact of fixing randomly selected layers during train- ing. The suggestion highlighted a need to evaluate the model’s stability and performance under such conditions. To address this, we conducted further experiments using the LLaMA-2-7B model, maintaining identical hyperpa- rameters and datasets as in our paper, and repeated the experiments three times to reduce variability from the ran- dom selection process. 22 The results are presented as table 12, with ’LISA-fix’ denoting the experiments with randomly fixed layers and the appended number indicating the different selected seeds. The results are presented as table 12, with ’LISA-fix’ denoting the experiments with randomly fixed layers and the appended number indicating the different selected seeds. A.5 Performance with Early Exiting Table 13: GSM8K Scores for LLaMA-2-7B when LISA meets the early exiting strategy DoLa. METHOD GSM8K % ↑ VANILLA 15 VANILLA + DOLA 11 FT + DOLA 16 LISA + DOLA 17 According to previous works [83, 84], the early exiting strategy in LLMs is effective. We are interested in investigating whether the LISA algorithm will have a different impact when combined with the early exiting strategy. To explore this, we conducted a series of experiments using the LLaMA-2-7B model, focusing on various training methods and early exit points. The early exiting method is DoLa [83]. We conducted all of our experiments using the LLaMA-2-7B model, selecting layers [0, 8, 16, 32] as early exit points for evaluation under four different training conditions: Vanilla (baseline), Vanilla with DoLa, Full Parameter Fine-Tuning (FT) with DoLa, and LISA with DoLa. The model used is the same one trained on the Alpaca-GPT4 dataset, as reported in section 4.2. This comprehensive approach enabled a detailed comparison of the model’s performance and layer representation capabilities under various training methodologies. Due to time constraints, our experiments were conducted on a subset of 100 questions from the GSM8K test set, employing the same prompt as in the DoLa paper. From the table 13, it can be seen that the LISA algorithm does not negatively affect the representation or performance of some layers of the model; instead, it contributes to some improvements in effectiveness. A.6 Comparison of Evaluation Loss Besides the training loss, we also care about how LISA performs on the validation dataset. So, we split the Alpaca-GPT4 dataset into train and validation sets, the ratio is 9 : 1, and ensure there is no data overlap between these two sets. Then we use the same setting in the sec 4.2, training the LLaMA-2-7B on the Alpaca-GPT4 dataset with full parameter training (FT), LoRA, GaLore, and LISA. As Figure 11 shows, the trend in validation loss mirrors that observed in training loss, with LISA exhibiting no signs of overfitting. This consistency underlines LISA’s robustness in maintaining performance across different dataset splits. 0 20 40 60 80 100 120 Step 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 Loss LISA GaLore LoRA Comparison of Validation Loss Curves on Alpaca-GPT4 Dataset for LLaMA-2-7B GaLore LoRA LISA FT Figure 11: Validation loss comparison on the Alpaca-GPT4 dataset for LLaMA-2-7B, showing LISA, GaLore, LoRA, and FT strategies, with arrows indicating specific observations in the loss trends. 23 Table 15: The statistics of datasets. # TRAIN and # TEST denote the number of training and test samples respectively. The unit for OpenWebMath is the number of documents. DATASET # TRAIN # TEST ALPACA GPT-4[60] 52,000 - MT-BENCH [67] - 80 GSM8K [73] 7,473 1,319 MMLU [68] - 14,079 AGIEVAL [69] - 9316 WINOGRANDE [70] - 44,000 PUBMEDQA [74] 211,269 1,000 OPENWEBMATH [72] 6.3M - A.7 Additional Observations of Layerwise Skewness We conduct further weight norm experiments on Mistral-7 B to support our motivation that the bottom and top layers have a more significant impact on the output. Figure 12 provides similar observations as LLaMA-2-7B, where the bottom layer has a larger weight norm than other layers. embed_tokens layers.0 layers.1 layers.2 layers.3 layers.4 layers.5 layers.6 layers.7 layers.8 layers.9 layers.10 layers.11 layers.12 layers.13 layers.14 layers.15 layers.16 layers.17 layers.18 layers.19 layers.20 layers.21 layers.22 layers.23 layers.24 layers.25 layers.26 layers.27 layers.28 layers.29 layers.30 layers.31 lm_head Model Layers 10 0 10 1 10 2 10 3 Weight Norm Weight Norm of Each Layer For Mistral-7B Mistral-7B with Full Parameters Mistral-7B with LORA Figure 12: Layer-wise weight norms during training of Mistral-7B with LoRA and Full Parameters training. B Training Setup and Hyperparameters B.1 Training Setup Table 14: Baseline Model Specifications MODEL NAME # PARAMS # LAYERS MODEL DIM # HEADS TINYLLAMA 1.1 B 22 2048 32 MISTRAL-7B 7 B 32 4096 32 LLAMA-2-7B 7 B 32 4096 32 LLAMA-2-70B 70 B 80 8192 64 In our experiments, we employ the LMFlow toolkit [85]* for conducting full parameter fine-tuning, LoRA tun- ing, and LISA tuning. We set the epoch number to 1 for fine-tuning and continual pre-training scenarios. Ad- ditionally, we utilized DeepSpeed of- fload technology [63] to run the LLMs efficiently. All experiments were conducted on 8× NVIDIA Ampere Architecture GPU with 48 GB memory. Table 14 and table 15 are the information covered in this paper. *https://github.com/OptimalScale/LMFlow 24 Table 16: The hyperparameter search identified optimal settings for each method: FP (Full Parameter Training), LoRA, GaLore, and LISA. FP LoRA LISA Model lr lr Rank lr γ K GPT2-Small 3 × 10−4 6 × 10−4 128 6 × 10−4 2 3 TinyLlama 5 × 10−6 5 × 10−5 128 5 × 10−5 2 10 Mistral-7B 5 × 10−6 5 × 10−5 128 5 × 10−5 2 10 LLaMA-2-7B 5 × 10−6 5 × 10−5 128 5 × 10−5 2 10 LLaMA-2-70B 5 × 10−6 5 × 10−5 128 5 × 10−5 4 10 Our study explored a range of learning rates from 5 × 10−6 to 3 × 10−4, applying this spectrum to Full Parameter Training, LoRA, and LISA methods. For LoRA, we adjusted the rank r to either 128 or 256 to vary the number of trainable parameters, applying LoRA across all linear layers. Regarding the number of sampling layers γ, our selections were guided by GPU memory considerations as reported in LoRA studies [9]; For the LISA algorithm, we selected γ = 2, and for experiments involving the 70B model, we opted for γ = 4. The sampling period (K), defined as the number of update steps per sampling interval, ranges from 1 to 50. This range was influenced by variables such as the size of the dataset, the batch size, and the number of training steps. To manage this effectively, we partitioned the entire training dataset into K segments, thereby enabling precise regulation of the training steps within each sampling period. B.2 Continual Pre-training Dataset We extracted a high-quality subset from OpenWebMath [72], using the ‘Math_score’ attribute from the metadata as the metric for high-quality instances. The ‘Math_Score’ represents the probability that a document is mathematical, and we set the threshold at 0.95. Finally, the number of tokens for this high-quality subset is 1.5 billion. B.3 Hyperparameter search We commenced our study with a grid search covering (i) learning rate, (ii) number of sampling layers γ, and (iii) sampling period K. Noting the effective performance of the LoRA method, we set the rank value to r = 128 or r = 256. The optimal learning rate was explored within the range {5 × 10−6, 10−5, 5 × 10−5, 6 × 10−4, 3 × 10−4}, applicable to full parameter training, LoRA, and LISA. For GaLore, we adhered to the official Transformers implementation*, utilizing default parameters, with the learning rate matching that of the full parameter training. Regarding the number of sampling layers γ, in alignment with Table 1, we selected values that matched or were lower than LoRA’s GPU memory cost. Consequently, γ = 2 was predominantly used in the LISA experiments, while γ = 4 was chosen for the 70B model experiments. For the sampling period K, we examined values within 1, 3, 5, 10, 50, 80, aiming to maintain the model’s update steps within a range of 10 to 50 per sampling period. This selection was informed by dataset size, batch size, and total training steps. The comprehensive results of our hyperparameter search, detailing the optimal values for each configuration, are presented in Table 16. C Licenses For instruction following and domain-specific fine-tuning tasks, all the datasets, including Alpaca [61], GSM8k [73], MMLU [68], AGIEval [69] and PubMedQA [74] are released under MIT license. *https://huggingface.co/blog/galore 25 WinoGrande [70] and MT-Bench [67] are under Apache-2.0 license. For GPT-4, the generated dataset is only for research purposes, which shall not violate its terms of use. 26 NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We proposed Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 11%-37% in terms of MT-Bench and multiple benchmarks. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Limitation section 6. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs 27 Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Discussion section 5. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Experimental Result section 4 Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 28 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See Experiment section 4 and Appendix B. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the"
            },
            {
                "section": "results",
                "content": "? Answer: [Yes] Justification: See Experiment section 4 and Appendix B. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Experiment section 4 and Appendix B. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). 29 • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Experiment section 4.1 and Appendix B. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research conducted in the paper conforms in every respect with the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: NA. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. 30 • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Licenses section C. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. 31 • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: NA. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 32"
            }
        ]
    },
    "Paper_10": {
        "title": "Thinking Forward: Memory-Efficient Federated Finetuning of Language Models",
        "sections": [
            {
                "section": "Abstract",
                "content": "Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropa- gation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce SPRY, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimations of the true gradients. SPRY achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in SPRY are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive SPRY’s convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, SPRY reduces the memory footprint during training by 1.4–7.1× in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. SPRY reduces the convergence time by 1.2–20.3× and achieves 5.2–13.5% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, SPRY only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. SPRY makes feasible previously impossible FL deployments on commodity mobile and edge devices. Our source code is available for replication at https://github.com/Astuary/Spry. 1"
            },
            {
                "section": "Introduction",
                "content": "In cross-device federated learning (FL), thousands of edge devices (called clients) collaborate through an orchestrator (called server) to jointly train a machine learning (ML) model [1, 2]. In each round of FL, the server sends an ML model to participating clients, who then update the model weights for several epochs on their individual data and send the new weights back to the server. The server aggregates the weights to update the model and initiates the next round of FL training. Due to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024). inherent privacy-preserving nature of FL, it has been adopted in many privacy-sensitive domains, such as healthcare [3], IoT [4, 5], and e-commerce [6]. In parallel, large language models (LLMs) have demonstrated impressive performance on natural language processing tasks [7, 8], creating a surge of interest in finetuning LLMs in FL settings [9, 10]. However, the problem is challenging in practice because of the memory requirements in finetuning LLMs. LLMs can have billions of weights, and finetuning them with backpropagation requires dozens of GBs of memory. These requirements easily overwhelm edge devices, particularly mobile phones with limited memory. The memory footprint of finetuning LLMs mainly comes from model weights and their gradients, optimizer states, and intermediate activations. There are three categories of existing algorithms that reduce the memory footprint of finetuning LLMs: parameter-efficient finetuning (PEFT) [11–16], quantization [17], and zero-order gradient estimation methods [18–20]. Although PEFT and quantization can reduce the memory consumption from parameters and optimizer states, the memory consumed by intermediate activations remains a significant bottleneck because these methods still use backpropagation to finetune LLMs. Back- propagation requires storing all intermediate activations during the forward pass to estimate gradients in the backward pass. For example, finetuning a 4-bit quantized Llama2-7B model [21] with LoRA techniques [12] requires ∼33.9GB of RAM, with 83.8% used for intermediate activations. Zero-order"
            },
            {
                "section": "methods",
                "content": "leverage finite difference [22] to estimate gradients and thus reduce the memory consumption from intermediate activations [19, 20, 23]. However, these methods suffer from slow convergence and poor model quality because the accumulation of truncation and round-off errors [24], a fundamental issue of finite difference, leads to noisy estimation of weight gradients. Forward-mode Auto-Differentiation (AD) [24, 23] has the potential to address the memory consump- tion problems of backpropagation without introducing the round-off errors of finite difference. It estimates gradients by computing a Jacobian-vector product (jvp) based on random perturbations of weights during the forward pass, alleviating the need to store all intermediate activations, similar to zero-order methods. jvp represents how much changing the weights in the direction of a random perturbation affects the outputs. However, simply replacing backpropagation with Forward-mode AD in FL settings does not produce convergence speed and accuracy performance comparable to established federated optimizers based on backpropagation, such as FEDAVG [1], FEDYOGI [25], FEDSGD [1]. Forward gradients are computationally inefficient and inaccurate in estimating true gradients when the number of trainable weights is large. We empirically observed that, for LLMs finetuned with the LoRA technique, Forward-mode AD suffers from 8.3–57.2% accuracy loss and is 1.4–3.9× slower to converge for models whose number of trainable weights exceed approximately 1.15M (see Appendix G). In this paper, we propose SPRY1, an FL algorithm for finetuning LLMs using Forward-mode AD while achieving low memory footprint, high accuracy, and fast convergence. SPRY tackles the shortcomings of Forward-mode AD by splitting the trainable weights among participating clients per FL round. SPRY improves computation efficiency as each client only needs to perturb a small fraction of trainable weights to derive their gradients, reducing the number of computations in each forward pass. SPRY achieves higher accuracy and faster convergence because the smaller number of trainable weights for each participating client allows computing gradients that are closer estimations of the true gradients. In contrast to zero-order methods where one training iteration requires 20–100 forward passes, each with a different perturbation [20, 19] to estimate weight gradients well, SPRY allows computing weight gradients from only one forward pass per iteration for each client. Unlike split learning [26], SPRY does not need to transfer intermediate activations among clients. The union of the partial weights trained from each participating client in an FL round updates all the trainable weights of the language model. Since only a subset of weights is finetuned per client, SPRY also saves client-to-server communication bandwidth. We formally prove that the global gradients aggregated on the server side in SPRY are unbiased estimators of the true global gradients in case of homogeneous data distributions across clients, while the heterogeneity increases the bias of the estimations. We also derive the convergence rate of SPRY, showing that the norm of global gradients decreases linearly with the inverse of the number of FL rounds. We further discuss how configurations in SPRY affect the convergence behavior of the algorithm and empirically validate the theoretical analysis. 1Named after its light memory consumption and speed. 2 We empirically evaluate SPRY’s memory efficiency, accuracy, computation efficiency, and communi- cation efficiency through experiments on a wide range of language tasks, models, and FL settings. SPRY achieves within 0.6–6.2% of the accuracy of the best-performing FL backpropagation, with 1.4–7.1× less memory consumption for each client and comparable time to convergence. SPRY also outperforms zero-order-based baselines with 5.2–13.5% higher accuracy, an average of 1.5–28.6× faster per-round computation time, and 1.2–20.3× faster convergence. We also compare SPRY’s communication efficiency to that of FEDAVG (per-epoch communication) and FEDSGD (per-iteration communication). For communication frequency of per-epoch, SPRY reduces the number of model weights sent from a client to the server by M times, where M is the number of participating clients per round. For per-iteration communication frequency, each client of SPRY only needs to send back a scalar to the server, fixing the client-to-server total communication cost to M scalar values. We make the following contributions: 1. SPRY, the first work that demonstrate the potential of Forward-mode AD for finetuning language models (with 18M to 13B parameters) in FL settings with low memory footprint, high accuracy, and fast convergence. 2. A federated optimization strategy that only requires a single forward pass per batch on each client to finetune a language model. 3. A theoretical analysis of how SPRY’s global gradients estimate true gradients based on the heterogeneity of FL clients, and a proof that SPRY’s convergence is linearly dependent on the number of FL rounds when a client’s learning rate is inversely proportional to the size of perturbations and client data heterogeneity. 4. An empirical evaluation shows that SPRY consumes 1.4–7.1× less memory than its backpropagation-based counterparts, and converges 1.2–20.3× faster with 5.2–13.5% higher accuracy compared to its zero-order counterparts. 2 Forward-mode Automatic Differentiation This section presents the background on Forward-mode Auto-Differentiation (AD) necessary to follow the work. Related works on zero-order optimization methods, Forward-mode AD, and FL for LLMs are discussed in detail in Appendix A. Forward-mode AD computes gradients by measuring how changes in model weights, in the direction of a random perturbation, affect the loss. Since these gradients are derived from a forward pass, they are referred to as forward gradients [23]. In contrast, backpropagation (also Reverse-mode AD) calculates a direction to adjust weights in, to decrease the loss. Formally, for each training iteration, given the trainable weights www, Forward-mode AD generates a random perturbation vvv whose size is the same as www. In one forward pass, given training data D, Forward-mode AD computes the value of the objective function f(www; D) and the Jacobian-vector product (jvp) as follows: Jfvvv = ∇fvvv(www; D) = h ∂f(ww w;D) ∂w1 . . . ∂f(ww w;D) ∂wd i [v1 . . . vd]T . (1) This jvp is a scalar for neural networks since the output of the objective function f is a scalar. Multiplying jvp with the perturbation vvv gives us the unbiased estimate of true gradients [23], ∇F(www) = Evvv [Jfvvv · vvv] = Evvv,D h\u0010h ∂f(w ww;D) ∂w1 . . . ∂f(w ww;D) ∂wd i vvvT \u0011 vvv i (2) = ED h ∂f(w ww;D) ∂w1 . . . ∂f(w ww;D) ∂wd i = ED [∇f(www; D)] . (3) The partial derivative ∂f(www; D)/∂www is computed by chain rule on intermediate activations in the forward pass [24]. Unlike backpropagation where all the intermediate activations need to be stored during the forward pass, Forward-mode AD only stores the previous layer’s activations in the forward pass for the chain rule derivatives. Hence, the memory overhead of deriving gradients would be the size of the largest activation in the forward pass. 3 SPRY: Memory-Efficient Federated Finetuning of Language Models While Forward-mode AD can decrease memory usage during model finetuning, merely substitut- ing backpropagation with Forward-mode AD in FL scenarios often results in poor accuracy and 3 Client #1 Client #2 Client #3 Client #4 Server jvp A Language Model with Frozen Backbone and Trainable PEFT Weights Weight perturbation 4. Send updated assigned weights 3a. Compute JVP 5. Aggregate trained layer to update the model , 1. Split layers across participant clients Forward gradient Updated weight 3b. Update the weights (  ,  ) = optim_step · (     ) = forward_AD 2. Send frozen and assigned weights, and a seed Scalar Vector or Matrix Legend Figure 1: Overview of SPRY, a federated learning framework to finetune language models with low memory footprint. The term “PEFT” stands for parameter-efficient fine-tuning. computational inefficiency. To address this challenge, SPRY recognizes that Forward-mode AD operates more efficiently and yields better gradient estimations when the trainable weight count is minimized. Therefore, SPRY optimizes performance by distributing trainable weights among participating clients, assigning each client a responsibility to compute gradients for only a subset of trainable weights. SPRY is compatible with PEFT methods such as IA3 [15], ADAPTER-based meth- ods [27], BITFIT [28], and LORA [12], which mitigate the memory consumption from gradients and optimizer states. In this work, we focus on LORA due to its demonstrated superiority, as highlighted in Appendix G. Figure 1 gives an overview of SPRY. It includes 5 main steps: (1) At the beginning of each FL round, the server assigns a few trainable layers to each of the participating clients of that round. (2) Each client is sent (i) the trainable weights of the layers assigned to it, (ii) frozen weights of the remaining layers if not previously received, and (iii) a scalar seed value. (3) On the client side, weight perturbations for the allocated layers are generated based on the received seed. These perturbations are utilized to update the assigned weights through computation of the forward gradients. (4) Clients only transmit the updated weights back to the server. (5) The server aggregates all the trained layer weights and updates the language model for the subsequent round. Next, we discuss the two key steps of SPRY in detail: Step (1), where the server assigns trainable weights to the participating clients and Step (3), where each client finetunes the assigned trainable weights using Forward-mode AD. 3.1 Assigning Trainable Layers to Clients at the Server-side To enable closer gradient estimations through forward gradients, the server reduces the number of trainable weights per client by selecting a layer and assigning it to a client in a cyclic manner. With LORA, the server selects a LoRA layer, which consists of a pair of weights (wA and wB matrices) for each client. When # trainable layers > # participating clients, each client will be assigned more than one layer. Otherwise, each layer will be assigned to more than one client. The server aggregates the trained weights from each client and updates the model using adaptive optimizers such as FEDYOGI. Adaptive optimizers are shown to be less prone to noisy updates compared to FEDAVG in the literature [25, 29]. The server keeps a mapping of layer names to client IDs, hence it can gather updated layer weights from all clients and update the model. FL often faces the data heterogeneity issue, where the data distribution of one client differs from another, leading to poor model accuracy. While the primary aim of SPRY does not directly tackle this issue, it seamlessly integrates with existing finetuning-based personalization techniques [30] to mitigate it. SPRY distributes trainable classifier layers to all participating clients, enabling each client to finetune these layers to personalize the jointly trained model. 3.2 Finetuning Weights with Forward Gradients on the Client-side Clients update the assigned trainable weights with gradients estimated through Forward-mode AD. Specifically, each participating client will get a copy of the trainable weights assigned to it and a 4 scalar seed value from the server. Using the seed value, the client generates a random perturbation for each trainable weight, following a normal distribution with a mean of 0 and a standard deviation of 1. Forward gradients are obtained during forward pass, as shown in Eq. 3. The subsequent steps depend on the communication frequency. Per-Epoch Communication. Per-epoch communication means that each client transmits the updated trainable weights to the server after every one or more epochs. Locally, the trainable weights are updated using optimizers such as SGD and ADAM. Only the updated trainable weights are sent back to the server, which reduces communication costs. Each client transmits the weights of max n #Trainable Layers #participating clients, 1 o layers. This means that if there are more trainable layers than participating clients, each client sends back weights for multiple layers. Per-Iteration Communication. Unlike FEDSGD, where gradients are sent back to the server and aggregated after each iteration, SPRY offers additional communication cost savings. In this communi- cation mode, SPRY only requires sending the jvp scalar value back to the server after each iteration of fine-tuning. Since the server has the seed value, it can generate the same random perturbation used by each client. Using the received jvp values and the generated random perturbations, the server can then compute the gradients and update the model weights. A detailed breakdown of communication and computation costs is given in Appendix F. 4 Theoretical Analysis This section theoretically analyzes convergence behaviors of SPRY. SPRY has a unique aggregation rule for the trainable weights as each client trains a subset of weights. It is also the first work to utilize forward gradients to train LLMs in FL settings where clients could have heterogeneous data distributions. Therefore, we theoretically analyze the effects of (a) data heterogeneity on gradient estimations of SPRY, and (b) configurations in SPRY, including the number of FL rounds, dimension of perturbations, the number of perturbations per iteration, data heterogeneity, and the number of participating clients, on the convergence of SPRY. The proofs are detailed in Appendix I. Theorem 4.1 (Estimation of the Global Gradient). In SPRY, global forward gradients ∇ˆf of the trainable weights w ∈Rd, with the corresponding weight perturbations v ∈Rd, computed by M participating clients is estimated in terms of true global gradients ∇f as, E[∇ˆf(w, v; D)]=∇f(w)+ 1 f M   P m∈f M1 PC c=1 αm,cE(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i P m∈f M2 PC c=1αm,cE(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ];(x, yc)) i ...   T (4) where the expectation is under the randomness of sampled data and random perturbation v. C is total number of classes and αm,c = \u0010 nc |D| −nm,cαc |Dm| \u0011 . For a class c, nc is its sample count, αc is its Dirichlet concentration parameter. For a client m, nm,c is the sample count of the cth class and Dm is the size of the data of client m. The global data is D = P m∈M Dm. f M is the set of clients training an arbitrary subset of weights, f M = | f Mi|, ∀i ∈[M/d]."
            },
            {
                "section": "Discussion",
                "content": ". We focus on analyzing how data heterogeneity affects the estimation error between the global forward gradient and the global true gradient. Specifically, the estimation error of SPRY depends on the coefficient αm,c = \u0010 nc |D| −nm,cαc |Dm| \u0011 . In data homogeneous settings, where all clients share the same data distribution, the Dirichlet concentration parameter αc, is 1 for any class c. As the ratio of nc/|D| (total samples in a class to total samples globally) matches the ratio of nm,c/|Dm| (total samples in a class to total samples for a client m), the bias term becomes 0 since αm,c = 0, ∀m, c. Hence, the global forward gradients are unbiased estimators of the true global gradients. In data heterogeneous settings, when data across clients becomes more heterogeneous, αc →0 and thus αm,c →nc/|D|, increasing the estimation error. Hence the global forward gradients are biased estimators that depend on the distances of the data distributions across participating clients. Theorem 4.2 (Convergence Analysis). Under the assumptions on L-smoothness (Asmp I.1), bounded global variance σ2 g between global true gradients and aggregated expected gradients of each client 5 (Asmp I.2), and bound on gradient magnitude G (Asmp I.3); and the following conditions on the local learning rate ηℓ, ηℓ= min ( O \u0012 τ 2 √β2ηGL \u0013 1 2 , O \u0012 1 √β2G \u0013 , O τ 3 p β2(1 −β2)G2 ! 1 2 , O f MK β2G(3d + K −1) P m∈[M] P c∈[C] α2m,c ! ) ; (5) The global true gradients of SPRY satisfies the following bound, min 0≤r≤R Er||∇f(w(r))||2 ≤f(w(0)) −ER[f(w(R))] ηR + \u0012 2 + ηηℓL 2τ 2 + √1 −β2Gηℓ τ 3 \u0013 \u0012σ2 g(1 −s)(3d + K −1) f MK \u0013 X m∈M X c∈[C] α2 m,c, (6) where R is the total number of FL rounds, w ∈Rd are trainable weights, v ∈Rd are random perturbation, K is count of random perturbations per iteration, η is global learning rate, τ is adaptability constant, and s is client sampling rate. Rest of the symbols are defined in Theorem 4.1."
            },
            {
                "section": "Discussion",
                "content": ". We focus on analyzing how different configurations in SPRY affect its convergence. (a) The number of FL rounds (R): The upper bounds of the norm of the global forward gradient in Eq. 6 decrease in proportion to the inverse of R, indicating the convergence of the algorithm up to the limits of data heterogeneity. (b) Dimension of perturbations (d): ηℓ∝1 d shows that as the number of weights to be perturbed increases, the learning rate must decrease. A lower learning rate can make convergence slower, or worse, as our empirical experiments in Appendix G will show, not converge at all. (c) The number of perturbations per iteration (K): We observe K both in nominator and denominator, which indicates that increasing K brings little advantage in convergence speed."
            },
            {
                "section": "Results",
                "content": "in Appendix G confirm the above statement. (d) Data heterogeneity: ηℓ∝ 1 α2m,c shows that more homogeneous data distributions across clients allow higher learning rate, and hence faster error reduction. This observation is corroborated by the comparison of convergence speeds between homogeneous and heterogeneous clients in Appendix H. (e) The number of clients training same subset of weights: ηℓ∝f M shows that more clients training the same subset of weights is beneficial for faster convergence, similar observation is shown in Appendix G. The theorem also sheds light on the accuracy performance gap between Forward-mode AD and backpropagation in FL settings. The upper bounds on the global forward gradient norm includes a second term that increases with α2 m,c and variance σ2 g, but does not decrease with R. This results in a gap between estimation errors of backpropagation-based methods like FEDAVG and SPRY. 5 Empirical Evaluation We empirically evaluate SPRY on 8 language tasks, 5 medium, and 3 large language models, un- der various FL settings. Our evaluation measures SPRY’s prediction performance, peak memory consumption, and time-to-convergence. We also ablate SPRY’s components to study the impact of communication frequency, the number of trainable parameters, the number of perturbations per iteration, the number of participating clients, and the importance of splitting layers on performance. Datasets, Tasks, and Models. Our evaluation uses 8 datasets: AG News [31] (4-class classifica- tion), SST2 [32] (2-class classification), Yelp [31] (2-class classification), Yahoo [31] (10-class classification), SNLI [33] (3-class classification), MNLI [34] (3-class classification), SQuADv2 [35] (Closed-book question answering), and MultiRC [36] (2-class classification). We chose these datasets because they allow us to generate heterogeneous splits in FL settings using Dirichlet distri- bution [37]. The default dataset split is across 1,000 clients, except the smallest datasets SST2 and MultiRC, where there are 100 clients. SQuADv2 has 500 total clients. Each dataset has two versions: (i) Dirichlet α = 1.0 (Homogeneous split), and (ii) Dirichlet α = 0.1 (Heterogeneous split). Our evaluation uses the following language models: OPT13B [38], Llama2-7B [21], OPT6.7B [38], RoBERTa Large (355M) [39], BERT Large (336M) [40], BERT Base (110M) [40], Distil- BERT Base (67M) [41], and Albert Large V2 (17.9M) [42]. For the billion-sized models, we use 4-bit quantization. For all the models, we use LORA as the PEFT method. Appendix B describes the datasets and hyperparameters in more detail. 6 Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on RoBERTa Large and LLMs. SQuADv2 uses F1 score. ↑shows that higher values are better. The datasets are split with Dir α = 0.1. ⋄= Llama2-7B. ⋆= OPT6.7B. □= OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15–13.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60–6.16%. Backpropagation-based"
            },
            {
                "section": "Methods",
                "content": "↑ Zero-order-based"
            },
            {
                "section": "Methods",
                "content": "↑ First-order Forward Mode AD ↑ Difference between performances of SPRY and FEDAVG FEDYOGI FWDLLM+ FEDMEZO BAFFLE+ SPRY best-performing backpropagation method ↑ best-performing zero-order method ↑ AG News 93.07% 92.77% 76.94% 70.56% 57.69% 87.89% −5.18% 10.95% SST2 88.00% 92.14% 84.41% 72.17% 61.57% 91.54% −0.60% 7.13% SNLI 86.45% 79.31% 74.30% 69.57% 62.10% 82.66% −3.79% 8.36% MNLI 84.29% 84.98% 72.66% 66.66% 62.85% 80.32% −4.66% 7.66% Yahoo 67.37% 63.08% 56.06% 44.69% 37.81% 61.21% −6.16% 5.15% Yelp 90.48% 79.10% 71.83% 65.10% 55.99% 85.33% −5.15% 13.50% MultiRC ⋄ 47.56% 72.53% 64.58% N/A 58.12% 68.65% −3.88% 4.07% SQuADv2 ⋆ 19.06 19.91 13.46 13.09 11.09 16.75 −3.16 3.29 SQuADv2 □11.88 11.30 7.85 8.07 6.92 8.84 −3.04 0.77 Comparison Counterparts and Metrics. We compare SPRY to (a) Backpropagation-based fed- erated optimizers FEDAVG [1], FEDYOGI [25], FEDSGD (Variant of FEDAVG with per-iteration communication) [1], (b) Zero-order federated methods FEDMEZO (federated version of MEZO [18]), BAFFLE [20], FWDLLM [19], all based on finite difference. MEZO uses prompt-based finetuning to improve the performance of finite differences. FWDLLM generates a random perturbation that has a high cosine similarity to the global gradients of the previous rounds. BAFFLE generates ∼100–500 perturbations per iteration. More details of these methods are in Appendix A. The original imple- mentations of FWDLLM and BAFFLE had excessive memory usage in their implementations. We improve their codebase to be memory-efficient by perturbing only the trainable weights, similar to SPRY. We refer to our implementation as FWDLLM+ and BAFFLE+. Evaluation of SPRY and its counterparts for classification tasks is on generalized accuracy Accg and personalized accuracy Accp, which are metrics measured on server-side aggregated model and client-side locally updated model, respectively. Similarly, for question-answering tasks, we measure Exact Matches and F1 Score. We also measure time to convergence and peak memory consumption during training. Our convergence criterion is the absence of change in the variance of a performance metric, assessed at intervals of 50 rounds. SPRY is implemented in Flower [43] library. Quantization is done using AutoGPTQ [44]. For the zero-order methods, we used their respective client-side implementations with the server simulation structure of Flower. We utilized two Nvidia 1080ti to conduct all experiments of sub-billion sized models and billion-sized models for SPRY and its zero-order methods. We used two RTX8000s and two A100s for Llama2-7B and OPT models on backpropagation-based methods respectively. Each experiment was run thrice with 0, 1, and 2 as seeds. 5.1 Accuracy Performance Comparison Table 1 reports the accuracy performance of SPRY and its backpropagation- and zero-order-based counterparts on heterogeneous datasets for million-sized RoBERTa Large, and billion-sized Llama2- 7B, OPT6.7B, and OPT13B. Similar results on personalized performance is shown in Appendix H, Figure 5. Results on MultiRC for FEDMEZO are absent since the prompt-based finetuning variant of Llama2-7B was unavailable. Results on more model architectures and dataset combinations are available in Appendix G. Details on the learning curves, homogeneous dataset splits, and variance across 3 runs are in Appendix H. Overall, SPRY achieves 5.15–13.50% higher generalized accuracy and 4.87–12.79% higher personal- ized accuracy over the best-performing zero-order-based methods across all datasets. FWDLLM+, the best-performing zero-order counterpart, attempts to reduce the effect of numeric instability of finite differences by (a) Sampling K perturbations (default K = 10) per batch for each client and picking 1 perturbation per batch that has the highest cosine similarity with the previous round’s aggregated gradients and (b) Only picking trained weights from clients whose computed gradients have variance lower than a set threshold. However, we posit that this strategy leads to some clients getting excluded due to a low variance threshold or outlying clients getting included due to a high 7 variance threshold. Besides, picking new perturbations based on the previous round’s aggregated gradients in the initial rounds can damage the learning trajectory. While BAFFLE+ samples more perturbation for each batch to make zero-order finite differences more tractable, the scale of language models demands perturbations on the scale of 500-1000 per batch, which becomes computationally infeasible. FEDMEZO manages to outperform BAFFLE+ due to its prompt-based finetuning trick but still falls short due to only using 1 perturbation per batch for finite differences on each client. In contrast, Forward-mode AD used in SPRY avoids the numerical instability from finite differences and improves accuracy by reducing trainable weights assigned to each client. Compared to backpropagation-based methods, FEDAVG and FEDYOGI, SPRY manages to come as close as 0.60-6.16% of generalized accuracy and 2.50–14.12% of personalized accuracy. The performance gap between backpropagation and Forward-mode AD arises because in backpropagation, weight updates are more accurate as all gradients are computed directly using the error signal of the objective function. In contrast, Forward-mode AD relies on random perturbations, which is relatively less accurate for gradient estimation. Nonetheless, the advantages of SPRY become evident when we see the peak memory consumption of Forward-more AD compared to backpropagation, which we will discuss next. 5.2 Peak Memory Consumption Comparison Back- propagation Zero-Order Finite Difference First-Order Forward Mode AD 0 1 2 Memory Footprint (in GBs) 2.23GB 1.56GB 1.6GB RoBERTa-Large Back- propagation Zero-Order Finite Difference First-Order Forward Mode AD 0 20 40 34.0GB 5.48GB 6.21GB Llama2-7B Back- propagation Zero-Order Finite Difference First-Order Forward Mode AD 0 20 40 60 Memory Footprint (in GBs) 56.44GB 6.04GB 7.75GB OPT6.7B Back- propagation Zero-Order Finite Difference First-Order Forward Mode AD 0 25 50 75 76.45GB 9.21GB 10.75GB OPT13B Parameters Activations Gradients + Optimizer States + Misc Figure 2: Peak memory consumption of SPRY’s Forward-mode AD versus backpropgation- and zero-order-based methods. RoBERTa Large, Llama2-7B, and OPT6.7B are profiled with a batch size of 8, and OPT13B with a batch size of 4. SPRY reduces total memory usage by 27.90–86.26% compared to backpropagation- based methods. The 1.54–1.96× additional memory SPRY uses, compared to zero-order-based"
            },
            {
                "section": "methods",
                "content": ", is offset by the accuracy gains (§ 5.1). Figure 2 shows peak memory consump- tion of backpropagation (used in FEDAVG, FEDYOGI), zero-order finite differences (used in FWDLLM+, BAFFLE+, FEDMEZO), and first- order forward mode AD (used in SPRY). The"
            },
            {
                "section": "methods",
                "content": "are profiled for a single client. Compared to backpropagation, Forward-mode AD reduces peak memory usage of RoBERTa Large by 27.90%, Llama2-7B by 81.73%, OPT6.7B by 86.26% and OPT13B by 85.93%. The sizes of trainable parameter (colored ) and gradient + optimizer state (colored ) are consis- tent across the 3 modes of computing gradients for all 4 models. Hence the savings come from the reduced memory footprint related to activa- tions (colored ) in Forward-mode AD. Com- pared to backpropagation-based methods, the memory cost related to activations is decreased by 12.12–49.25× in SPRY. Unlike storing all the intermediate activations in backpropagation, Forward-mode AD only has to store the previous layer’s activation in a forward pass. The activa- tion footprint of Forward-mode AD is equal to the size of the largest activation. Against zero-order methods, Forward-mode AD activations cost 1.96×, 1.95×, 1.83×, and 1.54× more for RoBERTa Large, Llama2-7B, OPT6.7B, and OPT13B respectively. The increasing cost comes from parallel evaluations of (a) the objective function on the original weights and (b) jvp computation on the perturbations in a single forward pass. However, as discussed in § 5.1, the increased memory cost is offset by a boost of up to 13.50% in accuracy performance. And as § 5.3 will discuss, Forward-mode AD reaches convergence faster than zero-order methods since it takes fewer steps to compute a closer gradient estimation. 5.3 Time to Convergence Comparison Figure 3 shows wallclock time-to-convergence for SPRY and its counterparts. We observe that SPRY is 1.15-1.59×, 6.16-20.28×, and 1.34-2.98× faster than the zero-order methods FWDLLM+, BAFFLE+, and FEDMEZO respectively. For a client in each round, SPRY achieves a faster per-round computation time of 1.46×, 28.57×, and 1.80× on average, against FWDLLM+, BAFFLE+, and FEDMEZO. Forward-mode AD achieves faster convergence and faster per-round computation by providing a more accurate gradient estimation through a single perturbation per batch, leading to fewer steps needed to reach convergence. Since each client in SPRY only trains partial weights, 8 it gains a speedup of 1.14× over backpropagation-based FEDAVG, FEDYOGI, and FEDSGD for RoBERTa Large. However, compared to the backpropagation-based methods, SPRY slows down for billion-sized LMs. We attribute this loss of speedup to the way jvp is computed in Forward-mode AD. jvp is computed column-wise, while its counterpart vjp in backpropagation are computed row-wise. The column-wise computation incurs time overhead. 5.4 Ablation Studies 0 200 400 0.0 0.2 0.4 0.6 0.8 1600 1800 2000 2200 2400 Time (in minutes) Test Accuracy Accg FedAvg FedYogi FedSgd FwdLLM+ FedMeZO Baffle+ Spry (a) AG News with RoBERTa Large 0 50 100 150 200 0 2 4 6 8 10 12 2700 2750 2800 Time (in minutes) Test F1 Score F1g (b) SQuADv2 with OPT13B Figure 3: Time to convergence for SPRY and its counterparts. SPRY achieves faster convergence than zero-order"
            },
            {
                "section": "methods",
                "content": "due to more accurate gradient estimations in a single perturbation. We summarize the ablation experiments on various com- ponents of SPRY. Further discussions are in Appendix G. SPRY can generalize to other language model architec- tures. Similar to the observations from Table 1, we see a trend of SPRY outperforming the best performing zero- order method FWDLLM+ by 3.15-10.25% for generalized accuracy, demonstrating that SPRY can generalize to other language model architectures. SPRY is compatible with other PEFT methods. We integrate SPRY with different PEFT methods like IA3, BITFIT, and CLASSIFIER-ONLY FINETUNING. Results shows that LORA with SPRY performs the best, with accuracy improvements of 10.60-16.53%. Effects of the number of trainable weights. We change the number of trainable weights by controlling the rank r and scale α hyperparameters in LORA. Results show that SPRY achieves the highest accuracy with (r=1, α=1) setting, which has the smallest trainable weight count. The result is consistent with our theoretical analysis in §4. Effects of communication frequency. Per-iteration com- munication in SPRY has been shown to boost accuracy by 4.47% compared to per-epoch communication. This improvement brings the accuracy within 0.92% and 0.96% of FEDAVG and FEDSGD, respectively. Effects of perturbation count per batch. We observe that increasing the number of perturbations per batch (K) for Forward-mode AD has little to no impact on the end prediction performance of SPRY, with K = 100 improving the generalized test accuracy by 1.1% over K = 1. The benefits of increasing K are however seen in the convergence speed. Setting K = 10 achieves a steady state (of accuracy ∼86%) around the 200th round, while the setting with K = 1 takes 500 rounds. Effects of participating client count. Increasing the client count increases the prediction performance of SPRY. For the SST2 dataset, with the total client count fixed to 100, the three settings C = 10, C = 50, and C = 100 produce accuracies of 85.14%, 86.56%, and 88.08%, respectively. We also see an improvement in the convergence speed as the participating client count increases. To achieve an accuracy of ∼85%; C = 10, C = 50, C = 100 require 500, 450, and 150 rounds, respectively. Importance of splitting weights. To understand the effects of splitting, we conduct the following two experiments: (a) With FEDAVGSPLIT, we apply the strategy of splitting trainable layers across clients (see § 3.1) to backpropagation-based FEDAVG, and (b) With FEDFGD, we omit the splitting strategy of SPRY for FL with Forward-mode AD. We observe that FEDAVGSPLIT fails to achieve similar accuracy to FEDAVG with a drop of 2.60-10.00%. FEDFGD fails to converge as the size of trainable weights increases, e.g., with RoBERTa Large with LORA, that has 1.15M trainable weights. This proves the necessity of splitting trainable weights for Forward-mode AD in SPRY. 5.5 Communication and Computation Costs Tables 2 and 3 in Appendix F shows the communication and computation overhead of SPRY against all its baselines. We summarize the main observations here: SPRY has lower communication cost due to the splitting strategy and scalar jvp. Suppose wg is the total trainable parameter count of a model to be trained in federated setting. The 9 backpropagation-based baselines; FEDAVG (per-epoch communication), FEDSGD (per-iteration communication), and FEDYOGI (per-epoch communication) transmit the entire set of trainable parameters to each participating client, and receives the same from each participating client. That"
            },
            {
                "section": "results",
                "content": "in “client to server” communication cost of wg and “server to client” communication cost of wg × M. The per-epoch versions of the zero-order baselines (FEDMEZO, BAFFLE, FWDLLM) follow a similar logic due to all the parameters needing to be transmitted to each client, with “client to server” communication costing wg, and “server to client” communication taking the cost of wg × M. However, the per-iteration versions of the zero-order baselines fare better, with “client to server” communication only requiring each client sending a scalar finite difference (cost of 1), and “server to client” communication accruing (wg + 1) × M cost, due to the server also needing to send a scalar seed to each client now. Meanwhile, SPRY only needs to send max \u0000 L M , 1 \u0001 layers (where L is the total layer count of a model, and M is the participating count of clients for a round), each layer of size wℓ. Hence, for per-epoch “client to server” communication accrues wℓmax \u0000 L M , 1 \u0001 , which is smaller than wg. Similarly, per- epoch “server to client” communication costs wℓM max \u0000 L M , 1 \u0001 , which is also smaller than the cost of wgM related to the baselines. For per-iteration communication, clients only need to send a scalar jvp (cost of 1) to the server; this matches the communication cost of per-iteration zero-order methods. Server needs to send a total of wℓM max(L, M) (derivation given in Table 2), which is a smaller cost than the costs of backpropagation and zero-order methods. SPRY accrues lower computation cost due to lower trainable parameter count. SPRY’s client- side computation cost is traded off by a faster convergence to higher accuracy through better gradient approximations compared to finite difference-based methods. And SPRY is the least computationally expensive on the server side due to needing to aggregate fewer parameters from the clients. Let’s assume that matrix multiplication costs c for each layer, resulting in a forward pass cost of c. The cost of backpropagation is 2c because the computation of the current layer’s weight gradient is c, and the cost of computing the previous layer’s activation gradient is another c. jvp computation in SPRY takes additional cost of c for each layer. Moreover, since jvp calculation happens through column-by-column vector multiplications, the related overhead is quantified by v. Hence backpropagation-based methods FEDAVG, FEDSGD, and FEDYOGI computationally costs 3Lc at client, and costs wg(M −1) at server (due to additions). Note that L amounts to all layers in the model here. FEDMEZO costs L(2c + 3wℓ) at client through two forward passes, and generating perturbations three times. FWDLLM and BAFFLE costs KL(2c + wℓ) due to K perturbations for all L layers, with two forward passes and generation perturbations once. Against that, SPRY costs 2 max( L M , 1)(c + v) + wℓL for a smaller count of L, traded-off by the jvp computation cost of v. On the server side, SPRY is the least computationally demanding. SPRY needs to aggregate a subset of layer weights from only the clients that were assigned to those layers. Computation cost on the server-side changes based on the communication frequency per-iteration communication incurs an additional overhead of wℓL( M L + 1) and wℓL(M + 1) (generation of perturbations at the server-side, and multiplying those perturbations with aggregate of the jvp values received from the clients) for SPRY and its zero-order counterparts respectively. 6"
            },
            {
                "section": "Conclusion",
                "content": "SPRY enables finetuning medium and large language models in cross-device FL. It introduces a training strategy where trainable weights are split across federated clients, so each client only applies Forward-mode AD to a fraction of the weights. This approach significantly reduces the memory footprint compared to backpropagation and achieves better gradient estimation, resulting in higher accuracy and faster convergence than zero-order methods. Experiments on various language tasks and models demonstrate SPRY’s effectiveness in reducing memory usage while maintaining accuracy comparable to backpropagation. We formally prove that the estimation bias of the global forward gradients in SPRY depends on data heterogeneity across clients. We also analyzed how the convergence rate of SPRY relates to the configurations of SPRY and FL settings including properties of weight perturbations, data heterogeneity, and the number of clients and FL rounds."
            },
            {
                "section": "methods",
                "content": "for natural language processing tasks. In Findings of the Association for Computational Linguistics: NAACL 2022. Association for Computational Linguistics, 2022. [10] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. Fedbert: When federated learning meets pre-training. ACM Transactions on Intelligent Systems and Technology (TIST), 2022. [11] Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin A Raffel. Distributed inference and fine-tuning of large language models over the internet. Advances in Neural Information Processing Systems, 2024. [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [13] Dongqi Cai, Yaozong Wu, Shangguang Wang, and Mengwei Xu. Fedadapter: Efficient federated learning for mobile nlp. In Proceedings of the ACM Turing Award Celebration Conference - China 2023. Association for Computing Machinery, 2023. [14] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [15] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems, 2022. [16] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021. 12 [17] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [18] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [19] Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. Fwdllm: Efficient fedllm using forward gradient. arXiv 2308.13894, 2024. [20] Haozhe Feng, Tianyu Pang, Chao Du, Wei Chen, Shuicheng Yan, and Min Lin. Does federated learning really need backpropagation? arXiv 2301.12195, 2023. [21] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv 2307.09288, 2023. [22] C. H. Richardson. An introduction to the calculus of finite differences. by c.h. richardson pp. vi, 142. 28s. 1954. (van nostrand, new york; macmillan, london). The Mathematical Gazette, 39(330), 1955. doi: 10.2307/3608616. [23] Atılım Güne¸s Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradients without backpropagation. arXiv 2202.08587, 2022. [24] Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 2017. [25] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021. [26] Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun. Splitfed: When federated learning meets split learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022. [27] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, 2024. [28] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 2022. [29] Kunjal Panchal, Sunav Choudhary, Subrata Mitra, Koyel Mukherjee, Somdeb Sarkhel, Saayan Mitra, and Hui Guan. Flash: concept drift adaptation in federated learning. In International Conference on Machine Learning, pages 26931–26962. PMLR, 2023. [30] Shanshan Wu, Tian Li, Zachary Charles, Yu Xiao, Ziyu Liu, Zheng Xu, and Virginia Smith. Motley: Benchmarking heterogeneity and personalization in federated learning. arXiv 2206.09262, 2022. [31] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Neural Information Processing Systems, 2015. Available at https: //huggingface.co/datasets/ag_news, https://huggingface.co/datasets/yelp_polarity, https://huggingface.co/datasets/yahoo_answers_topics, Accessed on 15 May, 2024. 13 [32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christo- pher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Com- putational Linguistics, 2013. Available at https://huggingface.co/datasets/stanfordnlp/sst2, Accessed on 15 May, 2024. [33] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical"
            },
            {
                "section": "Methods",
                "content": "in Natural Language Processing. Association for Computational Linguistics, 2015. Available at https://huggingface.co/datasets/stanfordnlp/snli, Accessed on 15 May, 2024. [34] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, 2018. Available at https://huggingface.co/datasets/ SetFit/mnli, Accessed on 15 May, 2024. [35] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 2018. Available at https:// huggingface.co/datasets/rajpurkar/squad_v2, Accessed on 15 May, 2024. [36] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, 2018. Available at https://huggingface.co/datasets/mtc/multirc, Accessed on 15 May, 2024. [37] Kunjal Panchal, Sunav Choudhary, Nisarg Parikh, Lijun Zhang, and Hui Guan. Flow: Per-instance personalized federated learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [38] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. arXiv 2205.01068, 2022. [39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. arxiv 1907.11692, 2019. [40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. arXiv 1810.04805, 2018. [41] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv 1910.01108, 2019. [42] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv 1909.11942, 2019. [43] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and Nicholas D Lane. Flower: A friendly federated learning research framework. arXiv preprint 2007.14390, 2020. [44] AutoGPTQ, 2024. URL https://github.com/AutoGPTQ/AutoGPTQ. [45] Richard L. Burden and J. Douglas. Faires. Numerical analysis / Richard L. Burden, J. Douglas Faires. Thomson Brooks/Cole, 8th ed. edition, 2005. ISBN 0534392008. [46] Károly Jordán. Calculus of finite differences. American Mathematical Soc., 1965. [47] Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou. Communication- efficient stochastic zeroth-order optimization for federated learning. IEEE Transactions on Signal Process- ing, 2022. [48] Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, and Siheng Chen. Openfedllm: Training large language models on decentralized private data via federated learning. arXiv 2402.06954, 2024. 14 [49] Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. Fate-llm: A industrial grade federated learning framework for large language models. arXiv 2310.10049, 2023. [50] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. Fedscale: Benchmarking model and system performance of federated learning at scale. arXiv 2105.11367, 2022. [51] Jae Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. In Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022). Association for Computational Linguistics, 2022. [52] Shubham Malaviya, Manish Shukla, and Sachin Lodha. Reducing communication overhead in federated learning for pre-trained language models using parameter-efficient finetuning. In Conference on Lifelong Learning Agents. PMLR, 2023. [53] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. [54] Seonghwan Park, Dahun Shin, Jinseok Chung, and Namhoon Lee. Fedfwd: Federated learning without backpropagation. arXiv 2309.01150, 2023. [55] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv 2212.13345, 2022. [56] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 2023. [57] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv 2403.14608, 2024. [58] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 15 A Related Work SPRY uses first-order forward gradients for federated finetuning of language models. Hence, we review the literature on estimating gradients with low memory consumption and show how SPRY represents a significant advancement in finetuning language models in FL. Zero-order Gradients. Gradients derived from finite difference [45, 22, 46] methods are called zero-order gradients since they don’t involve Taylor expansion of the objective function f. MEZO [18] has shown that finite difference with one perturbation per batch does not reach conver- gence on its own without additional tricks like prompt-based finetuning, which are highly specific to the tasks. For a more accurate gradient approximation, an average of zero-order gradients de- rived from multiple (∼10 to 100) random perturbations on the same input batch is required [24], leading to slow convergence. BAFFLE [20] and FEDZO [47] utilize zero-order gradients in feder- ated settings. To train vision models (of parameter count ≤13M), BAFFLE requires (a) ∼100-500 perturbations per batch for each client respectively, and (b) per-iteration communication among clients like FEDSGD [1]. FEDZO also requires ∼20 perturbations per batch for a vision model of size ≤25M. Besides, numerical errors associated with finite difference make MEZO, BAFFLE, and FEDZO suffer from sub-optimal predictions compared to backpropagation-based counterparts. SPRY, using Forward-mode AD, computes gradients more accurately in a single forward pass compared to averaged gradients obtained through finite difference methods. This higher accuracy is achieved without needing modifications to model architectures or task structures, while also maintaining a memory footprint similar to that of finite difference methods. First-order Forward Gradients. Gradients derived from Forward-mode Auto-Differentiation (AD) are considered first-order since it involves computing partial derivatives of the intermediate activations with respect to the input. We guide interested readers to the survey on different modes of automatic differentiation [24]. FGD [23] shows preliminary results on the speedup and comparable accuracy achieved by Forward-mode AD against backpropagation. The challenge that makes Forward-mode AD less popular than backpropagation is that gradients derived from forward mode require more jvp column-by-column evaluations per input batch as the number of trainable weights increases. Moreover, evaluation of FGD is limited to a multi-layer perceptron of size d ≈1.8M. Direct use of FGD to finetune language models leads to slow or no convergence. SPRY splits the trainable layers of a large language model across multiple clients in FL, letting each client finetune only a small subset of weights through forward gradients. Training or Finetuning Language Models in Federated Learning. In recent years, several frameworks have been proposed to train or finetune LLMs in FL [48–50]. The backpropagation-based"
            },
            {
                "section": "methods",
                "content": "[10, 51], even with parameter efficient finetuning (PEFT) and quantization [13, 52, 53], have large memory footprints due to the overhead related to activations, gradients, and gradient history storage for adaptive optimizers [18]. FEDFWD [54] applies FWDFWD [55] (which measures “goodness” of forward pass activations to judge which perturbations are useful) in FL, but FWDFWD struggles as model size scales up. FWDLLM uses zero-order gradients to finetune language models. It samples ∼10 perturbations per batch. For each batch, it picks 1 perturbation that has the highest cosine similarity with the previous round’s gradients. Sampling new perturbations based on aggregated gradients from previous rounds during the initial stages can disrupt the learning trajectory. SPRY requires 1 perturbation (without resampling) per batch to reach a higher prediction performance faster than FWDLLM. 16 B Datasets and Hyperparameters Here we provide details of the datasets and their corresponding training hyperparameters used in this work. Simulating Heterogeneity through Dirichlet Distribution. For each of the tasks, the class dis- tribution each client gets depends on the Dirichlet distribution, where a parameter α regulates the concentration of samples of a particular class for a specific client. Dir α = 1.0 means all clients have homogeneous datasets where each class is equally likely to be on each client. With Dir α →0, the datasets of each client get more heterogeneous, where the sample distribution of each class is more likely to be concentrated on only a subset of clients. Default Hyperparameters. Unless otherwise mentioned in dataset-specific paragraphs, the default hyperparameters for each method and for all datasets are stated here. For the backpropagation-based"
            },
            {
                "section": "methods",
                "content": "FEDAVG, FEDYOGI, and FEDSGD; we will fix the number of epochs to 1 since the goal of this work is to inch closer to backpropagation-like prediction performance while reducing the memory footprint. All the experiments have been run for 1500 FL rounds, except the experiments on OPT models, which are run for 600 FL rounds. Our observation from hyperparameter-tuning shows that the learning rate that gives the best performance is the same for all studied methods. BAFFLE and its memory-efficient improvement BAFFLE+ made by us, can perform better as the number of perturbations per batch increases, but due to the scale of experiments with 10-100 per round and up to 1500 rounds in the FL setting, we limit the total perturbations per batch of BAFFLE+ to 20 perturbations per batch and fixed finite difference step size σ = 1e-4. FWDLLM+ samples 10 perturbations for each batch, finite difference step size of σ = 1e-2. FEDMEZO samples 1 perturbation for each batch, with finite difference step size of σ = 1e-3. FEDMEZO also requires 3-5 epochs for each client. SPRY has 1 perturbation per batch for each client. For SPRY and its zero-order counterparts (BAFFLE+, FWDLLM+, and FEDMEZO), perturbations are sampled for a normal distribution with 0 mean and 1 variance. Default LORA r and α are 1 and 1, respectively. All methods use ADAMW as client-side optimizer. Besides FEDAVG, all methods use FEDYOGI as server-side optimizer. AG News. AG News dataset [31] has been derived from a corpus of 496,835 categorized news articles. A subset of the corpus is used that has 120,000 total training samples and 7,600 total testing samples spread equally across 4 classes. The news articles are classified into 4 classes: World, Sports, Business, and Sci/Tech. We split this data across 1000 clients. Each client gets an equal number of samples for train and test datasets. This dataset is under Creative Commons CCZero(CC0) public domain dedication. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 1e-5}. The batch size is set to 8. The max sequence length is 128. All methods use ADAMW as a client-side optimizer, while SPRY performs better with SGD. Variance threshold of FWDLLM+ is 1e+1. SST2. Stanford Sentiment Treebank Binary[32] (or SST2) dataset is for a binary sentiment classifi- cation task. The dataset has 11,855 sentences derived from a set of movie reviews. The corpus was parsed using the Stanford Parser into 215,154 discrete phrases annotated by 3 human judges. This dataset contains 67,349 training samples, 872 validation samples, and 1821 testing samples. This sentiment classification dataset has the following sentiments as classes: Positive, and Negative. These samples are equally split between 100 clients, depending on the Dirichlet distribution. This dataset is under Creative Commons CCZero public domain dedication. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 1e-5}. The batch size is set to 8. The max sequence length is 64. Variance threshold of FWDLLM+ is 5e+0. Yelp. The Yelp reviews dataset[31] is a binary classification dataset gathered during the 2015 Yelp Dataset Challenge. The full dataset has 1,569,264 total samples, and it defines 2 classification tasks. We use the polarity classification task, which is a binary classification problem. It considers 1-2 stars negative polarity and 3-4 stars positive polarity. This dataset has 280,000 training samples and 19,000 17 test samples in each polarity. We split this data into 1000 clients. This dataset is under the Apache License, Version 2.0. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 5e-5, 1e-5}. The batch size is set to 8. The max sequence length is 128. Variance threshold of FWDLLM+ is 5e+1. Yahoo. Yahoo! Answers Comprehensive Questions and Answers dataset [31] was gathered via the Yahoo! webscope program. The corpus itself contains 4,483,032 question/answer pairs, which were then formulated as a 10-class classification task. Each class in this dataset has 140,000 training and 5,000 testing samples.are The question/answer pairs are split into the following classes: 1. Society & Culture, 2. Science & Mathematics, 3. Health, 4. Education & Reference, 5. Computers & Internet, 6. Sports, 7. Business & Finance, 8. Entertainment & Music, 9. Family & Relationships, 10. Politics & Government. We split this dataset between 1000 clients, where each client gets an equal amount of data samples, where the data distribution is set by changing the Dirichlet distribution α to range from most homogeneous (α = 1) to least homogeneous (α = 0). This dataset is under the Apache License, Version 2.0. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 1e-5}. The batch size is set to 8. The max sequence length is 128. Variance threshold of FWDLLM+ is 5e+1. SNLI. The Stanford Natural Language Inference corpus [33] has 570,152 total sentence pairs. It is a natural language inference dataset, where the task is identifying if one sentence infers another. It has 550,152 training samples, 10,000 testing samples, and 10,000 evaluation samples. This dataset is split among 1,000 clients. The dataset has 3 classes: 1) The first sentence entails the second sentence, 2) The first sentence is neutral to the second sentence and 3) The first sentence contradicts the second sentence. This dataset is under CC BY-SA 4.0. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 5e-5, 1e-5}. The batch size is set to 8. The max sequence length is 80. Variance threshold of FWDLLM+ is 1e+2. MNLI. The Multi-Genre Natural Language Inference (MNLI) [34] corpus contains 432,702 sen- tence pairs which were crowd-sourced and then annotated with textual entailment information. This dataset is also a Natural Language Inference dataset as with SNLI. This dataset has 392,702 training, 20,000 evaluation, and 20,000 testing samples. These samples are split among 1,000 clients. This dataset draws from multiple sources, most of which are under the Open American National Corpus (OANC) license. The rest are under either the CC BY 3.0 Unported licenses or the CC BY-SA 3.0 licenses. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 5e-5, 1e-5}. The batch size is set to 8. The max sequence length is 80. Variance threshold of FWDLLM+ is 1e+2. SQuADv2. The Stanford Question Answering Dataset (SQuADv2) [35] consists of crowd-sourced questions about a set of Wikipedia articles. It is a reading comprehension dataset, where the answer to a question is a section (or a span) from the passage. It is also possible for the question to be unanswerable. The dataset contains 100,000 answerable and 50,000 unanswerable questions. This dataset was split into 500 clients. The heterogeneity is generated based on the topic labels (or “titles”) associated with each question in the dataset. There are 35 titles available. This dataset is under the CC BY-SA 4.0 license. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 1e-5}. The batch size is set to 8 for Forward-mode AD and zero-order differentiation-based methods. Backpropagation-based methods use a batch size of 4. The max sequence length is 400. Variance threshold of FWDLLM+ is 5e+1. 18 MultiRC. Multi-sentence Reading Comprehension (MultiRC) corpus is a dataset that contains short paragraphs with questions, whose answers can be found in the paragraph itself. We consider a task where we classify if the input question given as input is correct or false. It has 6k multi-sentence questions about 800+ paragraphs. Due to the scale of the dataset, we split it into 100 clients. This dataset is under the MIT license. Learning rate for backpropagation-based (FEDAVG, FEDYOGI, and FEDSGD), zero-order-based (FWDLLM, BAFFLE, and FEDMEZO), and first-order-based SPRY is {1e-3, 5e-4, 1e-4, 1e-5}. The batch size is set to 8. The max sequence length is 256. Variance threshold of FWDLLM+ is 1e+2. C Limitations and Future Work SPRY achieves a remarkable drop in memory consumption due to Forward-mode AD not having to store activations during the forward pass. However, the current implementation of Forward-mode AD by PyTorch is still suboptimal in terms of computation time. The high computation time is attributed to the column-by-column computation of the intermediate results of jvp. Improving the computation of jvp such that it takes significantly less time (almost half) than backpropagation remains an open and interesting problem. Moreover, there’s room for improvement in reducing the memory consumption to that of zero-order methods. In zero-order methods, the weights are perturbed and a forward pass is computed on the perturbed weights. However, with the current implementation of Forward-mode AD, perturbations create a separate copy from the original weights, which accrues additional overhead. To further utilize the computation capacity of clients in FL, device-heterogeneity-aware strategies on splitting and mapping layers to clients can be explored for SPRY, e.g., layer selection could happen on the client-side according to their data distributions. D Broader Impact Through SPRY, we aspire to bring impact in terms of data privacy and accessible finetuning of large language models (LLMs) on edge devices. For small and medium organizations and individual users, the cost to finetune these language models can be prohibitively expensive as the size of the trainable weights increases. With SPRY, we enable finetuning LLMs on resource-constrained edge devices with low memory consumption, making it feasible for a wider range of users. Moreover, the federated setting also provides benefits of data privacy. This can be ideal for use cases where LLMs can bring improved performance for a plethora of personalized downstream tasks, but the finetuning data containing sensitive and confidential information is never shared with a third party. With SPRY, such data remains on the user’s device, ensuring privacy while still allowing for effective finetuning of the LLM. However, making LLM finetuning accessible to a broader range of organizations and individuals comes with its own challenges like a spread of biases or misinformation. Without preprocessing and filtering client data on their devices, the LLMs can be fed harmful and misleading information. Hence, it is necessary to develop guardrails on what kind of data should be filtered in, to finetune LLMs with crowd-sourced compute resources. 19 E SPRY Pseudocode Algorithm 1 shows the workflow of SPRY. Algorithm 1: SPRY Input: R: Total number of rounds, r ∈[R]: Round index, M: Number of clients per round, m ∈[M]: Client index, M: Set of available clients, s: Client sampling rate, Dm: Dataset of client m, ηℓ: Local learning rate, www(r): Model weights at rth round, www(r) m : Subset of assigned trainable weights for client m at rth round, f: Objective function. Output: www(R+1): Model at the end of the training 1 Main SPRY 2 Server loads pre-trained LM www(1) with PEFT 3 for r ∈[R] round do 4 Sample M clients from M at rate of s 5 L ←list of trainable PEFT param names 6 client_layer_mapping ←MAPLAYERSTOCLIENTS(L, M) 7 for m ∈[M] in parallel do 8 www(r) m ←CLIENTTRAIN(www(r), client_layer_mapping[m]) 9 end 10 Build www′(r) with www(r) m using client_layer_mapping[m], ∀m ∈[M] 11 Use adaptive optimizer like FEDYOGI to build www(r+1) based on the aggregated www′(r) 12 end 13 end 14 Function MAPLAYERSTOCLIENTS(L, M) 15 client_idx ←0; mapping ←{} 16 for name ∈L do 17 rollover_idx ←client_idx % M 18 mapping[rollover_idx].update(name) 19 client_idx ←client_idx + 1 20 end 21 return mapping 22 end 23 Function CLIENTTRAIN(www(r) m , trainable_layers) 24 Freeze www(r) m parameters /∈trainable_layers 25 Generate perturbations vvv ←N(0, Iw w w(r) m .shape); ∀trainable www(r) m 26 jvp ←f.FORWARDAD(www(r) m ,vvv; Dm) 27 www(r) m ←www(r) m −ηℓ(vvv · jvp) 28 return all trainable www(r) m 29 end The function MAPLAYERSTOCLIENTS on Line 14 in Algorithm 1 shows how we have assigned only a few trainable layers to each client in FL, to make Forward-mode AD more effective at generating better estimation of the gradients. In function CLIENTTRAIN on Line 23 of Algorithm 1; each client m gets a copy of the entire language model www(r) m and a list trainable_layers of parameter names the client m has to train. A client m freezes the parameters that are not included in its trainable_layers. And for each of the parameter w(r) m which need to be trained, SPRY generates a corresponding random perturbation v using a normal distribution N(0, Iw(r) m .shape). Once a client m has obtained forward gradients of all the trainable parameters www(r) m , those parameters are locally updated with optimizers like SGD or ADAM. The updated trainable parameters www(r) m are sent back to the server. The server has a mapping of parameter names to client IDs, and hence it builds www′(r) by using www(r) m ∀m ∈[M]. If there are multiple clients mapped to the same parameter, then we take a weighted average (similar to FEDAVG) of all the parameters to build www′(r). SPRY uses adaptive optimizers like FEDYOGI at server-side on effective gradients ∆= www′(r) −www(r) to reduce the noise of forward gradients. 20 F Communication and Computation Costs F.1 Communication Costs Table 2 illustrates communication costs of SPRY and its backpropagation- and finite difference- based baselines. A discussion on communication modes of SPRY is also given in Section 3.2, “Per-Epoch Communication” and “Per-Iteration Communication”. Table 2: Communication cost of SPRY and all its baselines. M is the count of participation clients. Total count of trainable parameters of a global model is wg = wℓL (for simplicity, we assume that each layer has wℓparameters). Gradient computation Method (Comm. frequency) Communication cost (in parameter count) from each client to server for each communication round Communication cost (in parameter count) from server to all clients for each communication round Backpropagation (First-order gradients) FEDAVG / FEDYOGI (Per-epoch) wg wg × M FEDSGD (Per-iteration) wg wg × M Finite differences (Zero-order gradients) FEDMEZO / FWDLLM / BAFFLE (Per-epoch) wg wg × M FEDMEZO / FWDLLM BAFFLE (Per-iteration) 1 (of finite difference scalar) (wg + 1) × M (“1” is for perturbation seed) Forward-mode AD (First-order gradients) SPRY (Per-epoch) wℓ× max(L/M, 1) (Assuming L%M = 0 for each of exposition) wℓ× max(L/M, 1) × M = wℓ× max(L, M) SPRY (Per-iteration) 1 (of jvp scalar) (wℓ× max(L/M, 1) × M) +(1 × M) = wℓ× max(L, M) + M Here we discuss the costs related to those communication modes: Per-epoch Communication. SPRY’s client-to-server communication cost does not scale linearly with clients like in its backpropagation and finite-difference counterparts, but instead decreases or stays constant for L as more clients are present. Server-to-client communication cost is lower in SPRY due to only sending one layer per client when M > L or L M layers per client otherwise. This result follows from the below observation: Backpropagation-based and finite-difference-based methods have a communication cost of wg, where wg represents the global model size. Each client in [M] (set of participating clients) receives all trainable parameters from the server, requiring the server to send a total of wg × M parameters each round. SPRY’s communication cost per epoch is wℓmax( L M , 1), where L is the layer count and wℓis the count of parameters for each layer. Each client sends a subset of trainable parameters, incurring a communication cost of wℓL M parameters for L > M, and wℓfor L ≤M. When L ≤M, each client gets 1 layer, hence the communication for each client is wℓ. Per-iteration Communication. SPRY accrues lower communication cost than the finite difference and backpropagation counterparts due to the layer splitting strategy, and the server’s ability to compute gradients based on the jvp value. This is because: The communication cost from client to server for forward-mode AD and finite differences is 1. This is due to an FL round that involves (1) server selecting a random seed, (2) server sending it with trainable parameters to clients, (3) clients generating perturbations based on the seed, (4) deriving and 21 sending back a scalar or finite difference scalar to the server, and then (5) server computing gradients by multiplying the derived perturbations with the seed. The server to client communication is (wg + 1) × M, where the “+1” is due the randomness seed. F.2 Computation Costs Table 3 shows the computation costs of SPRY and its baselines, where the client-side cost is for each iteration, and the server-side cost is for each round. Briefly, SPRY’s client-side computation cost is traded off by a faster convergence to higher accuracy through better gradient approximations compared to finite difference-based methods. Furthermore, SPRY is the least computationally expensive on the server side due to needing to aggregate fewer parameters from the clients. Table 3 assumes that matrix multiplication costs c for each layer, resulting in a forward pass cost of c. The cost of backpropagation is 2c because the computation of the current layer’s weight gradient is c, and the cost of computing the previous layer’s activation gradient is another c. jvp computation in SPRY takes additional cost of c for each layer. Moreover, since jvp calculation happens through column-by-column vector multiplications (Sec 3.1 of [24]), the related overhead is quantified by v. Table 3: Computation cost of SPRY and all its baselines. The client-side cost is for each iteration, and the server-side cost is for each round. L is the layer count, M is the participating client count, c is the cost of matrix multiplication for each layer. v is the overhead related to column-by-column vector multiplications of jvp. wℓis the size of each layer and hence size of each layer’s perturbation too. K is the perturbation count per iteration. K = 1 for SPRY and FEDMEZO, and ∼20 for BAFFLE and FWDLLM. Gradient computation Method (Comm. frequency) Computation cost of each client for each iteration Computation cost of the server for each round Backpropagation (First-order gradients) FEDAVG / FEDYOGI (Per-epoch) 3Lc (Aggregating L layer weights from M clients) (M −1) × wℓL FEDSGD (Per-iteration) 3Lc (M −1) × wℓL Finite differences (Zero-order gradients) FEDMEZO (Per-epoch) L(2c + 3wℓ) (M −1) × wℓL FWDLLM / BAFFLE (Per-epoch) KL(2c + wℓ) (M −1) × wℓL FEDMEZO (Per-iteration) L(2c + 3wℓ) wℓL + wℓML + wℓ(M −1)L = 2MwℓL (perturbation generation + gradient calculation + weight update) FWDLLM / BAFFLE (Per-iteration) KL(2c + wℓ) 2MwℓL Forward-mode AD (First-order gradients) SPRY (Per-epoch) 2 × max( L M , 1) ×(c + v) + wℓL P M⊂[M] \u0010 (|M| −1)wℓmax ( L M , 1) \u0011 (usually, |M| = max ( M L , 1)) SPRY (Per-iteration) 2 × max( L M , 1) ×(c + v) + wℓL P M⊂[M] \u0010 2|M|wℓmax ( L M , 1) \u0011 Client-side per-iteration computation cost. Backpropagation needs 3 matrix multiplication op- erations per layer. For zero-order methods, there are 2 matrix multiplications (incurred due to two forward passes) per layer, and per perturbation within a training iteration; and additional overhead wℓKL for perturbation generation. MEZO requires generation of perturbations thrice for the same seed (Algorithm 1 in MEZO [18]). 22 SPRY ’s computation cost is 2 × max( L M , 1) × (c + v) + wℓL. Since SPRY allocates at most L M layers to each client, the computation cost only scales with max( L M , 1), against its counterparts scaling with L. However, forward-mode AD computes jvp column-wise, while its counterpart vjp in backpropagation is computed row-wise. This results in time overhead ( ) if the number of trainable parameters exceeds the output size (1 as loss is scalar), which is the case for neural networks. Therefore, SPRY’s per-iteration computation cost is higher compared to other approaches. Note that the per-iteration computation cost of SPRY is not the whole picture. It takes fewer communication rounds to reach higher accuracy due to better gradient approximation of forward- mode AD than finite difference methods. This is why \"Time to Convergence\" (Section 5.3) discusses a fair comparison of SPRY’s runtime and prediction performance. Server-side per-round computation cost. On the server side, SPRY is the least computationally demanding. SPRY needs to aggregate a subset of layer weights from only the clients that were assigned to those layers, while its counterparts need to aggregate all layers from all clients. Computation cost on the server-side changes based on the communication frequency per-iteration communication incurs an additional overhead of wℓL( M L + 1) and wℓL(M + 1) (generation of perturbations at the server-side, and multiplying those perturbations with aggregate of the jvp values received from the clients) for SPRY and its zero-order counterparts respectively. 23 Table 4: Generalized (Accg) and personalized (Accp) accuracies (the higher, the better) for SPRY and its backpropagation and zero-order based counterparts on various language model architectures. The datasets are split with Dir α = 0.1. Backpropgation-based Zero-order based Method First-order Forward-mode AD FEDAVG FEDYOGI FWDLLM+ SPRY Accg Accp Accg Accp Accg Accp Accg Accp AG News on BERT Base 93.00% 93.34% 93.31% 93.88% 83.41% 83.42% 86.74% 92.42% SST2 on DistilBERT Base 91.47% 95.28% 87.95% 92.97% 79.09% 80.94% 84.90% 87.12% SNLI on BERT Large 85.79% 89.36% 86.72% 90.33% 66.76% 64.84% 77.01% 77.21% Yahoo on DistilBERT Base 69.13% 74.75% 63.84% 71.25% 54.29% 55.87% 61.17% 61.47% Yelp on AlbertV2 Large 90.17% 92.78% 90.24% 94.00% 82.65% 83.25% 85.80% 86.00% 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg LoRA Bitfit IA3 Classifier-only (a) Effect of different PEFT"
            },
            {
                "section": "methods",
                "content": "on SPRY 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg FedAvg (Per-Epoch Communication) FedSgd (Per-Iteration Communication) Spry (Per-Epoch Communication) SpryPerIter (Per-Iteration Communication) (b) Effect of per-epoch and per-iteration communication 0 200 400 600 800 10001200 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg r = 1, =1 r = 8, =16 r = 16, =16 r = 32, =32 (c) Changing LORA r and α for SPRY Figure 4: Ablation studies on PEFT methods, communication frequency, and LORA hyperparameters. G Ablation Studies Here we will dive into various components of SPRY and see their impact on the performance of SPRY. SPRY can Generalize to Different Language Model Architectures. Table 4 shows generalized and personalized accuracies Accg and Accp for homogeneous data splits for language model architectures other than RoBERTa Large. For zero-order gradients, we show the results of the best-performing method, FWDLLM+. We see a similar trend of SPRY outperforming FWDLLM+ by 3.15-10.25% for generalized accuracy, and by 2.75-12.37% for personalized accuracy, exhibiting how SPRY is independent of model architectures. SPRY also comes as close as 4.44-9.71% to the best-performing backpropagation-based method. SPRY Supports Other PEFT Methods. Figure 4a shows the generalized accuracy of SPRY using three different PEFT methods: LORA, IA3, and BITFIT. We also experiment with finetuning only classifier layers, calling it CLASSIFIER-ONLY. LORA (with 0.3241% of the total parameters of RoBERTa Large) outperforms IA3 (with 0.3449% of the total parameters) by 10.60%, while BITFIT fails to converge for all datasets. Only training classifier layers is worse than finetuning LORA weights by 16.53%. The observation on comparison of LORA with IA3 is consistent with the work benchmarking various PEFT methods [56]. BITFIT has been observed to fail on LLMs [57]. Furthermore, unlike IA3, LORA has been shown to be successful at finetuning quantized billion-sized models in QLORA [58], making it a strong candidate for our work. Effects of Communication Frequency. One way to reduce the noise introduced by the random perturbations in gradient computation is, to communicate the gradients back to the server every iteration instead of every few epochs. Figure 4b shows results of per-epoch and per-iteration communication variants of SPRY and FEDAVG. 24 0 100 200 300 400 500 600 700 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg K = 1 K = 10 K = 20 K = 50 K = 100 (a) Changing K (Forward-mode AD perturbation count per iteration) 0 100200300400500600700800 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg C = 10 C = 50 C = 100 (b) Changing C (Per-round participating client count) 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg FedAvg FedAvgSplit FedFgd Spry 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 Generalized Test Accuracy Accg FedAvg FedAvgSplit FedFgd Spry (c) RoBERTa Large (Left) and BERT Base (Right) - Splitting parameters across clients on Backpropagation vs Forward-mode AD Figure 5: Ablation studies on perturbation counts, participating client counts, and layer splitting strategy. By communicating each iteration, we see a boost of 4.47% in the accuracy of SPRY, only 0.92% and 0.96% away from the accuracy of FEDAVG and FEDSGD respectively. Furthermore, as shown in Figure 4b, the convergence speed of SPRY also improves by communicating each iteration. As discussed in Section 3, to reduce the trade-off between performance gains and communication cost per iteration, each client can send only the jvp scalar to the server instead of transmitting all the assigned trainable weights [20]. With the seed value of the randomness, the server can then generate the random perturbation vector vvv, which was used by all the clients to generate their respective jvp values. Then the random perturbation is multiplied with the received jvp values of all clients, to compute gradients. Effects of the Number of Trainable Weights. Figure 4c displays the effect of changing trainable LORA parameter count on the prediction performance of SPRY. For DistilBERT Base (total parameter count of 66M), LORA reduces the trainable parameter count to 0.61M (0.91%), 0.74M (1.11%), 0.89M (1.33%), and 1.18M (1.77%) with LORA hyperparameter settings of (r=1, α=1), (r=8, α=16), (r=16, α=16), and (r=32, α=32). SPRY achieves the highest accuracy of 84.90% with (r=1, α=1) setting, which has the smallest trainable parameter count. The accuracy increases as the layer size decreases since fewer perturbed weights provide less noisy gradients. Effects of the Number of Perturbations per Batch. The effect of increasing the number of perturbations per batch and hence the number of jvp evaluations for a batch is shown in Figure 5a for the SST2 dataset. Here, gradients generated from each random perturbation and their corresponding jvp values are averaged to update the model. We observe that increasing K (perturbations per batch) for Forward-mode AD has little to no impact on the end prediction performance of SPRY, with K = 100 improving the generalized test accuracy by 1.1% over K = 1. However, the benefits of increasing the perturbation count per batch are seen in the convergence speed. Setting K = 10 achieves a steady state (of accuracy ∼86%) around 200th round, while the setting with K = 1 takes 500 rounds. The improvements in convergence speed are saturated for K > 10. This shows that more perturbations reduce the gradient estimation noise only to an extent. Effects of the Number of Participating Clients per Round. Figure 5b shows how changing the per-round number of participating clients C influences SPRY on the SST2 dataset. Increasing client count increases the prediction performance of SPRY. With the total client count fixed to 100, the three settings C = 10, C = 50, and C = 100 produce accuracies of 85.14%, 86.56%, and 88.08%, respectively. Similar to the findings of Section G, we also see an improvement in the convergence speed as the participating client count increases. To achieve an accuracy of ∼85%; C = 10, C = 50, C = 100 require 500, 450, and 150 rounds respectively. The performance gains and faster convergence are due to more clients training the weights of the same layers. The Importance of Splitting Layers. To understand the effects of splitting, we compare the results of the following two experiments: (a) With FEDAVGSPLIT, we apply the strategy of splitting trainable layers across clients (Section 3.1) to backpropagation-based FEDAVG, and (b) With FEDFGD, we omit the splitting strategy of SPRY. 25 Figure 5c shows the performance of FEDAVG and FEDAVGSPLIT against FEDFGD and SPRY for two LMs: RoBERTa Large (355M) and BERT Base (110M). We observe that FEDAVGSPLIT fails to achieve similar accuracy for both models with a drop of 2.60% and 10.00%. This is because in FEDAVGSPLIT, fewer clients are training each subset of weights. Moreover, we see a similar accuracy with an absolute difference of 2.70-3.61% between FEDAVGSPLIT and SPRY, since the trainable weight count per client is low. FEDYOGISPLIT follows the same observation of not achieving similar accuracy to FEDYOGI if the trainable weights are split across clients. On the contrary, FEDFGD converges for the smaller model BERT base, albeit 150 rounds slower than SPRY, and with 2.87% accuracy drop. But as the size of trainable weights increases, e.g., for RoBERTa Large, FEDFGD fails to converge. This proves the necessity of splitting layers for Forward-mode AD so that each client has fewer trainable weights to perturb. 26 H Additional Results H.1 Generalized Performance Curves Generalized results on homogeneous and heterogeneous clients with Dir α = 1.0 and α = 0.1 are shown in (a) Figures 6 and 8 for RoBERTa Large, Llama2-7B, OPT6.7B, OPT13B; and (b) Figure 7 for BERT Large, BERT Base, DistilBert Base, Albert Large v2. 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg FedAvg FedYogi FedSgd FwdLLM+ FedMeZO Baffle+ Spry (a) AG News with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (b) SST2 with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (c) SNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (d) MNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (e) Yahoo with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (f) Yelp with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (g) MultiRC with Llama2-7B 0 100 200 300 400 500 600 Rounds 0 10 20 30 40 Generalized Test F1 Score F1g (h) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 25 30 35 Generalized Test Exact Matches EMg (i) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 Generalized Test F1 Score F1g (j) SQuADv2 with OPT13B 0 100 200 300 400 500 600 Rounds 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Generalized Test Exact Matches EMg (k) SQuADv2 with OPT13B Figure 6: Generalized accuracy / F1 score / Exact matches for homogeneous clients (Dirichlet α = 1.0) setting 27 0 200 400 600 800 1000 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg FedAvg FedYogi FedSgd FwdLLM+ Spry (a) AG News with BERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (b) SST2 with DistilBERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (c) SNLI with BERT Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (d) Yahoo with DistilBERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (e) Yelp with Albert Large v2 Figure 7: Generalized accuracy /F1 score / Exact matches for homogeneous clients (Dirichlet α = 1.0) setting for a variety of language models 28 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg FedAvg FedYogi FedSgd FwdLLM+ FedMeZO Baffle+ Spry (a) AG News with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (b) SST2 with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (c) SNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (d) MNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (e) Yahoo with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (f) Yelp with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Generalized Test Accuracy Accg (g) MultiRC with Llama2-7B 0 100 200 300 400 500 600 Rounds 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Generalized Test F1 Score F1g (h) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 Generalized Test Exact Matches EMg (i) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 2 4 6 8 10 12 Generalized Test F1 Score F1g (j) SQuADv2 with OPT13B 0 100 200 300 400 500 600 Rounds 0 2 4 6 8 Generalized Test Exact Matches EMg (k) SQuADv2 with OPT13B Figure 8: Generalized accuracy / F1 score / Exact matches for heterogeneous clients (Dirichlet α = 0.1) setting 29 H.2 Personalized Performance Curves Personalized results on homogeneous and heterogeneous clients with Dir α = 1.0 and α = 0.1 are shown in (a) Figures 9 and 11 for RoBERTa Large, Llama2-7B, OPT6.7B, OPT13B; and (b) Figure 10 for BERT Large, BERT Base, DistilBert Base, Albert Large v2. Table 5 shows accuracy and F1 scores of SPRY and its backpropagation and zero-order based counterparts. 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp FedAvg FedYogi FedSgd FwdLLM+ FedMeZO Baffle+ Spry (a) AG News with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (b) SST2 with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (c) SNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (d) MNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (e) Yahoo with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (f) Yelp with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (g) MultiRC with Llama2-7B 0 100 200 300 400 500 600 Rounds 0 10 20 30 40 Personalized Test F1 Score F1p (h) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 25 30 35 Personalized Test Exact Matches EMp FedAvg FedYogi FedSgd FwdLLM+ Baffle+ Spry (i) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 Personalized Test F1 Score F1p (j) SQuADv2 with OPT13B 0 100 200 300 400 500 600 Rounds 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Personalized Test Exact Matches EMp (k) SQuADv2 with OPT13B Figure 9: Personalized accuracy / F1 score / Exact matches for homogeneous clients (Dirichlet α = 1.0) setting 30 0 200 400 600 800 1000 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp FedAvg FedYogi FedSgd FwdLLM+ Spry (a) AG News with BERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (b) SST2 with DistilBERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (c) SNLI with BERT Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (d) Yahoo with DistilBERT Base 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (e) Yelp with Albert Large v2 Figure 10: Personalized accuracy / F1 score / Exact matches for homogeneous clients (Dirichlet α = 1.0) setting for a variety of language models Table 5: Personalized accuracy (Accp) for SPRY and its backpropagation- and zero-order-based counterparts on RoBERTa Large and LLMs. SQuADv2 uses F1 score. ↑shows that higher values are better. The datasets are split with Dir α = 0.1. ⋄= Llama2-7B. ⋆= OPT6.7B. □= OPT13B. SPRY significantly outperforms the best-performing zero-order-based methods. Backpropagation-based"
            },
            {
                "section": "Methods",
                "content": "↑ Zero-order-based"
            },
            {
                "section": "Methods",
                "content": "↑ First-order Forward Mode AD ↑ Difference between performances of SPRY and FEDAVG FEDYOGI FWDLLM+ FEDMEZO BAFFLE+ SPRY best-performing backpropagation method ↑ best-performing zero-order method ↑ AG News 97.76% 97.71% 79.94% 72.69% 60.89% 89.91% -7.85% 9.97% SST2 95.84% 95.90% 85.51% 73.26% 64.55% 93.40% -2.50% 7.89% SNLI 97.41% 97.57% 74.53% 71.54% 68.55% 83.45% -14.12% 8.92% MNLI 90.38% 90.03% 72.71% 67.53% 63.58% 80.63% -9.75% 7.92% Yahoo 89.76% 89.64% 77.93% 67.64% 59.40% 82.80% -6.96% 4.87% Yelp 93.44% 97.81% 73.04% 68.77% 57.78% 85.83% -11.98% 12.79% MultiRC ⋄ 50.28% 75.21% 65.91% N/A 61.41% 71.20% -4.01% 5.29% SQuADv2 ⋆ 20.49 21.25 14.43 14.04 12.27 17.73 -3.52 3.30 SQuADv2 □13.06 12.49 8.96 9.10 8.17 9.88 -3.18 0.78 31 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp FedAvg FedYogi FedSgd FwdLLM+ FedMeZO Baffle+ Spry (a) AG News with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (b) SST2 with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (c) SNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (d) MNLI with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (e) Yahoo with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (f) Yelp with RoBERTa Large 0 200 400 600 800 1000 1200 1400 Rounds 0.0 0.2 0.4 0.6 0.8 1.0 Personalized Test Accuracy Accp (g) MultiRC with Llama2-7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 Personalized Test F1 Score F1p (h) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 5 10 15 20 Personalized Test Exact Matches EMp (i) SQuADv2 with OPT6.7B 0 100 200 300 400 500 600 Rounds 0 2 4 6 8 10 12 14 Personalized Test F1 Score F1p (j) SQuADv2 with OPT13B 0 100 200 300 400 500 600 Rounds 0 2 4 6 8 10 Personalized Test Exact Matches EMp (k) SQuADv2 with OPT13B Figure 11: Personalized accuracy / F1 score / Exact matches for heterogeneous clients (Dirichlet α = 0.1) setting 32 H.3 Experiment Variance Tables 6 and 7 show the variance of running the same experiments thrice with the random seeds 0, 1, and 2. Table 6: Experimental variance (±) for SPRY and its counterparts on RoBERTa Large. Backpropgation-based"
            },
            {
                "section": "Methods",
                "content": "Zero-order-based"
            },
            {
                "section": "Methods",
                "content": "First-order Forward-mode AD FEDAVG FEDYOGI FWDLLM+ FEDMEZO BAFFLE+ SPRY Accg Accp Accg Accp Accg Accp Accg Accp Accg Accp Accg Accp AG News 0.51% 0.44% 0.26% 0.21% 0.73% 0.71% 1.34% 1.27% 0.68% 0.51% 1.16% 1.08% SST2 0.37% 0.47% 0.43% 0.37% 1.26% 1.14% 1.13% 1.06% 0.41% 0.45% 0.77% 0.95% SNLI 0.45% 0.39% 0.17% 0.11% 0.82% 0.67% 0.74% 0.65% 0.84% 0.78% 0.51% 0.45% MNLI 0.63% 0.58% 0.29% 0.24% 1.39% 1.25% 1.98% 1.85% 1.15% 1.01% 1.45% 1.32% Yahoo 0.24% 0.33% 0.53% 0.47% 0.47% 0.44% 1.06% 0.93% 0.59% 0.48% 0.99% 0.85% Yelp 0.22% 0.25% 0.36% 0.27% 0.54% 0.49% 0.82% 0.66% 0.83% 0.71% 0.76% 0.61% Table 7: Experimental variance (±) for generalized (Accg for MultiRC / F1g for SQuADv2) and personalized (Accp for MultiRC / F1p for SQuADv2) accuracy or F1 score for SPRY and its counterparts. ⋄= Llama2 7B. ⋆= OPT 6.7B. □= OPT 13B. Backpropgation-based"
            },
            {
                "section": "Methods",
                "content": "Zero-order-based"
            },
            {
                "section": "Methods",
                "content": "First-order Forward-mode AD FEDAVG FEDYOGI FWDLLM+ FEDMEZO BAFFLE+ SPRY Accg Accp Accg Accp Accg Accp Accg Accp Accg Accp Accg Accp MultiRC ⋄ 0.58% 0.43% 0.34% 0.29% 0.89% 0.74% N/A N/A 1.34% 1.02% 0.97% 0.81% SQuADv2 ⋆1.73 1.07 1.42 0.84 2.17 1.87 1.78 1.51 2.78 2.01 1.76 0.99 SQuADv2 □0.86 0.43 0.61 0.45 1.16 0.97 1.03 0.87 1.34 1.14 0.97 0.81 33 I Proofs I.1 Basics Server Update. The server update of SPRY uses adaptive optimizer FEDYOGI. However, to simplify the proofs without losing generality, we use the server update of FEDADAM [25]. FEDADAM has the exact update rule as FEDYOGI but without a sign function in its calculation of the second moment of the gradients (See Algorithm 2 of AFO [25]). Hence, the server update of SPRY is w(r) ←w(r−1) + η ∆(r) √ Λ(r) + τ ∀trainable weights w(r) ∈[d]. (7) ∆(r) is the square of accumulated gradients from all clients. Λ(r) is the second moment of ∆(r). τ is a small positive real number, to prevent division by zero errors. Note that we are assuming flattened weights w ∈Rd without the loss of generality. With SPRY, the aim is to solve the following optimization problem: min w∈Rd f(w) = 1 m M X m=1 Fm(wm), (8) where m ∈ h (m−1)d M + 1, md M i , Fm(wm) = E(x,y)∼Dm[fm(wm, (x, y))] is the objective function, Dm is the dataset, and fm is the loss function of a client m ∈[M]. Accumulated Gradients. With SPRY, each client trains a subset of weights wm. In a low participa- tion rate setting, each wm is only trained by one of the participating clients from the set of available clients M. Although we make our analysis more generally applicable by showing multiple clients training the same wm. The true global gradients can be written as, ∇f(w) =  1 f M X m∈f M ∇F(wm) m ∈ \u0014(m −1)d M + 1, md M \u0015 , f M ⊂M   (9) where f M is a set of clients training the subset of the weights wm. f M is the size of f M. Client Update. The directional derivative of Forward-mode AD is denoted as ∇ˆfv(w; (x, y)), where v ∈Rd is the random perturbation of weights w and (x, y) are sampled from a dataset D. For each client m, through Forward-mode AD, we have ∇ˆfm(wm, vm; (x, y)) = (∇ˆfm,v(wm; (x, y)) · vm) to estimate the true gradient ∇Fm(wm): Evm,Dm h ∇ˆfm(wm, vm; Dm) i = 1 DK X (x,y)∼Dm K X i=1 Evi,m,(x,y) \u0002 ∇fm(wm; (x, y))vi,mvT i,m \u0003 (10) Evm,Dm h ∇ˆfm(wm, vm; Dm) i = ∇Fm(wm) ∵Theorem 1 of FGD [23] (11) Here, the expectation is under the randomness of sampled data D, and random perturbation v. K is the number of perturbations per batch. SPRY uses K = 1 by default, but here we aim to make our analysis more general to see the impact of K on various properties of SPRY. I.2 Assumptions We will be using the following assumptions to derive the first and second moment of the forward gradient and to calculate the rate of convergence of SPRY, Assumption I.1 (Smoothness). The gradient of function Fm is L-Lipschitz, ||∇Fm(w1) −∇Fm(w2)|| ≤L||w1 −w2||; ∀m ∈[M] and w1, w2 ∈Rd. 34 Assumption I.2 (Bounded Global Variance (Assumption 2 in AFO [25])). The variance of the global objective function f is bounded by σg as 1 M M X m=1 || [∇Fm(wm)]j −[∇f(wm)]j ||2 ≤σ2 g,j; ∀m ∈[M], w ∈Rd and ∀j ∈[d], where ∇Fm is the true gradient of client m. As defined earlier, m ∈ h (m−1)d M + 1, md M i . Assumption I.3 (Bounded Gradients (Assumption 3 in AFO [25])). The function fm(w; (x, y)) has G-bounded gradients such that for any client m ∈[M], weights w ∈Rd, and sampled data (x, y) ∼Dm; we have | [∇fm(w; (x, y))]j | < G, ∀j ∈[d] I.3 First and Second Moments of Forward Gradients We first prove that in SPRY, estimation of ∇f by the accumulated forward mode gradients ∇ˆf across all clients of an arbitrary round r, depends on the heterogeneity across client datasets. Statements on homogeneous data have been proven for FGD [23] and MEZO [18] in single-client or centralized settings. Here we focus on the specific federated setting of SPRY, where gradients are accumulated differently than in traditional FEDAVG [1]. In SPRY, we have ∇f(w) = Ev h ∇ˆf(w, v) i =  1 f M X m∈f M E h ∇ˆfm(wm, vm; Dm) i \f\f\f ∀f M ⊂M; m ∈ \u0014(m −1)d M + 1, md M \u0015 , (12) where w represents weights, v are their corresponding perturbations, and Dm is the dataset of client m. We omit the round index of the model weights w and their perturbations v for this section since the same relationship will hold for any arbitrary round r. Theorem I.4 (Estimation of the Global Gradient). In SPRY, global forward gradient ∇ˆf of the trainable weights w ∈Rd, with the corresponding weight perturbations v ∈Rd, computed by M participating clients is estimated in terms of true global gradient ∇f as, Ev,D[∇ˆf(w, v; D)] = ∇f(w) + 1 f M   P m∈f M1 PC c=1 αm,cE(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i P m∈f M2 PC c=1 αm,cE(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; (x, yc)) i ...   T where C is total number of classes and αm,c = \u0010 nc |D| −nm,cαc |Dm| \u0011 . For a class c, nc is its sample count, αc is its Dirichlet concentration parameter. For a client m; nm,c is the sample count of the cth class, and Dm is the size of the data of client m. The global data is D = P m∈M Dm. f M is the set of clients training an arbitrary subset of weights, f M = | f Mi|; ∀i ∈[M/d]. Proof. Suppose the global dataset D is defined as a combination of all Dm. Hence, D = ∪m∈[M]Dm. In SPRY, the global forward gradient is defined as Ev,D[∇ˆf(w, v; D)] = E   1 f M P m∈f M1 ∇ˆfm(w[1, d M ], v[1, d M ]; D) 1 f M P m∈f M2 ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; D) ... 1 f M P m∈f MM/d ∇ˆfm(w[ (M−1)d M +1,d], v[ (M−1)d M +1,d]; D)   T (13) 35 To combine the gradient estimates from the above equation for all clients m ∈M, we first consider the bias bm between the gradient estimate under (a) Globally combined data D and (b) Data Dm of an arbitrary client m. Measuring the dataset bias. Note that we consider samples (x, y) of Dm to be sampled from D, since Dm ⊂D. Evm,D h ∇ˆfm(wm, vm; D) i = Evm,Dm h ∇ˆfm(wm, vm; Dm) i + bm (14) Computing the bias term b requires information on dataset distribution. In FL settings, Dirichlet distribution to spread the data into heterogeneous splits is a popular way to simulate heterogeneity [37]. The biased dataset Dm is sampled from the combined dataset D using the Dirichlet distribution. This allows us to utilize the properties of the distribution to derive the relationship between forward gradients on global data and on individual local data: For classification tasks, let’s say D has C total classes: y1, . . . , yC. We have α1, . . . , αC as concen- tration parameters of the Dirichlet distribution. The expected forward gradient for the combined dataset D can be expressed as a weighted sum of the expected forward gradients for each class C: Evm,D h ∇ˆfm(wm, vm; D) i = C X c=1 nc |D|Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i , (15) where nc is the sample count of class c. And for the biased dataset Dm sampled with Dirichlet concentration parameters α1, . . . , αC, the expected forward gradient can be expressed as, Evm,Dm h ∇ˆfm(wm, vm; Dm) i = C X c=1 nm,cαc |Dm| Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i , (16) where nm,c is the sample count of class c for client m. Subtracting Equation 16 from Equation 15, bm = Evm,D h ∇ˆfm(wm, vm; D) i −Evm,Dm h ∇ˆfm(wm, vm; Dm) i (17) = C X c=1 \u0012 nc |D| −nm,cαc |Dm| \u0013 Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i (18) For any m ∈ h (m−1)d M + 1, md M i , we have Evm,Dm h ∇ˆfm(wm, vm; Dm) i = Fm(w) from Lemma 1 in FGD [23]. Plugging in the above result and Equation 18 in Equation 14, Evm,D h ∇ˆfm(wm, vm; D) i = Evm,Dm h ∇ˆfm(wm, vm; Dm) i + bm = ∇Fm(wm) + C X c=1 \u0012 nc |D| −nm,cαc |Dm| \u0013 Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i (19) For ith row in Equation 13, Evm,D h ∇ˆfi(wm, vm; D) i = 1 f M X m∈f M1 Evm,D h ∇ˆfm(wm, vm; D) i (20) = 1 f M X m∈f M1 ∇Fm(wm) + C X c=1 \u0012 nc |D| −nm,cαc |Dm| \u0013 Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i! (21) = ∇f(wm) + 1 f M X m∈f M1 C X c=1 \u0012 nc |D| −nm,cαc |Dm| \u0013 Evm,(x,yc)∈D h ∇ˆfm(wm, vm; (x, yc)) i (22) 36 Putting Equation 22 in Equation 13, Ev,D[∇ˆf(w, v; D)] (23) =   ∇f(w[1, d M ]) + 1 f M P m∈f M1 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i ∇f(w[ d M +1, 2d M ]) + 1 f M P m∈f M2 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; (x, yc)) i ...   T (24) =   ∇f(w[1, d M ]) ∇f(w[ d M +1, 2d M ]) ...   T + 1 f M   P m∈f M1 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i P m∈f M2 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; (x, yc)) i ...   T (25) = ∇f(w) + 1 f M   P m∈f M1 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i P m∈f M2 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; (x, yc)) i ...   T (26) Next, we formulate the norm of the forward gradient ∇ˆf. Lemma I.5 (Norm of the Forward Gradient). Under Assumption I.2, and at the participation rate of s, M participating clients training weights w ∈Rd through random perturbations v ∈Rd in SPRY derives the accumulated forward gradient ∇ˆf(w, v; D) such that, Ev,D||∇ˆf(w, v; D)||2 = E||∇f(w)||2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   + 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 , (27) where D is the combination of datasets of all M clients, K is the number of perturbations per batch, f M is the count of clients training a particular weight subset, σ2 g is the upper bound of the global gradient variance. For a total of C classes in D, we define nc as the sample count of the cth class, αc is Dirichlet concentration parameter for the cth class, D is the size of the global data. For a client m; nm,c is the sample count of the cth class for client m, and Dm is the size of the data of client m. Proof. The proof follows a similar style of Lemma 2 in MEZO [18]. The difference in our setting is that we have ∇ˆf(w, v; D) which is an aggregate of ∇ˆfm(w, v; D) derived from the federated clients, while MEZO has results under the setting of a single client. From Theorem I.4 we have, Ev,D h ∇ˆf(w, v; D) i =   ∇f(w[1, d M ]) + 1 f M P m∈f M1 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[1, d M ], v[1, d M ]; (x, yc)) i ∇f(w[ d M +1, 2d M ]) + 1 f M P m∈f M2 PC c=1 \u0010 nc |D| −nm,cαc |Dm| \u0011 E(x,yc)∈D h ∇ˆfm(w[ d M +1, 2d M ], v[ d M +1, 2d M ]; (x, yc)) i ...   T (28) 37 where Evm,(x,yc)∼D h ∇ˆfm(wm, vm; (x, yc)) i = 1 |D|K X (x,yc)∼D X i∈[K] E \u0002 (∇fm (wm; (x, yc)) · vi,m) vT i,m \u0003 , using Equation 10. The right hand side expectation is also under the randomness of partial perturba- tions. Here, K is perturbation count, and |D| is the size of the combined dataset D = ∪m∈[M]Dm. To compute the second moment for a client m, Evm,(x,yc)∼D h ∇ˆfm(wm, vm; (x, yc)) · ∇ˆfm(wm, vm; (x, yc))T i = 1 |D|2K2 X (x,yc∼D) (x,yc∼D) X i∈[K] j∈[K] E \" \u0000∇fm(wm; (x, yc))vi,mvT i,m \u0001 · \u0000∇fm(wm; (x, yc))T vj,mvT j,m \u0001 # (29) For simplicity, let a and b be two arbitrary vectors representing E [∇fm(wm; (x, yc))] and E \u0002 ∇fm(wm; (x, yc))T \u0003 , respectively. For the sum over all perturbations, we have two cases (all the expectations are under the randomness of partial v), 1. If i ̸= j. (Occurs K(K −1) times) E \u0002 vi,mvT i,m abT vj,mvT j,m \u0003 = E \u0002 vi,mvT i,m \u0003 · abT · E \u0002 vj,mvT j,m \u0003 (30) (since vi and vj are independent of each other and a, b don’t depend on v) = I · abT · I = abT (31) 2. If i = j. (Occurs K times) E \u0002 vi,mvT i,m abT vj,mvT j,m \u0003 = E \u0002 vi,mvT i,m abT vi,mvT i,m \u0003 (32) = Evi \u0002 v4 i \u0003 ⟨a, b⟩ (33) For all such vi with i ∈[K], Ev[v⊗4]⟨a, b⟩= 3d Sym(I⊗4)⟨a, b⟩ (34) = 2d · abT + d · I · aT b (35) Plugging in the results of the above two cases in Equation 29, ∴E h ∇ˆfm(wm, vm; (x, yc)) · ∇ˆfm(wm, vm; (x, yc))T i = 1 |D|2K2 X (x,yc∼D) (x,yc∼D) \u0002 K(K −1)abT + 2dKabT + dK · I · aT b \u0003 (36) = 1 |D|2K X (x,yc∼D) (x,yc∼D) \" (2d + K −1)E \u0002 ∇fm(wm; (x, yc))∇fm(wm; (x, yc))T \u0003 + d · I · E h ∇fm(wm; (x, yc))∇fm(wm; (x, yc))T i# (37) Here, the randomness is under the sampled data. For the sum over samples of D, we have two cases, 1. If (x, yc) ̸= (x, yc). (Occurs |D|(|D| −1) times) E[∇fm(wm; (x, yc))∇fm(wm; (x, yc))T ] = ∇Fm(wm)∇Fm(wm)T (38) 38 2. If (x, yc) = (x, yc). (Occurs |D| times) E[∇fm(wm; (x, yc))∇fm(wm; (x, yc))T ] = ∇Fm(wm)∇Fm(wm)T + Σ(wm) (39) Combining both the cases, we get E(x,yc)∼D[∇fm(wm; (x, yc))∇fm(wm; (x, yc))T ] = |D|2∇Fm(wm)∇Fm(wm)T + |D| · Σ(wm) (40) Plugging in Equation 40 in Equation 37, Evm,(x,yc)∼D[∇ˆfm(wm, vm; (x, yc))∇ˆfm(wm, vm; (x, yc))T ] = 1 |D|2K \" (2d + K −1)|D|(|D|∇Fm(wm)∇Fm(wm)T + Σ(wm)) + d · |D| \u0000|D| · ||∇Fm(wm)||2 + tr (Σ(wm)) \u0001 # (41) = (2d + K −1) K \u0012 ∇Fm(wm)∇Fm(wm)T + 1 |D|Σ(wm) \u0013 + d K \u0012 ||∇Fm(wm)||2 + 1 |D|tr (Σ(wm)) \u0013 (42) = 3d + K −1 K E(x,yc)∼D||∇fm(wm; (x, yc))||2 (43) Hence for client m, the expected norm of forward gradients under randomness of perturbations v is, Evm,(x,yc)∼D||∇ˆfm(wm, vm; (x, yc))||2 = 3d + K −1 K E(x,yc)∼D||∇fm(wm; (x, yc))||2 (44) = 3d + K −1 K ||∇Fm(wm)||2 (45) For ith row of E h ∇ˆf(w, v; D) i of Equation 28, E ∇ˆfi(w[ (i−1)d M +1, id M ], v[ (i−1)d M +1, id M ]; D) 2 = E ∇fi(w[ (i−1)d M +1, id M ]) 2 + 1 (f M)2 X m∈f Mi X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 E(x,yc)∼D ∇ˆfm(w[ (i−1)d M +1, id M ], v[ (i−1)d M +1, id M ]; (x, yc)) 2 (46) The left-hand side expectation under the randomness of sampled data D and subset of random perturbations v. Plugging in Equation 45 in the above equation and using i = h (i−1)d M + 1, id M i , Evi;D ∇ˆfi(wi, vi; D) 2 = E ||∇fi(wi)||2 + (3d + K −1) (f M)2K X m∈f Mi ||∇Fm(wi)||2 X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (47) 39 Since ∇ˆfi, ∀f M ⊂M are independent of each other, we can compute the norm of ∇ˆf as follows, Ev,D||∇ˆf(w, v; D)||2 = E||∇f(w)||2 + 3d + K −1 (f M)2K ! X m∈M ∇Fm(wm) 2 X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (48) = E||∇f(w)||2 + 3d + K −1 (f M)2K ! X m∈M ∇Fm(wm) −∇f(wm) + ∇f(wm) 2 X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (49) = E||∇f(w)||2 + 2(3d + K −1) (f M)2K ! X m∈M ∇Fm(wm) −∇f(wm) 2 X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + 2(3d + K −1) (f M)2K ! X m∈M ∇f(wm) 2 X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (50) Using Assumption I.2 and limited participation rate of s, Ev,D||∇ˆf(w, v; D)||2 = E||∇f(w)||2 + 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + \u00122(3d + K −1) f MK \u0013 E ||∇f(w)||2 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (51) Rearranging the terms, we get, Ev,D||∇ˆf(w, v; D)||2 = E||∇f(w)||2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   + 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (52) I.4 Convergence Rate of SPRY The general template for the convergence analysis of SPRY is similar to FEDADAM, hence we will follow Theorem 2 of AFO [25]. Our aim here is to highlight the differences in treatment of our gradient estimator ∇ˆfm ∀m ∈[M], and the aggregate global gradient ∇f as shown in Equation 9. Theorem I.6. Under the assumptions on L-smoothness (Asmp I.1), bounded global variance σ2 g of accumulated gradients (Asmp I.2), and bound on gradient magnitude G (Asmp I.3) and the following conditions on the local learning rate ηℓ, ηℓ= min ( O \u0012 τ 2 √β2ηGL \u0013 1 2 , O \u0012 1 √β2G \u0013 , O \u0012 τ 3 √β2 √1 −β2G2 \u0013 1 2 , O f MK β2G(3d + K −1) P m∈[M] P c∈[C] α2m,c ! ) ; (53) SPRY satisfies the following bound, min 0≤r≤R Er||∇f(w(r))||2 ≤f(w(0)) −ER[f(w(R))] ηR + \u0012 2 + ηηℓL 2τ 2 + √1 −β2Gηℓ τ 3 \u0013 σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] α2 m,c, (54) 40 where R is the total round count, w ∈Rd are the trainable weights, v ∈Rd are the random perturbations, K is the count of random perturbations per batch, η is the global learning rate, τ is adaptability hyperparameter, s is client sampling rate. The rest of the symbols are defined in Theorem I.4. Proof. As shown in Equation 7, an update of the model weights w at server-side in SPRY looks like, w(r+1) = w(r) + η ∆(r) √ Λ(r) + τ (55) Using Assumption I.1 and then the server-side update rule, we have f(w(r+1)) ≤f(w(r)) + D ∇f(w(r)), w(r+1) −w(r)E + L 2 ||w(r+1) −w(r)||2 (56) = f(w(r)) + η ∇f(w(r)), ∆(r) √ Λ(r) + τ + η2L 2 X i∈[d] (∆(r) i )2 \u0012q Λ(r) i + τ \u00132 (57) Taking expectation over randomness of round r and simplifying the terms, Er[f(w(r+1))] ≤f(w(r)) + η * ∇f(w(r)), Er \" ∆(r) p βΛ(r−1) + τ #+ | {z } R1 +η2L 2 X i∈[d] Er   (∆(r) i )2 ( q Λ(r) i + τ)2   + η * ∇f(w(r)), Er \" ∆(r) √ Λ(r) + τ − ∆(r) p βΛ(r−1) + τ #+ | {z } R2 (58) Bounds for R2 are derived in the exactly same manner as “Bounding R2” in Theorem 2 of FEDADAM [25], R2 ≤ p 1 −β2Er d X j=1 G τ ×   (∆(r) j )2 q Λ(r) j + τ   (59) Bounding R1 has a different treatment due to the distinct aggregation strategy of SPRY: R1 = * ∇f(w(r)), Er \" ∆(r) p βΛ(r−1) + τ #+ (60) Since ∆(r) is piece-wise made of aggregations of forward gradients for several parts of the model weights, as shown in Equation 12, we first center each gradient piece for the computation of the squared norm, ∴R1 = * ∇f(w(r)), Er \" −ηℓ∇ˆf(w(r), v(r), D) p βΛ(r−1) + τ #+ (61) Using ab ≤(a2 + b2)/2, R1 ≤−ηℓ 2 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ + ηℓ 2 Er||∇ˆf(w(r), vr, D)||2 (62) Using the result of Lemma I.5, ∴R1 ≤−ηℓ 2 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ + ηℓ 2 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + ηℓ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132  Er||∇f(w(r))||2 (63) 41 Putting R1 and R2 bounds in Equation 58, Er[f(w(r+1))] ≤f(w(r)) −ηηℓ 2 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ + ηηℓ 2 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + ηηℓ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132  Er||∇f(w(r))||2 + η2L 2 X i∈[d] Er \" (∆(r) i )2 Λ(r) i + τ 2 # + η√1 −β2G τ X i∈[d] Er   (∆(r) i )2 q Λ(r) i + τ   (64) Summing over r = 0 to R −1 and using telescoping sum, we get ER[f(w(R))] ≤f(w(0)) −ηηℓ 2 R−1 X r=0 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ + ηηℓR 2 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + ηηℓ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   R−1 X r=0 Er||∇f(w(r))||2 + \u0012η2L 2 + η√1 −β2G τ \u0013 R−1 X r=0 X i∈[d] Er \" (∆(r) i )2 Λ(r) i + τ 2 # | {z } R4 (65) Bounding R4 follows a similar derivation as “Bounding R1”, R4 = E R−1 X r=0 X i∈[d] (∆(r) i )2 Λ(r) i + τ 2 = E R−1 X r=0 X i∈[d] [−ηℓ∇ˆf(w(r), v(r), D)]2 i Λ(r) i + τ 2 (66) ≤η2 ℓE R−1 X r=0 ∇ˆf(w(r), v(r), D) τ 2 2 (67) Using Lemma I.5 once again, ∴R4 ≤η2 ℓ τ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132  E R−1 X r=0 ||∇f(w(r))||2 + η2 ℓ τ 2 2σ2 g(1 −s)R(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (68) 42 Updating Equation 65 with the bounds of R4, ER[f(w(R))] ≤f(w(0)) −ηηℓ 2 R−1 X r=0 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ + ηηℓR 2 2σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 + ηηℓ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   R−1 X r=0 Er||∇f(w(r))||2 + \u0012η2L 2 + η√1 −β2G τ \u0013 η2 ℓ τ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   R−1 X r=0 Er||∇f(w(r))||2 + \u0012η2L 2 + η√1 −β2G τ \u0013 η2 ℓ τ 2 2σ2 g(1 −s)R(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (69) Rearranging the terms, R−1 X r=0 X j∈[d] [∇f(w(r))]2 q βΛ(r−1) j + τ ≤f(w(0)) −ER[f(w(R))] η + 2Rσ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 +  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   R−1 X r=0 Er||∇f(w(r))||2 + \u0012 ηL + 2√1 −β2G τ \u0013 ηℓ τ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   R−1 X r=0 Er||∇f(w(r))||2 + \u0012ηL 2 + √1 −β2G τ \u0013 ηℓ τ 2 Rσ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (70) Getting a lower bound for the left hand side term through using the fact √ Λ(r−1) ≤ηℓKG from Theorem 2 of AFO [25], R−1 X r=0 d X j=1 Er[∇f(w(r))]2 j q β2Λ(r−1) j + τ ≥ R−1 X r=0 d X j=1 Er[∇f(w(r))]2 j √β2ηℓKG + τ ≥ R √β2ηℓKG + τ min 0≤r≤R Er||∇f(w(r))||2 (71) 43 ∴ R √β2ηℓKG + τ min 0≤r≤R Er||∇f(w(r))||2 ≤f(w(0)) −ER[f(w(R))] η + \u0012 2 + ηηℓL 2τ 2 + √1 −β2Gηℓ τ 3 \u0013 Rσ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 +  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132  E R−1 X r=0 ||∇f(w(r))||2 + \u0012 ηL + 2√1 −β2G τ \u0013 ηℓ τ 2  1 + \u00122(3d + K −1) f MK \u0013 X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132  E R−1 X r=0 ||∇f(w(r))||2 (72) Considering the coefficients of E||∇f(w(r))||2 terms, conditioning on the following inequality, R √β2ηℓKG + τ ≥ \u0012ηηℓL τ 2 + 2ηℓ √1 −β2G τ 3 + 1 \u0013  1 + 2(3d + K −1) f MK X m∈[M] X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132   (73) We get the following condition on the local learning rate, ηℓ= min ( O \u0012 τ 2 √β2ηGL \u0013 1 2 , O \u0012 1 √β2G \u0013 , O \u0012 τ 3 √β2 √1 −β2G2 \u0013 1 2 , O    f MK β2G(3d + K −1) P m∈[M] P c∈[C] \u0010 nc |D| −nm,cαc |Dm| \u00112    ) (74) for the following bound on the gradient norm, min 0≤r≤R Er||∇f(w(r))||2 ≤f(w(0)) −ER[f(w(R))] ηR + \u0012 2 + ηηℓL 2τ 2 + √1 −β2Gηℓ τ 3 \u0013 σ2 g(1 −s)(3d + K −1) f MK ! X m∈M X c∈[C] \u0012 nc |D| −nm,cαc |Dm| \u00132 (75) 44 NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The claims on (a) SPRY consuming significantly less memory than its backpropagation-based counterparts, (b) SPRY achieving higher accuracy than its zero- order counterparts, and comparable accuracy to the backpropagation-based counterparts, (c) SPRY running faster in terms of wallclock time to convergence than its zero-order counterparts are proved in Section 5.2, 5.1, and 5.3 respectively. The theoretical claims are discussed and proved in Section 4 and Appendix I. The claims on direct application of Forward-mode AD in an FL setup being inefficient and inaccurate are supported by experiments shown in Appendix G. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations and potential paths for future work are discussed in Appendix C. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 45 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions and proofs are listed in detail in Appendix I. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The details about implementation and experimental setup are given at the onset of Section 5. The pseudocode is in Appendix E, and each part of SPRY is explained in Section 3. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. 46 In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The source code is currently available as supplementary material accompanying this work and an anonymized version is also available here. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the"
            },
            {
                "section": "results",
                "content": "? Answer: [Yes] Justification: The experimental setting is described in Section 5. Full details about datasets, their splits, and hyperparameters are given in Appendix B. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Experimental variance related to the main results in the paper, across 3 differently seeded runs is given in Appendix H.3. Guidelines: • The answer NA means that the paper does not include experiments. 47 • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute resources are discussed at the onset of Section 5. Details on memory consumption are given in Section 5.2. Time to convergence is discussed in Section 5.3. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work does not involve human subjects. All the experiments are done on open-source datasets, which are widely used in the natural language community. Moreover, the goal of SPRY is to enable crowd-sourcing compute resources to finetune LLMs, without making private data accessible to a third party. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? 48 Answer: [Yes] Justification: The positive and negative broader impact is discussed in Appendix D. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We have not created any new models or new data. Our main goal is to finetune the existing pre-trained language models. The datasets we have used are open-source, and free-to-use. The license information of datasets is given in Appendix B. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Licenses for all the datasets are stated in Appendix B. Our work is within the usage bounds of each of the datasets. Dataset URLs are also provided under the references section. Guidelines: 49 • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The source code (also available as a supplemental material) of SPRY comes with a detailed README.md, explaining how to run the presented experiments of our algo- rithm and its counterpart baselines. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing and research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? 50 Answer: [NA] Justification: Our work does not involve research with or on human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 51"
            }
        ]
    },
    "Paper_11": {
        "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey Zeyu Han1, Chao Gao2, Jinyang Liu1, Jeff (Jun) Zhang3, and Sai Qian Zhang*4 2University of California, Riverside",
        "sections": [
            {
                "section": "Abstract",
                "content": "—Large models represent a groundbreaking advance- ment in multiple application fields, enabling remarkable achieve- ments across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particu- larly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computa- tional overhead. Moreover, we provide an overview of applica- tions developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT al- gorithm and its system implementation, offering detailed insights into recent advancements and practical applications. Index Terms—Large Language Model, Parameter-Efficient Fine-tuning, Computer System, Distributed System. I. INTRODUCTION Large Models (LMs) have recently captured considerable public interest. Their ability to understand context and nuances enables them to proficiently handle diverse tasks across mul- tiple domains, including natural language processing (NLP), computer vision (CV), etc. In the field of NLP, Large Lan- guage Models (LLMs) have achieved significant advance- ments across various tasks including text generation [1], [2], translation [3], [4], personalized chat-bots [5], [6], [7], and summarization [8], demonstrating remarkable proficiency. * Corresponding author Earlier studies [1] have suggested that LLMs exhibit high levels of generalization, enabling them to apply their acquired knowledge to new tasks not included in their original training. This capability is commonly known as zero-shot learning. Nevertheless, fine-tuning remains essential to further enhance LLMs for optimal performance on new user datasets and tasks. Due to its scale, a widely adopted strategy for fine-tuning LLMs involves adjusting a limited number of LLM parame- ters while keeping the remainder unchanged. This technique, termed Parameter-Efficient-Fine-Tuning (PEFT), involves se- lectively adjusting a small proportion of their parameters while keeping the rest unaltered. Furthermore, the application of PEFT extends beyond the realm of NLP and quickly attracts interest in the CV community for handling fine-tuning vision models with large parameters, such as Vision Transformers (ViT) and diffusion models, as well as disciplinary models such as vision-language models. In this survey, we systematically review and categorize recent advancements in PEFT algorithms as well as the system implementation costs associated with various PEFT algorithms across diverse scenarios. Figure 1 presents the overview con- tent for this survey. In section II, we present some fundamental concepts for LLM and PEFT, including computational flow for LLM, basic knowledge of PEFT, commonly used datasets and tasks, and evaluation benchmarks. We categorize all types of PEFT algorithms in Section III according to their computational flow. In Section III-A, we detail additive algo- rithms that either introduce new weight parameters or modify activations. Algorithms that only require fine-tuning of existing parameters are categorized as selective approaches, which are introduced in Section III-B. In Section III-C, we explore reparameterized PEFT, which constructs a (low- dimensional) reparameterization of original model parameters for training while transforming the weights back to maintain the inference speed. Additionally, there exist algorithms that combine the above techniques, and we have classified these as hybrid approaches, elaborating on them in Section III-D. We also investigate strategies for further reducing the computational complexity of different PEFT algorithms, including KV-cache management, pruning, quantization, and memory optimization, in Section IV. In Section V, we expand the scope of this survey beyond the computational perspective to involve various potential application scenarios. Specifically, we explore innovations that arXiv:2403.14608v7  [cs.LG]  16 Sep 2024 2 Background Computational flow for LLM PEFT Taxonomy Selective PEFT Additive PEFT System Design Challenge System Design for PEFT Centralized PEFT Serving System PEFT for LLMs Distributed PEFT Training System Hybrid PEFT PEFT overview Reparameterized PEFT Efficient PEFT Design KV-cache Management for PEFT Efficiency PEFT Pruning PEFT Quantization Memory-efficient PEFT Parallel PEFT Training System Apply PEFT for other Applications PEFT for ViTs PEFT for VLAs PEFT for Diffusion Models Downstream tasks Section 2 Section 3 Section 4 Section 5 Section 6 2.1 2.2 2.3 3.1 3.2 3.3 3.4 4.1 4.2 4.3 4.4 5.1 5.2 5.3 5.4 6.1 6.2 6.3 6.4 Fig. 1: A content overview covered in the survey. applying PEFT techniques to different model architecture, including LLMs (Section V-A), Vision Transformer (Sec- tion V-B), Vision-Language alignment models (Section V-C), and Diffusion models (Section V-D), for varied downstream tasks, underscoring PEFT’s versatility and applicability in a range of scenarios. After that, in Section VI, we explore the system design challenge for PEFT methods. The discussion includes three advanced system solutions for practical PEFT deployment: PEFT query serving (Section VI-B), distributed tuning (Section VI-C), and concurrent PEFT tuning (Sec- tion VI-D). Finally, in Section VII, we summarize our survey and propose several potential future directions from both algorithmic and systemic perspectives, aiming to offer valuable insights for further research and development in the field. II. BACKGROUND In this section, we first discussed the computation flow of LLM, including its fundamental components, computational complexity, and the flow of computations it involves as a case study. We then provide a brief overview of different PEFT algorithms in section II-B. A. Computation flow for LLaMA In order to gain a deeper understanding of LLM and other Transformer-based models, we employ LLaMA-7B, a cutting- edge open-source LLM model, to scrutinize the architecture of LLM as well as Transformer. As shown in Figure 2 (a), LLaMA consists of three major components: an embedding block, a stack of decoder blocks, and a head block which consists of linear and softmax layer. The embedding layer’s primary role is to transform unstructured textual information, into chunks of discrete numerical vectors (tokens) to facilitate subsequent processing. The embedded tokens are then deliv- ered to the decoder layers for further processing. Each LLaMA decoder is composed of two fundamental components: Multi- head Self-Attention (MSA) and Feedforward Network (FFN). In the MSA module, each of the tokens will be clustered by an attention map obtained by a dot production between two linear mappings of the input tokens. Then the grouped tokens will be further processed by a Feedforward Neural network. Additionally, Root Mean Square Layer Normalization (RM- SNorm) [9] is adopted in LLaMA as a replacement for Layer Normalization to ensure efficient training. LLM distinguishes itself from other deep neural network (DNN) models such as convolutional neural networks (CNN) in two significant ways. Firstly, LLM exhibits an inherent autoregressive nature, necessitating multiple iterations to com- plete the generation task. Moreover, LLM incorporates an attention mechanism, a component with computational com- plexity that scales quadratically with the length of the inputs. On the other hand, the inherent computation characteristic of LLM lies in the attention blocks inside each decoder layer. Figure 2 (c) depicts the high-level overview of the computation flow in the attention block. During the inference process, each decoder takes a three- dimensional tensor x P Rbˆlˆd as the input tokens. The input tokens are first multiplied with three weight matrices WQ, WK, and WV , producing the output referred to as query(Q), key(K) and value(V ). Given the MSA module’s inability to recognize positional data and the inherent auto- regressive nature of LLMs, the query and key will undergo a process using Rotary Positional Embedding [10] (RoPE, denoted as Rp.q in Eq 1) to encode the position information. Subsequently, the key and value will be combined with prior tokens. After the positional embedding, the intermediate activation will then undergo a series of multiplication, softmax, and residual addition to generate MSA output as described in Eq 9. To be noted here, dk in the equation refers to the number of feature dimensions in the multi-head attention mechanism. Q, K, V “ RpWqxq, RpWkxq, Wvx (1) SApxq “ Softmaxp QKT ?dhead qV (2) MSApxq “ rSA1pxq; SA2pxq; . . . ; SAkpxqsWo (3) The SA output will then be forwarded to the FFN blocks for further processing. The FFN block will have another three 3 <BOS> FFN SA LLaMA ML ML is LLaMA LLaMA is awesome LLaMA awesome <EOS> Decoder Decoder Decoder Linear & Softmax … Q K V LoRA FC FC ReLU Adapter Decoder SA FFN Wdown Wup Input tokens Prompt (c) Embedding (b) (a) Fig. 2: (a) LLaMA architecture. (b) LLaMA auto-regressive pattern. (c) Three common PEFT operations. All the learnable components are highlighted in red, while the frozen components are highlighted in grey. LoRA is applied on all the Query, Key, and Value blocks. The adapter targets the FFN module. Soft-Prompt focused on tuning the input activation of each decoder. We only show one decoder for illustration simplicity. matrices Wup, Wdown, and Wgate and the computation can be illustrated by: FFNLLaMapxq “ WuppSiLUpWgatexq d pWdownxqq ` x, (4) where x denotes the input of the FFN layer, and SiLU is the nonlinear function used in LLaMA. In the original Transformer, the FFN block can be demonstrated by: FFNT ransfomerpxq “ WuppReLUpWdownxqq ` x. (5) The output of the last decoder layer will be sent to a linear layer, which then generates a probability distribution spanning the complete vocabulary to predict the next token in the sequence. The produced token will then be concatenated with the previous tokens and used as the input for the next round of processing. This generating process repeats in an auto-regressive manner until a full sequence of tokens, referred to as a completion, is produced (Figure 2 (b)). For training, the computation flow is similar to that for inference, except that the generated sentences are directly compared to the ground truth output and generate the training loss. Gradients will then be computed across the LLM weights to minimize this training loss. To analyze the computation cost and memory overhead in LLM, we also set a series of parameters used in later section III. Table I shows the parameter size and computation dimension in the LLaMA-7B model as a starting example. LLM models generate tokens (words) one for each round, depicted in Fig 2, based on the previous prompt (input) and previously generated sequence. This process will be repeated until the model outputs hits and termination token. To accel- erate the inference process in LLM models, people take the strategy of storing the previous Keys and Values in the Key- Value cache (KV-cache), so they don’t need to recalculate them for each new token. Mathematically, we can represent the total decoders’ KV-cache memory cost in equation 6. In the equation, l and b are the context length and batch size and L refers to the number of layers. The dhead is the head dimension and nhead is the number of heads. Size “ L ˆ 2 ˆ b ˆ l ˆ dhead ˆ nhead (6) B. Overview on Parameter Efficient Fine Tuning Fine-tuning remains essential to enhance LLM performance on unseen user datasets and tasks. With the size of the model growing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard full fine-tuning paradigm requires thousands of GPU work in parallel, which is highly inefficient and unsustainable. A type of algorithm has been raised namely Parameter-efficient fine-tuning (PEFT) which aims to tune minimal parameters to achieve better performance over full tuning on downstream tasks. In parallel developments, large-scale pre-trained models in vision and multimodal domains have also demonstrated their effective representational learning capabilities, enabling adap- tation from large datasets to smaller ones or across various data modalities through fine-tuning. Consequently, this capability has made PEFT increasingly attractive to the wider research community. We categorized the PEFT algorithms into additive, selec- tive, reparameterized, and hybrid fine-tuning based on their operations. As Figure 3 depicts, three major additive fine- tuning algorithms are normally used: (1) Adapter; (2) Soft Prompt; (3) Others. They differ from each other in terms of the different additional tunable modules or parameters. Selective fine-tuning, on the other hand, doesn’t require any additional parameters, it selects a small subset of parameters from the backbone model and only makes them tunable while keeping the majority of parameters untouched during fine-tuning on downstream tasks. We categorized selective fine-tuning based 4 TABLE I: Configuration parameters and computation operation for LLaMA-7B architecture Operation Weights Symbol Weights Dimension Input Tensor Dimension Complexity Eq. 1 WQ, WK, WV d ˆ k ˆ d k b ˆ l ˆ d Oplq Eq. 2 - - b ˆ l ˆ 3 ˆ k ˆ d k Opl2q Eq. 3 Wo d ˆ d b ˆ l ˆ d Oplq Eq. 4 Wup, Wdown, Wgate d ˆ 4d b ˆ l ˆ d OR l ˆ b ˆ 4d Oplq on the grouping of chosen parameters: (1) Unstructural Mask- ing; and (2) Structural Masking. Reparametrization represents transforming model parameters between two equivalent forms. Specifically, reparametrized fine-tuning introduces addi- tional low-rank trainable parameters during training, which are then integrated with the original model for inference. This approach is categorized into two main strategies: (1) Low- rank Decomposition, and (2) LoRA Derivatives. Hybrid fine- tuning explores the design spaces of different PEFT methods and combines their advantages. C. Downstream Tasks for LLM Evaluation Two types of tasks have been widely used for LLM eval- uation, the first type is the General Language Understand- ing Evaluation (GLUE) [11] benchmark, which integrates nine sentence or sentence-pair language understanding tasks (CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and WNLI), chosen for their diversity in dataset sizes, text genres, and difficulty levels, and is based on established existing datasets. It also includes a diagnostic dataset specifically designed to evaluate and analyze model performance across various linguistic phenomena inherent in natural language. Additionally, it features a public leaderboard to track perfor- mance on the benchmark and a dashboard to visualize model performance on the diagnostic set. The other type of dataset that has been used in recent LLM papers is common sense reasoning which integrated into our study caters to a variety of research facets: (1) OpenBookQA [12] is curated to foster research in advanced question-answering, delving into a profound understanding of both the subject matter and the language in which it is articulated. (2) PIQA [13] primarily emphasizes everyday scenarios, demonstrating a predilection for unconventional solutions. (3) Social IQA [14] emerges as a novel question- answering benchmark tailored for gauging social common- sense intelligence. (4) HellaSwag [15] serves as a dataset, the essence of which is to ascertain the capability of machines in aptly concluding sentences. (5) BoolQ [16] is a dataset dedi- cated to question-answering, particularly for binary responses (yes/no queries). (6) WinoGrande [17] is introduced as a fresh compilation, encompassing a substantial 44,000 problems. (7) ARC-easy [18] presents itself as a novel dataset constituting genuine grade-school level multiple-choice science questions, designed to invigorate research in intricate question-answering. (8) ARC-challenges [18], distinctively, encompasses solely those questions that were inaccurately addressed by both a retrieval-based algorithm and a word co-occurrence algorithm. Image recognition is the primary benchmark and application for vision models, exemplified by benchmarks such as fine- grained visual categorization (FGVC) and visual task adapta- tion benchmark (VTAB). Beyond image classification, video action recognition is another key application area, involving datasets like Kinetics-400 [19], SSv2 [20], and HMDB51 [21]. Additionally, PEFT has been utilized for dense prediction tasks, using datasets like MSCOCO [22], ADE20K [23], and PASCAL VOC [24]. D. Evaluation Benchmarks for PEFT To help readers evaluate the performance differences be- tween various PEFT methods under a unified standard, a com- prehensive benchmark is essential. Next, we discuss several commonly used benchmarks. From the algorithmic perspective, [25] benchmarks the performance of several PEFT algorithms across more than 100 NLP tasks and conducts systematic experiments based on criteria such as performance, convergence, efficiency, combin- ability, scalability, and transferability. Similarly, [26] and [27] have also established targeted benchmarks to evaluate different PEFT algorithms. From the system perspective, three commonly used bench- marks are outlined below to evaluate system performance. The first benchmark is the ShareGPT dataset [28], which includes real-world interactions with OpenAI’s ChatGPT. It encompasses a broad spectrum of conversational queries and responses that are representative of typical user interactions with large language models (LLMs). This dataset is vital for evaluating the system’s ability to manage diverse and realistic conversational requirements, focusing on the accuracy of responses and efficiency in handling requests. The second benchmark involves the Microsoft Azure Func- tion Trace from the years 2019 and 2021 [29], containing logs from serverless computing activities via Azure Functions. While these logs are from a general serverless computing con- text rather than LLM-specific applications, they offer insights into the computational demands driven by events. These traces simulate the arrival patterns and workload intensities that LLM systems might face, including irregular and peak demands, thus acting as practical proxies for LLM inference tasks. The third benchmark is based on the Gamma process [30], a prevalent approach in simulations to model the timing of incoming requests in queueing and service systems. This method facilitates the creation of workloads with varied arrival rates and patterns, producing synthetic, yet realistic request 5 PEFT Methods for PLMs Additive Fine-tuning Adapter-based Fine-tuning Adapter Design Serial Adapter [31], Parallel Adapter [32], CIAT [33], CoDA [34] Multi-task Adaptation AdapterFusion [35], AdaMix [36], PHA [37], AdapterSoup [38], MerA [39], Hyperformer [40] Soft Prompt-based Fine-tuning Soft Prompt Design Prefix-tuning [41], Prefix-Propagation [42], p-tuning v2 [43], APT [44], p-tuning [45], prompt-tuning [46], Xprompt [47], IDPG [48], LPT [49], SPT [50], APrompt [51] Training Speedup SPoT [52], TPT [53], InfoPrompt [54], PTP [55], IPT [56], SMoP [57], DePT [58] Others (IA)3 [59], MoV [60], SSF [61], IPA [62] Selective Fine-tuning Unstructural Masking U-Diff pruning [63], U-BitFit [64], PaFi [65], FishMask [66], Fish-Dip [67], LT-SFT [68], SAM [69], Child-tuning [70] Structural Masking S-Diff pruning [63], S-BitFit [64], FAR [71], Bitfit [72], Xattn Tuning [73], SPT [74] Reparameterized Fine-tuning Low-rank Decomposition Intrinsic SAID [75], LoRA [76], Compacter [77], KronA [78], KAdaptation [79], HiWi [65], VeRA [80], DoRA [81] LoRA Derivatives Dynamic Rank DyLoRA [82], AdaLoRA [83], SoRA [84], CapaBoost [85], AutoLoRA [86] LoRA Improvement Laplace-LoRA [87], LoRA Dropout [88], PeriodicLoRA [89], LoRA+ [90], MoSLoRA [91] Multiple LoRA LoRAHub [92], MOELoRA [93], MoLORA [60], MoA [94], MoLE [95], MixLoRA [96] Hybrid Fine-tuning UniPELT [97], S4 [98], MAM Adapter [32], NOAH [99], AUTOPEFT [100], LLM-Adapters [101], S3PET [102] Fig. 3: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Large Models. Output Combine Frozen Learnable Input Output Input (c) Reparameterization PEFT (a) Additive PEFT (b) Selective PEFT Merge Output Input Input (train) Fig. 4: Different types of PEFT algorithms. scenarios that a system could encounter during actual opera- tions. Such synthetic workloads are crucial for testing system performance under controlled conditions that resemble real- world user activity. III. PEFT TAXONOMY The PEFT strategies can be broadly classified into four categories: additive PEFT (Section III-A), which modifies the model architecture by injecting new trainable modules or parameters; selective PEFT (Section III-B), which makes a subset of parameters trainable during fine-tuning; repa- rameterized PEFT (Section III-C), which constructs a (low- dimensional) reparameterization of the original model param- eters for training, then equivalently transforms it back for inference; and hybrid PEFT (Section III-D), which combines advantages from different PEFT methods to build a unified PEFT model. An overview of different types of PEFT algo- rithms is depicted in Figure 4. A. Additive PEFT Standard full fine-tuning entails substantial computational expenses and could also potentially harm the model’s gener- alization ability. To mitigate this problem, a widely employed approach is to maintain the pre-trained backbone unchanged and introduce only a minimal number of trainable parameters that are strategically positioned within the model architecture. While fine-tuning for a specific downstream task, only the weights of these additional modules or parameters are updated, which results in a substantial reduction in storage, memory, and computational resource requirements. Due to their char- acteristic of adding parameters, these techniques can be termed as Additive Tuning, as shown in Figure 4 (a). Next, we discuss several popular Additive PEFT algorithms. 1) Adapters: Adapter approaches involve the insertion of small adapter layers within Transformer blocks. Typically, an adapter layer consists of a down-projection matrix Wdown P Rrˆd, followed by a non-linear activation function σp¨q, and an up-projection matrix Wup P Rdˆr. In this context, d represents the dimension of the hidden layer, and r serves as the bottleneck dimension, which is a hyperparameter used in configuring the adapters. Denote hin as the input to the adapter, the computation within the adapter module (with residual) can be summarized as follows: Adapterpxq “ WupσpWdownxq ` x. (7) The concept of adapters in the field of NLP was initially introduced by Serial Adapter [31] as shown in Figure 5 (a). In their approach, each Transformer block is enhanced by adding two adapter modules, with one positioned after the self-attention layer and the other after the FFN layer, respectively. Subsequent research has aimed to address the additional computational cost associated with adapter layers. A modified framework AdapterFusion [35] was proposed, where adapter layers are inserted only after the ’Add & Norm’ step following the FFN layer to enhance the computational efficiency. The adapters mentioned above follow a sequen- tial design, placing adapter layers as bottlenecks within the Transformer blocks. This approach may potentially reduce the model’s parallelism and require a trade-off between inference efficiency and accuracy. In contrast, [32] introduced a parallel 6 Transformer Module Adapter Transformer Module (a) Serial Adapter (b) Parallel Adapter Transformer Module (c) CoDA all tokens all tokens top-k tokens all tokens (d) Adapter Layer Wdown Wup ReLU Adapter Adapter Fig. 5: Illustration of three representative adapter-based fine-tuning algorithms. Blue represents frozen, while yellow represents trainable. adapter (PA) approach as depicted in Figure 5 (b), which reorganizes the traditionally sequential adapter layers into a parallel side-network that runs alongside each Transformer sublayer. Similarly, CIAT [33], CoDA [34] and KronA [78] also adopts a parallel adapter design. Except for the parallel design, CoDA employs a sparse activation mechanism to improve the inference efficiency as shown in Figure 5 (c). Specifically, CoDA uses a soft top-k selection process that identifies k important tokens in each layer, which will be processed by both the frozen pre-trained Transformer layer and the adapter branch to maintain model accuracy. In contrast, those unimportant tokens are only processed by the adapter branch while skipping the heavy pre-trained layer, therefore optimizing for inference efficiency without compromising overall performance. To enhance the performance and generalization of adapters, various studies have implemented multi-task learning strate- gies, such as AdapterFusion [35], AdaMix [36], PHA [37], AdapterSoup [38], MerA [39], and Hyperformer [40]. AdapterFusion keeps all pre-trained adapters in the model and employs a fusion module to merge the multi-task in- formation. Unlike AdapterFusion, MerA merges pretrained adapters into a single one through optimal transport based on weights and activations. This approach avoids introduc- ing any additional trainable parameters, thereby enhancing computational efficiency. Hyperformer stores the multi-task information in a shared hypernetwork, which generates task and layer-specific adapter parameters conditioned on task and layer ID embeddings. Given a new task, only an additional task embedding needs to be learned, therefore reducing the number of trained parameters. 2) Soft Prompt: Alternatively, prompt tuning presents an additional approach for refining the model to achieve improved performance through fine-tuning. Instead of optimizing dis- crete token representations through in-context learning, there is a prevailing belief that the continuous embedding space of soft prompts inherently contains more information [103]. Drawing inspiration from this concept, researchers directly prepend adjustable vectors, referred to as soft prompts, to the start of the input sequence. This can be represented as follows: Xplq “ rsplq 1 , . . . , splq NS, xplq 1 , . . . , xplq NXs (8) where Xplq is the sequence of input tokens for layer l, including soft prompt tokens splq i followed by the original input tokens xplq i . NS is the number of soft prompt tokens, and NX is the number of original input tokens. Prefix-tuning [41] introduces learnable vectors that are prepended to keys k and values v across all Transformer layers. To ensure stability during the optimization process, Prefix- tuning adopts a reparameterization strategy, which utilizes an MLP layer to generate these prefix vectors rather than optimizing them directly. After fine-tuning, only the prefix vectors are saved for inference. This technique has been adapted and improved in several studies [42], [43], [44]. For instance, p-tuning v2 [43] removes reparameterization and expands its usage to broader model scales and NLP tasks. APT (Adaptive Prefix Tuning) [44] enhances Prefix-tuning by introducing an adaptive gate mechanism to control the prefix importance in each layer. Concurrent work p-tuning [45] and prompt-tuning [46] apply learnable vectors only at the initial word embedding layer rather than all layers to enhance training and inference efficiency. It’s important to highlight that prompt-tuning demonstrates its effectiveness primarily in the context of large models, specifically those with over 11 billion parameters [46]. Complementing this, Xprompt [47] eliminates the negative prompt tokens through a hierarchi- cally structured pruning, which closes the performance gap at smaller model scales. [104] provides some theoretical analysis towards prompt tuning, demonstrating its universality and limitations in limited-depth Transformers. IDPG (Instance- Dependent Prompt Generation) [48] improves prompt tuning by generating prompts based on each input sentence with a lightweight prompt generator. In a related approach, LPT (Late Prompt Tuning) [49] also leverages a prompt generator to obtain instance-aware prompt. Unlike previous work, LPT adds these prompts only after an intermediate layer, rather than at the initial or all layers. This strategic placement eliminates the gradient calculation below the intermediate layer, thereby significantly accelerating the training speed. Simultaneously, LPT can improve the overall performance due to the shorter backpropagation path preserves more task-related information. Inspired by LPT, SPT (Selective Prompt Tuning) [50] delves deeper into the importance of prompt inserting strategies. It introduces a learnable probabilistic gate in each layer to determine whether to use the prompt propagated from the pre- vious layer or inject a newly generated prompt. APrompt [51] employs another prompt inserting strategy. In addition to input prompts inserted at the beginning of the input sequence for each Transformer layer, APrompt also prepends additional learnable prompts to the respective query, key, and value 7 V K Q ⊙ lk lv lff ⊙ softmax Wdown 𝛔 ⊙ Wup (a) (IA)3 ⊙ ⊕ Operation 1 Operation 2 (b) SSF scale shift Fig. 6: Illustration of (IA)3 and SSF. Blue represents frozen, while yellow represents trainable. matrices in the self-attention blocks to learn new attention patterns. Besides, APrompt incorporates the learning of a task- specific head. The concept of soft prompts has been employed for various downstream tasks [105], [106], although their training can be prone to instability and slow convergence. To address this, SPoT [52] uses a source prompt learned from one or multiple tasks to initialize prompts for new tasks. Similarly, the transfer of soft prompts from one task to initialize another is proposed in TPT (transferable prompt tuning) [53], which demonstrates that a better prompt initialization results in a large training convergence speedup. InfoPrompt [54] develops two mutual information-based loss functions, i.e., head loss and representation loss, to find better prompt initialization and learn sufficient task-relevant information, thereby also expediting convergence. PTP [55] delves into the root causes of training instability. It identifies the steep nature of the loss landscape in conventional prompt tuning, where minor varia- tions in input data can lead to significant loss fluctuations. To mitigate this, PTP introduces perturbation-based regularizers to smooth the loss landscape and consequently stabilize the training process. DePT [58] decomposes the soft prompt into a shorter soft prompt with a pair of low-rank matrices, which are optimized with two distinct learning rates. This strategy not only improves performance but also enhances training and inference efficiency. SMoP (Sparse Mixture-of-Prompts) [57] reduce the training and inference cost by utilizing short soft prompts. During training, multiple short soft prompts are trained, each tailored to specific subsets of the dataset. During inference, SMoP integrates a gating mechanism that routes each input instance to an appropriate short prompt. This technique not only increases efficiency in both training and inference stages but also retains performance comparable to those achieved with longer soft prompts. To further cut down the number of soft prompt parameters, IPT (Intrinsic Prompt Tuning) [56] identifies an intrinsic task subspace by training an auto-encoder on multiple tasks. Tuning on new tasks then requires adjusting only a few parameters within this subspace, significantly reducing the number of training parameters. 3) Other Additive Methods: Apart from the methods men- tioned above, there appear other approaches that strategi- cally incorporate additional parameters during the fine-tuning process. For example, (IA)3 [59] introduces three learnable rescaling vectors: lk P Rdk, lv P Rdv, and lff P Rdff , to rescale the key, value, and FFN activations, respectively, as depicted in Figure 6 (a). The operations within the self- attention block can be described as follows: SApxq “ SoftmaxpQplk d KT q ?dhead qpplv d V q. (9) In FFN, the rescaling can be denoted as: FFNT ransfomerpxq “ Wupplff d σpWdownxqq, (10) where d is Hadamard product. Furthermore, the scale vectors lk and lv can be seamlessly integrated into the weight matrices of AQ and AW . This integration effectively eliminates the ex- tra computational costs during inference. A similar technique SSF [61] also performs linear transformation to the model activations, as illustrated in Figure 6 (b). Specifically, after each operation (i.e., MSA, FFN, and layer normalization) in the pre-trained model, an SSF-ADA layer is injected, which performs scaling and shifting to the features generated from the operation. During fine-tuning, only those SSF-ADA layers can be updated, while during inference, similar to (IA)3, these SSF-ADA layers can be merged into model weights, so no ad- ditional inference overhead would be incurred. IPA (Inference- Time Policy Adapters) [62] offers a novel approach to align LLMs, such as GPT-4, with user-specific requirements without modifying the base model’s parameters. This is particularly significant when dealing with models whose parameters are extremely large and often not directly accessible. IPA achieves this by combining (through multiplication and normalization) the output distribution of a base LLM (base policy) with that of a smaller-sized model (adapter policy) during the decoding phase. During training, the policy adapter’s parameters are fine-tuned using reinforcement learning, while the base pol- icy’s parameters remain fixed. During inference, IPA decodes with the combined distribution of the base model and the trained policy adapter, tailoring it to fulfill specific user-defined criteria. B. Selective PEFT Rather than additive PEFT, which increases the model complexity by adding more parameters, selective PEFT fine- tunes a subset of the existing parameters to enhance model performance over downstream tasks, as depicted in Figure 4 (b). Specifically, given a model with parameters θ “ tθ1, θ2, ..., θnu where each θi denotes an individual model parameter and n represents the total count of these parameters, the process of selective PEFT is represented by applying a binary mask M “ tm1, m2, ..., mnu to these parameters. Each mi in M is either 0 or 1, indicating whether the corresponding parameter θi is selected (1) or not selected (0) for fine-tuning. The updated parameter set θ1 after fine-tuning is given by: θ1 i “ θi ´ η ¨ mi ¨ BL Bθi (11) where η represents the learning rate, and BL Bθi is the gradient of the loss function with respect to the parameter θi. In this formulation, only the parameters that are selected (i.e., mi “ 1) are updated during backpropagation. 8 (a) Unstructural Masking (b) Structural Masking Frozen Learnable Fig. 7: Illustration of two parameter masking methods. Diff pruning [63] is a representative work that applies a learnable binary mask to the model weights during fine- tuning. To achieve parameter efficiency, the mask is reg- ularized by a differentiable approximation of the L0-norm penalty. PaFi [65] simply select model parameters with the smallest absolute magnitude as trainable. FishMask [66] de- termines parameter importance using the approximate Fisher information. It then selects the top k parameters based on this information to form the mask M. Similarly, Fish-Dip [67] also uses Fisher information to calculate M, but the mask will be re-calculated dynamically in each train period. LT- SFT [68] introduces another technique to determine parameter importance inspired by the Lottery Ticket Hypothesis [107], [108], where the subset of parameters that change the most during an initial fine-tuning stage is selected to form the mask M. SAM [69] proposes a second-order approximation method, which approximates the original problem with an analytically solvable optimization function, to help decide the parameter mask. Child-tuning [70] proposes two approaches to select a child network during each training iteration, where only the parameters within this child network can be updated. However, the above unstructured parameter masking results in an uneven distribution of non-zero masks and diminished hardware efficiency when implementing PEFT. As shown in Figure 7, the structured mask organizes parameter masking in regular patterns, unlike unstructured ones that apply it randomly, thus enhancing computational and hardware effi- ciency during training. Therefore, various structured selective PEFT techniques have undergone extensive investigation. Diff pruning proposes a structured pruning strategy by partitioning the weight parameters into local groups and strategically elim- inating them together. Similarly, FAR [71] fine-tunes BERT models by grouping weights of the FFN in Transformer blocks into nodes, then ranking and selecting the learner nodes using L1 norm. To further reduce the memory access frequency, they also reconfigure the FFN by grouping the learner nodes. Bitfit [72] is proposed to only fine-tune the bias parameters of each DNN layer, and achieves competitive results for small models. However, this method fails to handle large models. [64] applies NAS to Bitfit, where S-BitFit keeps the structural nature in Bitfit that restricts NAS algorithm must choose whether δb “ 0 or not for each bias module. Similar to Bitfit that fine-tunes a specific module in Transformer, Xattn Tuning [73] fine-tunes only the cross-attention layers. SPT (sensitivity-aware visual parameter-efficient fine-tuning) [74] first identifies the sensitive parameters measured by the loss reduction when being tuned. This sensitivity is calculated using a first-order Taylor expansion, derived from a single forward and backward pass before fine-tuning in one shot. Next, SPT finds the weight matrices whose number of sensitive parameters exceeds a pre-defined threshold and then applies a selected PEFT technique (e.g., LoRA and Adapter) to these targeted weights to achieve structural tuning. C. Reparameterized PEFT Reparameterization stands for equivalently transforming a model’s architecture from one to another via transforming its parameters. In the context of PEFT, this often means constructing a low-rank parameterization to achieve the goal of parameter efficiency during training. For inference, the model can be converted to its original weight parameterization, en- suring unchanged inference speed. This procedure is depicted in Figure 4 (c). Earlier research studies [75] have shown that common pre-trained models exhibit an exceptionally low intrinsic di- mensionality. In other words, it is possible to find a low- dimensional reparameterization that is effective for fine-tuning as the entire parameter space. Intrinsic SAID [75] is the pi- oneering work in investigating the intrinsic dimension feature during the fine-tuning of LLMs. However, the most widely recognized reparameterization technique is LoRA (Low-Rank Adaptation) [76], [109], as shown in Figure 8 (a). For a given pre-trained weight matrix W0 P Rdˆk, LoRA introduces two trainable weight matrices, Wup P Rdˆr and Wdown P Rrˆk where the rank r ! minpd, kq, operating in parallel to W0. Let hin represent the input. Under normal conditions, the output through W0 is hout “ W0hin. Instead, LoRA modifies this output by introducing an incremental update ∆W that encapsulates task-specific knowledge: hout “ W0hin ` α r ∆Whin “ W0hin ` α r WupWdownhin, (12) where α denotes a scaling factor. At the onset of training, Wdown is initialized using a random Gaussian distribution, while Wup is initialized to zero, ensuring that ∆W initially holds a value of zero. LoRA is straightforward to implement and has been evaluated on models with up to 175 billion parameters. Fig 8 (c) used a single decoder as an example, the frozen and learnable components are highlighted in grey and red, respectively. Once fine-tuning is complete, LoRA’s adaptive weights seamlessly integrate with the pre-trained backbone weights. This integration ensures that LoRA main- tains the model’s efficiency, adding no extra burden during inference. In LoRA training, selecting an appropriate rank has always been a challenging issue. To address this, DyLoRA [82], as depicted in Figure 8 (b), trains the LoRA module on a range of ranks within a predefined training budget, rather than adhering to a single, fixed rank. Specifically, for a given rank range R “ trmin, rmin`1, . . . , rmaxu, DyLoRA dynamically chooses a rank r P R at each iteration of the training process. Consequently, the matrices Wdown and Wup are tailored for the selected rank r, resulting in truncated versions WdownÓr “ Wdownr1 : r, :s and WupÓr “ Wupr:, 1 : rs, and the subsequent forward and backward pass during this iteration will be restricted 9 Pre-trained Weights W0 ∊ Rd×k Wdown ∊ Rr×k Wup ∊ Rd×r (a) LoRA × d d rmax r r rmax Wdown↓r Wup↓r Wup Wdown × (b) DyLoRA Pre-trained Weights Decompose W0 ∊ Rd×k Pre-trained Weights Magnitude Direction m∊ R1×k 1/॥V+ΔV॥c × V ∊ Rd×k Wup ∊ Rd×r Wdown ∊ Rr×k × (c) DoRA Fig. 8: Illustration of three representative reparameterized PEFT algorithms. Blue represents frozen, while yellow represents trainable. on WdownÓr and WupÓr instead of Wdown and Wup. With this dynamic and search-free approach, DyLoRA significantly reduces the training time required to find an optimal and fixed LoRA rank for specific tasks. AdaLoRA [83] reformulates the ∆W with a singular value decomposition (SVD), denoted as ∆W “ PΛQ, where P P Rdˆr and Q P Rrˆk are orthometric, Λ is a diagonal matrix containing singular values tλiu1ďiďr. All three weight matrices are made learnable. During training, the singular values are pruned iteratively based on their importance scores, which are constructed from the moving average of the magnitude of the gradient-weight product. To ensure the orthogonality between P and Q, i.e., P T P “ QQT “ I, an additional regularizer term is included in the loss: RpP, Qq “ ››P T P ´ I ››2 F ` ››QQT ´ I ››2 F . (13) This adaptive approach enables the model to dynamically ad- just the rank within each LoRA module, effectively managing its parameter counts based on the significance of the weight matrices. However, according to SoRA [84], the importance scores used in AdaLoRA are heuristically constructed, which lacks rigorous theoretical motivation. Additionally, both mov- ing average operation and calculation of Eq. 13 introduce extra computation costs during training. To address this, SoRA eliminates the orthogonality premise of P and Q. Instead, a gating unit g P Rr between Wup and Wdown is directly applied and optimized: hout “ Wuppg d pWdownhinqq, (14) where d is Hadamard product. The gate g is updated using a variation of proximal gradient iteration for l1 loss [110], [111], which has a clear mathematical meaning and does not need the heuristic premise. After training, the zeroed-out gate units are pruned by removing the corresponding columns and rows in Wdown and Wup. Several subsequent studies have aimed to improve LoRA’s performance in various aspects. For instance, Laplace- LoRA [87] notices that fine-tuned LLMs often exhibit over- confidence. To enhance the calibration of fine-tuned LLMs, Laplace-LoRA utilizes a Bayesian approach, specifically a post-hoc Laplace approximation [112], [113], to the posterior over the LoRA parameters. LoRA Dropout [88] introduces random noises to the learnable low-rank matrices and in- creases parameter sparsity to reduce the risk of overfitting. LoRA+ [90] proposes to set different learning rates for the LoRA matrices Wdown and Wup, such that ηup “ ληdown with λ ą 1 fixed and tune ηdown. MoSLoRA (Mixture-of-Subspaces LoRA) [91] decomposes LoRA into subspaces via structural reparameterization, then employs a learnable mixer, trained jointly with the original LoRA weights, to fuse the subspaces. Similarly to LoRA, MoSLoRA can also be merged into the original weights. Thanks to the modular design of LoRA, many studies incorporate multiple LoRA modules in their frameworks to enhance performance. For example, LoRAHub aggregates various LoRA modules trained on different tasks. Given a handful of examples from a new task, LoRAHub can autonomously compose compatible LoRA modules without human intervention via a gradient-free method Shiwa [114]. MOELoRA employs a Mixture-of-Experts (MOE) approach to train LoRA in a multi-task setting, resulting in multiple expert LoRA modules. To retrieve parameters for certain tasks, MOELoRA utilizes a task-motivated gate function that assigns contribution weights to each expert based on the task ID, and the final parameters are calculated through a weighted sum of all experts. In addition to LoRA, several other reparameterization tech- niques are emerging with significant potential. For instance, Compacter [77] introduces a light-weight adapter modules by parameterizing the Wdown and Wup as W “ řn i“1 Ai bBi, where Ai P Rnˆn, Bi P R r n ˆ d n , and b denotes the Kronecker product. They further decrease the parameter count by desig- nating Ai as shared parameters and reparameterizing Bi using the product of two low-rank matrices, effectively reducing the parameter complexity from Oprdq to Opr`dq. Related studies, such as KronA [78] and KAdaptation [79], also employ the Kronecker product to reparameterize adapter weights, aiming to achieve parameter reduction. HiWi [65] proposes an adapter fine-tuning method that applies an adapter directly to pre- trained parameters instead of hidden representations as: W 1 “ W ` σpWWdownqWup, (15) where W denotes the weights or biases within the Transformer block’s feed-forward layer. Notably, during inference, this method computes W 1 in advance, ensuring that the model’s 10 inference latency remains on par with that of traditional full fine-tuning. VeRA (Vector-based Random Matrix Adapta- tion) [80] employs a single pair of frozen low-rank matrices Wup and Wdown that are shared across all layers, and adapts these matrices by learning small, trainable scaling vectors represented as b and d (formally denoted by diagonal matrices Λb and Λd). Specifically, the reparameterization is given by: hout “ W0hin ` ΛbWupΛdWdownhin, (16) where both Wup and Wdown are initialized using a random Gaussian distribution. Similar to LoRA, the scaling vector b is initialized to zeros to ensure that the weight matrix is unaffected during the first forward pass. This method signif- icantly reduces the number of trainable parameters compared to LoRA yet maintains the same performance, enabling the fine-tuning of larger models on a single GPU. DoRA (Weight- Decomposed Low-Rank Adaptation) [81] presents a novel approach as illustrated in Figure 8 (c) by decomposing model weights W0 P Rdˆk into magnitude and direction as follows: W0 “ m V }V }c “ }W0}c W0 }W0}c , (17) where m P R1ˆk is the magnitude vector, V P Rdˆk is the directional matrix, with } ¨ }c being the vector-wise norm of a matrix across each column. Subsequently, DoRA adopts a unique fine-tuning strategy for m and V . While both are tunable, only V undergoes LoRA reparameterization, defined as: W 1 “ m V ` ∆V }V ` ∆V }c “ m W0 ` WupWdown }W0 ` WupWdown}c , (18) where ∆V is the incremental directional update learned by LoRA, and the underlined parameters denote the trainable parameters. Through this methodology, DoRA consistently outperforms LoRA across various tasks and models, demon- strating its superiority. D. Hybrid PEFT The efficacy of various PEFT methods can significantly differ across different tasks. As a result, numerous studies aim to either combine the advantages of diverse PEFT approaches or seek to establish a unified perspective by analyzing the similarities among these methods. For instance, UniPELT [97] integrates LoRA, prefix-tuning, and adapters into each Trans- former block. To control which PEFT submodules should be activated, they also introduce a gating mechanism. This mechanism consists of three small FFNs that each produce a scalar value G P p0, 1q, which is then applied to the LoRA, prefix, and adapter matrices, respectively. Across var- ious setups, UniPELT has consistently shown improvements in accuracy ranging from 1% to 4%. S4 [98] explores design spaces for several PEFT methods (i.e., Adapter (A), Prefix (P), BitFit (B), and LoRA (L)) to uncover underlying design patterns. After a series of experiments, their findings include: (1) Applying the spindle grouping partitioning for Transformer layers, which results in four layer groups Gi for i P t1 . . . 4u. Layers in one group have similar behaviors together, which means should apply similar PEFT strategies. (2) Allocating the number of trainable parameters to layers uniformly. (3) Tuning all the groups. (4) Assigning different PEFT strategies in different groups. The resulting design space that has the best performance is: G1 : pA, Lq, G2 : pA, Pq, G3 : pA, P, Bq, G4 : pP, B, Lq MAM Adapter[32] explores the intrinsic similarity between three additive PEFT methods: adapters, prefix-tuning, and LoRA, which leads to the development of three variants: Parallel Adapter, which places adapter layers alongside spe- cific layers (SA or FFN) instead of after them; Multi-head Parallel Adapter, which divides the parallel adapter into multiple heads, each affecting the head attention output in SA; and Scaled Parallel Adapter, which adds a scaling term after the parallel adapter layer, similar to LoRA. Extensive experimentation revealed that the most effective configura- tion involves using prefix-tuning in the SA layer and the scaled parallel adapter in the FFN layer, which is called the MAM Adapter. LLM-Adapters [101] builds an easy-to-use framework that incorporates various PEFT techniques into LLMs. Through comprehensive benchmarking across multiple datasets, the study reveals several key insights: (1) The most effective locations for series adapters, parallel adapters, and LoRA are after the MLP layers, alongside the MLP layers, and simultaneously following the Attention layers and MLP layers, respectively. (2) Smaller LLMs utilizing PEFT can achieve competitive or even superior results on certain tasks when compared to their larger counterparts. (3) With appropriate in-distribution fine-tuning data, smaller models are capable of surpassing larger models in task-specific performance. Several studies leverage neural architecture search (NAS) to find better PEFT combination approaches. For example, NOAH [99] discovers that different PEFT configurations are specifically tailored for different tasks. To address this issue, NOAH employs NAS to identify the most effective PEFT con- figurations for each dataset. Specifically, NOAH’s searching space encompasses three PEFT methods: Adapter, LoRA, and Visual Prompt Tuning (VPT). It utilizes AutoFormer [115], a one-shot NAS algorithm, for the efficient discovery of optimal prompt modules. In a related vein, AUTOPEFT [100] first establishes a searching space that includes serial adapters, parallel adapters, and prefix tuning. After that, they propose an effective NAS method based on a high-dimensional multi- dimensional Bayesian optimisation [116]. Both NOAH and AUTOPEFT demonstrate the capability of NAS in enhancing PEFT configurations across a variety of tasks. IV. EFFICIENT PEFT DESIGN Processing latency and peak memory overhead are pivotal factors to consider from a computational standpoint. This section introduces a key characteristic in LLMs aimed at balancing between latency and memory usage (Section IV-A). Following this, we explore strategies for developing efficient PEFT methods to address computational challenges, including PEFT pruning (Section IV-B), PEFT quantization (Sec- tion IV-C), and memory-efficient PEFT techniques (Sec- tion IV-D), each designed to enhance model performance 11 Efficient PEFT Design PEFT Pruning AdapterDrop [117], SparseAdapter [118], SPLoRA [119], LoRAPruning [120], ProPETL [121] PEFT Quantization BI-Adapter [122], PEQA [123], QLoRA [124], LoftQ [125], LQ-LoRA [126], QA-LoRA [127], INT2.1 [128], QDyLoRA [129], BitDelta [130] Memory-efficient PEFT Side-Tuning [131], LST [132], Res-Tuning [133], MEFT [134], LoRA-FA [135], HyperTuning [136], PEFT Plug-in [137], MeZO [138], GaLore [139] Fig. 9: Taxonomy of Efficient PEFT Design. while minimizing resource consumption. It is noteworthy that quantization inherently addresses memory overhead concerns. However, given its distinct characteristics, we address these quantization methods separately rather than incorporating them under the memory-efficient PEFT section. A. KV-cache Management for PEFT Efficiency The core of the LLMs model lies in an auto-regressive Transformer model. When we consider the auto-regression characteristic, it becomes a major challenge in designing an inference system, because every time a new token is generated, the entire LLM model has to transfer all the weights from different memories to the memory of the graphics processor, which is very unfriendly to single-user task scheduling or multi-user workload balance. The challenging part of serving the auto-regressive paradigm is that all previous sequences have to be cached and saved for the next proceeding iteration; the cached activation generated from the previous sequences is stored as the Key-Value Cache (KV-cache). To effectively manage these challenges, S-LoRA [140] employs a Unified Paging mechanism within a unified memory pool that dynam- ically allocates and manages memory in a paged fashion. This sophisticated approach minimizes memory fragmentation and enhances the efficiency of KV-cache storage by allowing for flexible and efficient memory access patterns. These pages are managed such that the KV-cache associated with each adapter is segmented into manageable blocks, streamlining access and reducing the overhead associated with variable cache sizes. By dynamically adjusting to different KV-cache requirements, S- LoRA maintains high throughput and performance, ensuring that the system remains responsive and efficient even as it scales to serve thousands of adapters simultaneously. This efficient handling of KV-cache is crucial for supporting the auto-regressive nature of LLMs in high-demand environments, optimizing both single-user and multi-user workload balanc- ing. B. Pruning Strategies for PEFT The inclusion of pruning can substantially enhance the efficiency of PEFT methods. In particular, AdapterDrop [117] explores the removal of adapters from lower transformer layers and multi-task adapters in AdapterFusion [35], which shows that the pruning can improve the training and in- ference efficiency with minimal decrease in performance. SparseAdapter [118] investigates different pruning methods and finds that high sparsity ratio (80%) can outperform stan- dard adapters. Additionally, the Large-Sparse configuration, which increases the bottleneck dimension while maintaining a constant parameter budget (e.g., doubling dimensions with a 50% sparsity), substantially enhances the model’s capacity, resulting in improved performance. SPLoRA [119] adopts channel-based pruning to the LoRA weights Wdown and Wup. This pruning affects not only the source weights W0, but also the LoRA parameters Wup and Wdown. Similarly, Lo- RAPruning [120] adopts structured pruning not only to the pretrained model weights but also to the LoRA weights. In contrast to unstructured LoRA pruning methods, which primarily focus on sparsifying model weights while leaving LoRA weights dense, thus making weight merging challenging to achieve, LoRAPruning enables the weights to be merged easily. Additionally, this work also introduces a novel criterion that utilizes LoRA’s gradients as an approximation of the gradients for the pre-trained weights, enabling the estimation of weight importance. ProPETL [121] constructs a single shared prototype (e.g., adapter, prefix, or LoRA) across layers and tasks. In addition, ProPETL learns binary masks to prune different sub-networks in different layers and tasks. As a result, the parameters can be reused across layers and tasks, largely increasing the parameter efficiency. C. Quantization Strategies for PEFT Quantization serves as another popular technique for im- proving computational efficiency and reducing memory us- age. For example, by investigating the loss landscape of adapters, BI-Adapter [122] finds that adapters are resistant to noise in parameter space. Building on this insight, the authors introduce a clustering-based quantization approach. Remarkably, they demonstrate that a 1-bit quantization of adapters not only minimizes storage requirements but also achieves superior performance among all precision settings. PEQA (Parameter-Efficient and Quantization-aware Adapta- tion) [123] uses a two-stage pipeline to achieve parameter- efficient and quantization-aware fine-tuning. In the first stage, the pre-trained FFN weight matrix W P Rnˆm is quantized to W “ s ¨ W, where s P Rnˆ1 represents per-channel scales and W denotes the quantized weight. In the second stage, W remains fixed, and fine-tuning is only conducted on s. This approach not only ensures memory efficiency but also facilitates parameter efficiency. QLoRA [124] proposes several novel techniques, including a 4-bit NormalFloat, a Double Quantization, and a Paged Optimizers, to backprop- agate a 4-bit quantized pretrained language model into LoRA. 12 These techniques enable the fine-tuning for a 65B language model on a single 48GB GPU while maintaining similar performance to the full 16-bit fine-tuning. Similar to the original implementation [76], QLoRA attaches the fixed zero- initialized LoRA weights to the quantized pre-trained model as the training start point. However, when applying the ex- treme low-bit (e.g., 2-bit) quantization, the huge quantization error can adversely impact the initialization of LoRA fine- tuning, i.e., quantizationpW0q ` WdownWup ‰ W0 where Wdown “ 0, which will harm the fine-tuning performance as shown in the work by [134]. To solve this, several quanti- zation strategies are proposed to eliminate the quantization error. For example, LoftQ (LoRA-Fine-Tuning-aware Quanti- zation) [125] presents an innovative framework that provides a superior initialization point of quantized backbone weights and LoRA weights for subsequent LoRA fine-tuning. This approach addresses the discrepancies caused by quantization through the optimization of a Frobenius norm objective during network initialization, which takes both the LoRA weights and the quantized pre-trained backbone into consideration. LoftQ exhibits superior performance in 2-bit quantization over QLoRA, as well as greater generalization for downstream tasks. LQ-LoRA [126] uses an iterative algorithm inspired by robust principal components analysis [141], [142] which decomposes the weight W0 such that W0 « Q ` L1L2 to resolve the inaccuracy caused by the quantization error, where Q is the quantized component which remains fixed and L1L2 is the trainable low-rank component. Moreover, this approach leverages integer linear programming to determine a mixed quantization strategy, enabling dynamic quantization configurations for each weight matrix while adhering to a predetermined total bit rate limit. QA-LoRA [127] address another limitation of QLoRA, which struggles to preserve its quantized property post-fine-tuning. In QLoRA, the quantized pre-trained weight (NF4) has to be recovered to FP16 to match the LoRA weight precision (FP16) during weight merging. Instead, QA-LoRA uses INT4 quantization and introduces group-wise operators to enable quantization during the infer- ence stage, therefore improving the efficiency and accuracy compared with QLoRA. BitDelta [130] introduces a novel 1- bit post-training quantization method that acts on the weight delta between a fine-tuned model and its underlying pre- trained model. Specifically, given the weight matrices Wfine and Wbase from the fine-tuned and base models respectively, the weight delta ∆“ Wfine ´ Wbase is binarized as ˆ∆“ α d Signp∆q. Here, α, a high-precision scalar, is initialized based on the mean absolute delta value α “ 1 nm ř ij |Wij|, with Signp¨q indicating the sign of ∆. BitDelta further calibrates the scaling factors via distillation on a compact calibration dataset, while the binary matrices remain unchanged. This approach notably streamlines the deployment of multiple fine- tuned models on shared servers by utilizing a singular full- precision base model alongside efficiently batched 1-bit deltas. D. Memory-efficient PEFT Methods Fine-tuning the full LLMs necessitates substantial training memory owing to their considerable size. While most PEFT"
            },
            {
                "section": "methods",
                "content": "primarily target parameter efficiency, they still in- cur a significant memory overhead during training because gradient computation and backpropagation are still necessary for these methods. For example, prevalent PEFT techniques such as adapters and LoRA can only reduce memory usage to approximately 70% compared to full model fine-tuning ac- cording to some literature [132], [137]. From a computational perspective, memory efficiency also remains a critical factor that cannot be overlooked. To improve memory efficiency, various techniques have been developed to minimize the need for caching gradi- ents for the entire LLM during fine-tuning, thereby reducing memory usage. For example, both Side-Tuning [131] and LST (Ladder-Side Tuning) [132] introduce a learnable net- work branch parallel to the backbone model. By channeling the backpropagation exclusively through this parallel branch, it circumvents the need to store gradient information for the main model’s weights, thus markedly reducing memory requirements during training. Similarly, Res-Tuning [133] disentangles the PEFT tuners (e.g., prompt tuning, adapter) from the backbone model. On top of the disentanglement, a memory-efficient fine-tuning framework named Res-Tuning- Bypass is proposed, which generates a bypass network in parallel with the backbone model by removing the data flow from the decoupled tuners to the backbone. This eliminates the requirement for gradient caching within the backbone model during backpropagation. MEFT [134] (memory-efficient fine- tuning) is an approach inspired by the reversible model [143]. During the training of a reversible model, intermediate ac- tivations are not required to be cached in the forward pass. During backpropagation, they can be recalculated from the final output. To save the memory during fine-tuning, MEFT investigates how to transform an LLM to its reversible counter- parts without additional pre-training. A critical aspect of this transformation is the careful initialization of newly introduced parameters in the pre-trained models. MEFT demonstrates the importance of parameter initialization and suggests that these parameters must be initialized in a manner that preserves the pre-trained model’s starting point, ensuring that the fine-tuning of the modified model achieves performance on par with full fine-tuning methods. With this key consideration, MEFT intro- duces three distinct methods, each significantly curtailing the memory demands traditionally required for storing activations. LoRA-FA [135] addresses a limitation about memory over- head in LoRA fine-tuning. During training, LoRA modules still require high activation memory consumption. This is be- cause, during backpropagation, large input activations must be stored during the forward pass to compute gradients. LoRA-FA resolves this issue by freezing both the pre-trained weights W0 and the projection-down weights Wdown, and only updating the projection-up weights Wup. Consequently, the input activation hin no longer needs to be stored, as the intermediate activation Wdownhin is adequate for gradient computation for Wup. Given that r ! d, the memory requirement for activations in LoRA- FA can be significantly reduced. To further reduce memory usage during fine-tuning, some"
            },
            {
                "section": "methods",
                "content": "attempt to circumvent backpropagation within LLMs to address this issue. HyperTuning [136] employs a Hyper- 13 Model to generate PEFT parameters using only fewshot exam- ples. This approach demonstrates results comparable to those obtained through full model fine-tuning. PEFT Plug-in [137] first trains PEFT modules on small language models, which is more memory efficient compared to training on large ones. Subsequently, the research introduces a suite of techniques for seamlessly integrating these trained PEFT modules into LLMs during inference. This strategy effectively circumvents the necessity of gradient-based optimization directly on the larger models, resulting in substantial memory savings. However, it is important to note that both HyperModel and PEFT Plug-in still require additional model training, and this training cost cannot be entirely overlooked. MeZO [138] introduces a memory- efficient zeroth-order (ZO) optimizer for LLMs. Unlike con- ventional PEFT techniques, which rely on backpropagation to compute gradients for updating model parameters, MeZO fine- tunes LLMs through only forward passes. It accomplishes this by employing a ZO gradient estimator to calculate the gradient. Notably, MeZO implements an in-place solution for the classic ZO gradient estimator, effectively mitigating memory con- sumption during inference execution. This innovative approach allows for efficient fine-tuning of LLMs containing 30 billion parameters on a single GPU with 80GB of memory, all while maintaining performance that is comparable to fine-tuning using backpropagation. Furthermore, it can substantially de- crease storage demands in comparison to the traditional PEFT"
            },
            {
                "section": "methods",
                "content": "such as LoRA and Adapter. V. PEFT FOR DNNS OF OTHER APPLICATIONS In Section III, we outlined four categories of PEFT methods along with their improvements. Nonetheless, our discussion did not fully extend to the utilization or adaptation of PEFT techniques beyond traditional architectures (e.g., LLMs) or standard benchmarks (e.g., the GLUE dataset), where the ma- jority of the discussed PEFT methods are applied. Therefore, in this section, we will highlight and discuss several most representative works that leverage PEFT strategies for various downstream tasks. We do not aim to cover all PEFT applica- tion scenarios in this section. Our objective is to showcase the significant influence of PEFT within various research domains and demonstrate how to optimize and tailor general-purpose PEFT methods to achieve enhanced performance in specific models or tasks. Typically, fine-tuning happens when adapting a pre-trained backbone model to specialized downstream tasks. To this end, this section organizes the discussion around various model architectures, which include: LLM, Vision Transformer (ViT), Vision-Language Alignment Model (VLA), and Diffusion model. Within each architectural category, the discussion is further classified based on different downstream tasks. A. PEFT for LLMs – Beyond the Basics Instead of common tasks in NLP such as NLU and NLG, PEFT techniques boast a wide array of applications across diverse scenarios. PEFT has been successfully implemented in commonsense question answering [144], [145], multi-level im- plicit discourse relation recognition [146], out-of-distribution detection [147], privacy protection [148], [149], federated learning [150], and social biases mitigation [151]. In this section, we pay more focus on three representative downstream tasks: visual instruction following, continual learning, and context window extension. 1) Visual Instruct Following: Several studies, including VL-BART [152], MiniGPT-4 [153], and LLaVA [154], have successfully extended the capabilities of LLMs, initially de- signed for pure text, to comprehend and generate responses to visual inputs. These enhanced models, namely visual instruct- following LLMs, can process both images and text to produce textual responses, which can be benchmarked on tasks such as image captioning [155], [156], [157], [158] and visual question answering (VQA) [159], [160], [161]. However, these methods fine-tune the entire LLM to learn the visual representations, which can be inefficient in both time and memory. Therefore, it is natural to apply PEFT techniques in the fine-tuning of visual instruct-following LLMs. An earlier work VL-Adapter [162] directly applies several PEFT methods (Adapter [31], Hy- performer [40] and Compacter [77]) on VL-BART [152] then benchmarks them on several image-text and video-text tasks. Results show that vanilla adapters are the best among them, which can achieve performance on par with full fine- tuning. However, considering the functionality gap between the encoders and decoders in VL-BART, directly assigning identical modular modifications will lead to suboptimal perfor- mance. Therefore, VL-PET [163] selectively integrates PEFT modules into different components of the encoder and decoder. They also introduce a granularity-controlled mechanism for finer-grained control. To adapt the recently prevalent LLaMA model, LLaMA- Adapter [164] prepends a set of learnable prompts (similar to prefix tuning) to the input tokens in LLaMA’s higher trans- former layers. To avoid the unstable fine-tuning with large loss values at early training stages, instead of the randomly initial- ized weights of other PEFT methods, LLaMA-Adapter adopts a zero-initialized attention mechanism, which learns a zero- initialized gating factor to adaptively control the contribution of adaptation prompts to the word tokens. This can maintain the fine-tuning starting point the same as the original model and progressively inject new knowledge into the model, where a similar idea can be found in MEFT [134] and LoftQ [125] discussed earlier. To represent visual information, LLaMA- Adapter extracts multi-scale global image features using a CLIP image encoder and then projects them to linguistic em- bedding space. After that, the feature is element-wisely added onto the adaptation prompts at all inserted transformer layers. LLaMA-Adapter only introduces 1.2M learnable parameters in LLaMA-7B and costs less than one hour for fine-tuning on 8 A100 GPUs. A following work LLaMA-Adapter V2 [165] demonstrates that the simple multimodal fusion in LLaMA- Adapter cannot generalize to more challenging open-ended multimodal reasoning tasks, where the visual cues tend to dominate the adaptation prompts than the language instruction data. To address this, LLaMA-Adapter V2 decouples the learn- ing of instruction-following ability (to generate long language responses) and vision-language alignment to avoid interfer- ence between visual and language fine-tuning. Specifically, 14 LLaMA-Adapter V2 sets disjoint parameter groups which are respectively learned from image-text pairs and language instruction data. The visual adaptation prompts are inserted in the early stage of LLM, while the language adaptation prompts remain at the higher transformer layers similar to the LLaMA- Adapter. Additionally, LLaMA-Adapter V2 introduces more learnable parameters and several expert systems (e.g., caption- ing, detection, and OCR) to enhance multimodal performance. LayerNorm Tuning [166] adjust only the weights of the LayerNorm within each attention block. This straightforward technique can achieve comparable or even better performance than the finetuning, while offering about 10× more parameter efficiency than LoRA. 2) Continual Learning: Continual Learning (CL) aims to learn a sequence of new tasks over time within one single model, which has broad application in scenarios such as dialogue systems [167], information extraction systems [168], and question answering systems [169]. The main challenge in CL is catastrophic forgetting [170]. A popular practice, called architecture-based methods, tackles the CL by maintaining task-specific parameters in the model for each new task. There- fore, it’s natural to leverage PEFT methods for CL tasks [171], [172], [173], [174]. For example, AdapterCL [171] pa- rameterizes each new task using residual adapters. During testing, since the task-id is not provided, AdapterCL uses an entropy-based classifier to select which adapter to use for accomplishing a specific task. CPT (Continual Prompt Tuning) [172] trains a soft prompt for each task. Instead of training soft prompts from scratch, CPT proposes a series of techniques (continual prompt initialization, query fusion, memory replay, and a memory-guided technique) to achieve knowledge transfer from preceding and subsequent tasks. O-LoRA (orthogonal low-rank adaptation) [175] employs a strategy of learning distinct tasks within separate low-rank vector subspaces that are kept orthogonal to each other in order to minimize interference. This approach can effectively reduce catastrophic forgetting during the acquisition of new tasks. 3) Context Window Extension: LLMs are typically trained with a pre-defined context size. For example, LLaMA and LLaMA2 have pre-defined context sizes of 2048 and 4096 tokens, respectively. The positional encoding RoPE has weak extrapolation properties [176], which means the performance drops obviously given an input length exceeds the pre-defined context length. To solve this, a naive solution is to fine- tune a pre-trained LLM to a longer context. However, this escalates computational costs quadratically with context size, straining memory and processing resources. To address this, LongLoRA [177] proposes to fine-tune a pre-trained LLM using LoRA to enlarge the context size. To reduce the perplexity gap between LoRA tuning and full fine-tuning, LongLoRA also opens embedding and normalization layers for training. In order to further improve training efficiency in a long context scenario, LongLoRA further introduces a novel shifted sparse attention (S2-Attn) as an efficient substitute for standard self-attention during training. A subsequent study LongQLoRA [178] combines the advantages of LongLoRA with QLoRA and Position Interpolation [10] to save GPU memory. This work successfully extends the context length of LLaMA2-13B from 4096 to 8192 on a single V100 with 32GB memory. LLoCO [179] introduces a pipeline that learns contexts offline through the combination of context compression and LoRA. The process begins by compressing documents into compact contexts, then fine-tuning LLM us- ing LoRA on the compacted context to improve the LLM’s ability to accurately extract and utilize information from these compressed representations. During model serving, a standard RAG retriever selects both the compressed document and the most relevant LoRA module, and applies them to the LLM for inference. This approach effectively extends the context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. In addition to limited training-stage sequence length, real- world system memory constraints introduce another critical bottleneck to the context window. Specifically, the capacity of the KV-cache is curtailed by available system memory. For example, a 30B parameter LLM operating with an input length of 1024 and a batch size of 128 might necessitate up to 180GB for the KV-cache [180], thereby restricting the feasible size of the context window. In response to this, some strategies have resorted to quantizing the KV cache [181], [182], but quanti- zation will certainly compromise performance. To effectively counteract this issue without significant loss, GEAR [183] presents a novel approach by employing a low-rank matrix to capture the majority of coherent bases of quantization error, complemented by a sparse matrix that addresses errors from outlier entries, thus efficiently minimizing approximation errors. B. PEFT for ViTs ViT [184] has emerged as a powerful backbone model in the recent computer vision community. In the ViT model, images are treated as sequences of fixed-size patches analogous to how LLM uses discrete tokens. These patches undergo linear em- bedding and then receive positional encodings. Subsequently, they are processed through standard Transformer encoders. The training of ViT can be supervised [184], [185] or self- supervised [186], [187], and ViT can achieve superior perfor- mance when training with more data and using larger model size [188]. However, such scaling up inevitably escalates training and storage costs. Therefore, similar to LLMs, PEFT is widely implemented in various downstream tasks, such as dense prediction [189], continual learning [190], [191], deep metric learning [192]. Here, we focus on two typical tasks to showcase the involvement of PEFT: image classification and video recognition. 1) Image Classification: Image classification on targeted visual datasets is a very common demand and has extensive applications, while pre-train then fine-tuning paradigm serves as a widespread strategy. A variety of methods leverage PEFT techniques to achieve efficient model tuning [193], [189], [194], [195]. For instance, AdaptFormer [194] inserts adapter modules in parallel to the FFN of the original ViT model for visual recognition tasks. VPT (Visual Prompt Tuning) [193] prepends a small amount of task-specific parameters into the input sequence of each Transformer layer. When applying 15 ViT to downstream tasks, only these added parameters and the classification head are set to trainable. [196] notices that compared with supervised ViT, VPT often underperforms with self-supervised ViT. Further analysis demonstrates that differ- ent pre-trained methods and downstream tasks have varying degrees of dependency on transformer blocks at different lo- cations. To tackle this issue, the research introduces adaptable gates for ViT blocks. These gates dynamically modulate the contribution of prompt tokens to ViT blocks, allowing for a more targeted adaptation of the model to the task at hand. 2) Video Recognition: Several works consider the more challenging adaptation problem that transfers ViT to down- stream tasks that have a much larger domain gap. For example, ST-Adapter (Spatio-Temporal Adapter) [197] and AIM [198] both insert adapters layers into pre-trained ViT blocks. Their primary goal is to model spatial-temporal information, thereby enabling efficient adaptation of ViTs from image models to video tasks. Notably, both methodologies have exhibited performance that surpasses traditional full-model fine-tuning approaches. C. PEFT for VLAs Vision-language alignment models (VLA), such as CLIP [199], ALIGN [200], DeCLIP [201], and FLAVA [202], are designed to learn a good image and text features which can be aligned within a unified representation space. Each VLA typically consists of separate image and text encoders that extract respective features. Contrastive learning is leveraged in these models to effectively align the image and text features. Fine-tuning is leveraged to improve the performance of VLA in specific datasets or tasks, but fine-tuning the full model is computationally intensive. For instance, fine-tuning CLIP RN50x64 requires a batch size of 32,768 and 18 days of training on 592 V100 GPUs [199]. Moreover, full fine-tuning on smaller datasets often leads to catastrophic forgetting [170]. In response to these challenges, and drawing inspiration from the success of PEFT techniques in NLP, a range of PEFT strategies have been proposed and implemented in VLA models, such as semantic segmentation [203], [204], [205], point cloud understanding [206], [207], [208], [209], video understanding [210], [211], [212], visual reasoning [213], [214], temporal action detection [215], to name a few. This section will focus on one common task that uses VLAs: open-vocabulary image classification. 1) Open-vocabulary Image Classification: In open- vocabulary image classification, earlier works design class-specific prompts, e.g., a photo of a [CLASS], for each category, and rank images based on their similarity to these textual descriptions. CoOp (Context Optimization) [216] replaces the handcrafted text prompt with learnable vectors, while keeping the entire VLA fixes during training. CoCoOp (Conditional Context Optimization) [217] builds on this by tackling CoOp’s limitations in generalizing to unseen classes. It introduces a lightweight neural network that generates an input-specific context token, dynamically adapting the prompt based on each image, thereby enhancing generalizability, but at the cost of increased computational demands due to the instance-aware operation. ProGrad [218] addresses the over-fitting risk in CoOp in a few-shot setting by regularizing the soft prompt updates whose gradient is aligned to the general knowledge only updates the prompt whose gradient is aligned (or non-conflicting) to the general knowledge offered by the original prompt. MaPLe [219] notes that existing"
            },
            {
                "section": "methods",
                "content": "learn prompts either in the language or in the vision branch of CLIP, which is not efficient in leveraging the multimodal nature of VLAs. To address this, MaPLe proposes branch-aware hierarchical prompts that simultaneously adapt both language and vision branches, and achieves superior performance. TPT (test-time prompt tuning) [220] studies prompt tuning on the fly without additional training samples. Specifically, during inference, TPT first augments the input image into various views, which are then utilized to tune the learnable prompts. The primary training objective is to ensure the VLA can generate consistent responses when faced with these differing views. A following work DiffTPT [221] further enhances the data diversity of test samples through diffusion models. In another direction, several studies explore the usage of adapters in VLA. For example, CLIP-Adapter [222] inte- grates residual-style adapters after CLIP’s text and visual en- coders. Therefore, unlike CoOp and CoCoOp, CLIP-Adapter avoids the gradient backpropagation through CLIP’s encoders, leading to reduced computational requirements in terms of both training memory and time. Tip-Adapter [223] adopts the same design with CLIP-Adapter. Different from CLIP- Adapter, the weights of the adapter are obtained in a training- free manner from a query-key cache model [224], [225] constructed from few-shot supervisions in a non-parametric manner. As a result, Tip-Adapter exhibits great efficiency compared to CLIP-Adapter’s SGD training process. D. PEFT for Diffusion Models Diffusion models [226], [227] are a class of generative models that learn to generate data by transforming random noise into a structured output by a progressive denoising process. During training, diffusion models learn to reverse the noise added to training data using a denoising network, while in inference, they start from noise, using a denois- ing network to iteratively create data that mirrors the same distribution as the training examples. Diffusion models have various applications [228], [229], [230], [231], [232], while the most notable is stable diffusion [233], which bridges the gap between text and image with its robust capability to generate coherent and contextually relevant images directly from textual descriptions. Numerous studies leverage PEFT techniques to adapt a pre-trained diffusion model for downstream tasks, in- cluding accelerating sampling speed [234], [235], text-to-video adaptation [236], [237], text-to-3D adaptation [238], etc. This section mainly focuses on two scenarios: integrating additional input modalities beyond mere text-based conditioning, and customizing content generation based on pre-trained diffusion model. 1) Additional Input Control: To incorporate additional in- put modalities (e.g., layout, keypoints) while retaining the 16 extensive knowledge in the pre-trained model, GLIGEN introduces a novel approach, which maintains the original model’s weights intact and integrates new, trainable gated Transformer layers [239] that take in the new grounding input. The resulting model can not only accurately repre- sent the grounding conditions but also produce high-quality images. Remarkably, the model can also generalize well to unseen objects during inference. ControlNet [240] fine-tunes a trainable copy of the encoding layers from Stable Diffusion while locking its pre-trained parameter weights. The fixed original model and the trainable copy are bridged through zero convolution layers. These layers, starting with zero-initialized weights, are designed to progressively adapt during training, ensuring that harmful noise does not affect the pre-trained features of Stable Diffusion at the beginning of training. This refined model is capable of conditioning on a variety of inputs such as Canny edges, Hough lines, user scribbles, human key points, segmentation maps, shape normals, depths, etc. Con- cept Sliders [241] introduces a plug-and-play LoRA adaptors to allow precise editing of concepts (e.g., age, smiling) within a diffusion model. T2I-Adapter [242] introduces a lightweight adapter model designed to align external control signals with the internal knowledge of text-to-image diffusion models. This adapter enables precise manipulation through structural control (e.g., sketch, depth map, semantic segmentation map, and keypose), color control (e.g., hue and color distribution), and integrating various controls by composing multiple adapters. 2) Customized Generation: The effectiveness of text-to- image diffusion models is limited by the user’s ability to articulate the desired target through text descriptions. For instance, it is difficult to describe the precise features of an innovative toy car which is not encountered during large-scale model training. Consequently, the objective of customized generation is to enable the model to grasp new concepts from a minimal set of user-supplied images. Textual Inversion [243] addresses this by finding a new pseudo-word S˚ (similar to soft prompt discussed in Section III-A2) that represents new, specific concepts in the textual embedding space of pre-trained text-to-image diffusion models. The pseudo-word S˚ is opti- mized via the original optimization goal in diffusion models given a small image set (typically 3-5 images) depicting the concept, and the pre-trained model is left untouched. During inference, S˚ can be treated like any other word and composed with other textual queries (e.g., ”a photo of S˚ on the beach”). Custom Diffusion [244] tackles a more challenging setting: compositional fine-tuning of multiple concepts. It fine-tunes only the Wk, Wv mapping from text to latent features in attention layers, which yields superior performance in multi- concept learning scenarios. Additionally, during fine-tuning, Custom Diffusion prevents model forgetting by introducing a small set of real images with captions akin to the target, alongside employing augmentation for faster convergence and improved results. IP-Adapter [245] identifies limitations in current approaches (e.g., ControlNet and T2I-Adapter) which project condition signals into the cross-attention modules. When handling image conditions aiming at controlling content, these methods are unable to generate images faithful to the prompted image. The issue stems from that merging image features and text features within cross-attention layers loses image-specific information, leading to only coarse-grained controllable generation such as image style rather than image content. To overcome this, IP-Adapter introduces a novel decoupled cross-attention mechanism to distinguish between text and image features. IP-Adapter adds an additional cross- attention layer exclusively for image features in each cross- attention layer, and only the parameters of the new cross- attention layers are trained. VI. SYSTEM DESIGN CHALLENGE FOR PEFT A. System design for PEFT In this section, we begin by providing a concise overview of cloud-based PEFT systems and analyzing the design chal- lenges. These include the efficient handling of numerous task- specific queries via centralized PEFT query servicing, the resolution of privacy and data transmission issues through distributed PEFT training, and the complexities associated with concurrent multi-PEFT training processes. Centralized systems are required to process a substantial volume of queries with minimal latency and maximal throughput. Distributed training frameworks must address privacy concerns and the computational inefficiencies that arise from data exchanges between users and cloud services. Furthermore, multi-PEFT training necessitates the optimization of memory utilization, the management of simultaneous model training, and the formulation of system architectures capable of supporting multi-tenant workloads effectively. These challenges under- score the imperative for innovative approaches to improve scalability, safeguard privacy, and optimize resource allocation in PEFT system architectures. Following this, we present the corresponding metrics employed for evaluating the system performance. Furthermore, we delve into three prospective utilization scenarios to illustrate the challenges in system design. 1) Centralized PEFT Query Serving: Cloud providers have recently introduced a range of LLM services aimed at pro- viding user applications through application programming interfaces (APIs) [246], [247]. These APIs facilitate the seam- less integration of many machine-learning functionalities into applications. When receiving one query for one specific down- stream task through API, the cloud-based server processes the query with one featured LLM model. Under this scenario, the importance of PEFT becomes apparent. Cloud providers store only a single copy of the LLM and multiple PEFT modules featuring different downstream tasks. This setup allows the LLM to maintain various branches of PEFT modules, each linked to specific API queries, i.e., PEFT queries. Centralized PEFT query serving solutions address scenarios where multiple PEFT queries arrive in quick succession. A case study of one state-of-the-art system for this purpose is discussed in Section VI-B. Figure 10 (b) illustrates the computation pattern for multi-query PEFT inference, wherein packed PEFT queries are scheduled and executed according to their deadlines and current system conditions. 2) Distributed PEFT Training: In most cases, personal- ized tasks are not fully supported with pre-trained models, 17 LLMs Edge Device Personal data Cloud Trainable Modules 🔥 Frozen Large Models Scheduler Request Pool Query Response Execution Engine Serving System I like I enjoy LLM programming (a) (b) Fig. 10: (a) Distributed-based system computation pattern; (b) centralized PEFT Query inference. consequently, extra fine-tuning is required to be executed with the methodologies mentioned in the previous sections. However, significant concerns arise when considering the transfer of datasets to cloud providers, given the issues related to data privacy, copyright, proprietary information, and the complexities and inefficiencies involved in data transmission. Section VI-C gives two approaches that address this concern. 3) Multi-PEFT Training: Different from multiple-PEFT serving, tuning with multiple customized PEFTs always in- volves different backbone LLMs. Therefore, simultaneously tuning multiple PEFTs can pose considerable challenges. Challenges like how to manage memory gradient and model weights storage, and how to design an efficient kernel for batching PEFT training remain unsolved. PEFTs will be cat- egorized based on their PEFT algorithms and backbone LLM models. The design challenge involves how to consolidate multiple PEFTs with the same LLM backbone and multiple different LLM backbones simultaneously. We present case studies related to this topic in Section VI-D. 4) Evaluation Metrics: For the proposed evaluation met- rics, without loss of generality, we adopt large language models as the basis for our metric definitions. To evaluate the system performance of PEFT serving sys- tems, we propose a set of evaluation metrics: ‚ System throughput: Considering PEFT queries as inter and intra tasks, we use tokens per second to measure the system throughput. ‚ Memory footprint: Run-time memory consumption dur- ing query serving, the memory utilization comes from both model parameters and KV-cache as mentioned in Section IV-A. ‚ Accuracy performance: Real-world queries normally have different context lengths, and performance with variation length serves as a performance benchmark. ‚ Quality of services: Queries are associated with latency requirements and deadline missing rates are considered as another benchmark. To assess the efficacy of PEFT training systems, we also establish a set of evaluative metrics: ‚ Accuracy performance: Performance of the fine-tuned model over the downstream tasks. ‚ Compute cost: The compute cost during forward and backward propagation operations on cloud servers and edge devices. ‚ Communication cost: Refers to the volume of data involved during the transfer of intermediate data between the edge device and the cloud. B. Centralized PEFT Serving Frameworks The PEFT algorithm is notable for its ability to distin- guish between modifiable and immutable weights within a model. This characteristic inspires developers to amalgamate diverse LLMs with distinct PEFT techniques into collective units. PetS, as introduced in [248], advocates for a com- prehensive approach to managing multiple PEFT tasks by suggesting a unified serving framework. The framework’s core advancement lies in the translation of varying PEFT tasks into integrated computation kernels to enhance efficiency. Moreover, PetS pioneers an orchestrated batching approach and a scheduling methodology, aiming to augment system throughput and leverage task parallelism respectively. As depicted in Figure 11, the PetS framework begins with users registering PEFT tasks through a standardized Application Programming Interface (API). Upon registration, developers are expected to provide the Pre-Trained Model Tag (e.g., LLaMA), PEFT parameters in a compressed format, and the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit, etc.). These tasks are then endowed with unique identifiers, and the inference engine takes charge of query processing. PetS bifurcates the primary computational workload (e.g., linear layer computations) into three distinct computational operations: (1) Dense Matrix-Vector Multiplication (MVM) leveraging universally accessible, pre-trained weights. (2) Bias vector addition (Vadd), using either common or task-exclusive biases. (3) A combination of Sparse/dense MVM operations employing task-specific PET parameters. A unified pre-trained weight matrix W is employed across PetS, facilitating the batching of initial operations, Xt ˆ W. However, subsequent task-specific computations involving PET parameters, despite being relatively minimal in complexity, are processed individ- ually. Considering the Adapter and Bitfit tasks as an illustration, both aim at the MLP component of LLMs. The Adapter task integrates additional weight segments, whereas Bitfit adjusts bias elements. The Adapter operation is modeled as Y “ Xin1 ˆ pW ` Wadq ` b0, where Xin1 represents the input for the Adapter task, W and Wad are the original and adapter-specific PEFT weights respectively, and b0 is the initial bias. The Bitfit operation, on the other hand, is defined as Y “ Xin2 ˆ W ` b1, with b1 symbolizing the Bitfit- adjustable bias. These operations are further synthesized as tY1, Y2u “ tXin1, Xin2u ˆ W ` tXin1 ˆ Wad, 0u ` tb0, b1u, delineating that the tXin1, Xin2u ˆ W part is amenable to batching through MVM, while the tb0, b1u segment pertains to the Vadd operation. For tasks like Diff-Pruning III-B, is a little bit different than Bitfit and Adapter. For Diff-Pruning, the computation concerning the shared weight and ‘difference’ are conducted separately. Then the results are added up, namely Xt ˆ pW ` δtq “ Xt ˆ W ` Xt ˆ δt , here, the W denotes the backbone model weights while δt denotes the pruned weights which can be represented as Sparse MVM. 18 PET Serving PET Inference Pipeline Pre-train Model ID Shadow Parameters PET Type Pre-train Model ID Shadow Parameters PET Type Pre-trained Model Tag PET Parameters PET Type PET Parameters Shared Model Parameters Register Tasks u Task Register PET Manager Task Manager Parameter Repository v w <Task_id> <Input Data> … Query 0: Query 1: Input Queries x Performance Model Batch Scheduler Scheduling Policy Engine PET Task Scheduler PET Operator Library y Input Analyzing Input Reformatting User Inputs <Task_id> <Input Data> Fig. 11: PetS system overview: (1) Tasks register; (2) Task manager (3) Task schedule; (4) Task serving. (Image is taken from PetS [248]) Task 0 Task 4 Step 1: Intra-Task Batching Task 1 Task 2 Task 3 Mini Batch 𝛽−Model 𝛼−Model PET-OPs Profiling Batch 1 Batch 0 B=2, S=34 Step 2: Inter-Task Batching Batch 2 Macro Batch Shared-OPs Profiling B=4, S=34 Task 0 Task 1 Task 3 Task 2 Task 4 Fig. 12: Coordinated Batching (CB) Strategy The other challenge PetS proposed is how to schedule dif- ferent PEFT requests to achieve high performance. PetS sched- uler achieves high parallelism through a two-level scheduling policy: Coordinated Batching (CB) and Macro-batch Stream- ing (MS) as Figure 12 depicts. Through CB, the input queries will first be clustered based on their input length and then grouped based on their shared operator. This is to make sure the same sequence length of queries will be executed without wasting padding. MS strategy will take the grouped queries after coordinated batching and the theoretical latency for different operators as well as the system modeling parameters to generate the best execution order. The other example design is DLoRA [249], which intro- duces a system that improves the efficiency of serving low- rank adaptation (LoRA) models for large language models (LLMs) by dynamically managing the merging and unmerging of LoRA adapters and the migration of requests across worker replicas. This dynamic orchestration addresses the challenges of high memory footprints, low GPU utilization, and load imbalance caused by variable input and output lengths in traditional LLM serving systems. dLoRA’s novel approaches, including a credit-based batching algorithm and a request- adapter co-migration algorithm, significantly enhance through- put. C. Distributed PEFT Training Frameworks We already know that fine-tuning LLM for downstream tasks is challenging for two reasons: dual privacy concerns between cloud server and data owner, and issues with com- putational resources and efficiency. Firstly, the privacy of both parties is at risk: the weights of large models are often proprietary and not made public. Sharing data with model owners for fine-tuning can lead to data privacy concerns while providing model weights to data proprietors could compromise the ownership of proprietary models. Secondly, even if down- stream users have access to pre-trained weights, the stringent hardware requirements make transfer learning impractical for most end users. To resolve these two issues, DLoRA [250] presents a distributed PEFT framework. During the PEFT process, the backbone LLM is executed in the cloud servers while the PEFT modules are trained entirely within the user devices. DLoRA scheme is depicted in Figure 10(a). Similarly, Offsite-Tuning [251] presents a privacy- preserving and efficient transfer learning framework that enables foundational models to adapt to downstream tasks without the need to access the complete model weights. The key insight of Offsite-Tuning is the cloud provider sends an adapter and an emulator to the data proprietor. Then, with the assistance of the emulator, the data proprietor fine-tunes the adapter. The fine-tuned adapter is then sent back to the cloud side, which integrates it into the complete model, creating a fine-tuned foundational model for downstream users. Offsite- Tuning safeguards the privacy of data proprietors since they do not need to share their training data directly. It also protects the foundational model owners, as the complete model weights are not shared, and the emulator provided is lossy, with significantly degraded performance. Compared to existing fine-tuning methods that require access to the full model weights, Offsite-Tuning is more resource-efficient because it allows for fine-tuning through a compressed emulator without needing the complete model. D. Parallel PEFT Training Frameworks Unlike the PEFT query serving system, which aims to accommodate flexible multi-PEFT algorithms, Punica [252] focuses solely on facilitating multiple-LoRA blocks for various tasks. Designing multiple PEFT training systems presents key challenges in two main aspects: ‚ Efficient concurrent execution of multiple PEFT models with the same LLM backbone. ‚ Designing an efficient system for multi-tenant serving with different LLM backbones. a) Efficient kernel design: Punica addresses the first chal- lenge by using existing matrix multiplication for the backbone computation and introducing a new CUDA kernel, Segmented Gather Matrix-Vector Multiplication (SGMV), for adding the PEFT add-ons to the backbone computation in a batched manner. This kernel parallelizes the feature-weight multipli- cation for different requests in the batch and groups requests corresponding to the same PEFT model to increase operational intensity and use GPU Tensor Cores for acceleration. 19 The second challenge is beyond the computational cost, designing an efficient system architecture that can effectively serve multi-tenant PEFT model workloads on the smallest set of GPUs possible while occupying the least amount of GPU resources is another significant challenge. Punica addresses this by scheduling user requests to active GPUs that already serve or train PEFT models, thereby improving GPU utiliza- tion. For older requests, Punica periodically migrates them to consolidate workloads, thus freeing up GPU resources for new requests. b) Multi-Tenant PEFT design: Designing an efficient system for the multi-tenant PEFT model serving in the Punica framework focuses on addressing several key challenges to maximize hardware utilization and minimize resource con- sumption. The system aims to consolidate multi-tenant LoRA serving workloads onto the smallest set of GPUs possible. This consolidation is achieved through strategic scheduling of user requests to active GPUs that are already serving or training LoRA models, thereby improving GPU utilization. For older requests, Punica periodically migrates them to consolidate workloads further, thus freeing up GPU resources for new requests. It incorporates on-demand loading of LoRA model weights, which introduces only millisecond-level latency. This feature provides Punica with the flexibility to dynamically consolidate user requests to a small set of GPUs, without being constrained by the specific LoRA models already running on those GPUs. Besides that, Punica identifies that the decode stage is a predominant factor in the cost of model serving, Punica’s design primarily focuses on optimizing decode stage performance. Other aspects of model serving leverage straight- forward techniques, such as on-demand loading of LoRA model weights, to efficiently manage resource utilization. VII. CONCLUSION AND FUTURE DIRECTIONS In the current era dominated by large models and large datasets, PEFT stands out as a highly attractive method for efficiently adapting models to downstream tasks. This tech- nique gains its appeal by addressing the significant challenges posed by traditional full-model fine-tuning, which often places substantial computational and data demands. This survey of- fers a comprehensive examination of the most recent advance- ments in PEFT, including algorithmic design, computational efficiency, application scenarios, and system implementation for PEFT. It offers a comprehensive taxonomy and explanation that serves as an excellent guidance and knowledge base, which enables readers of various levels and disciplines to swiftly grasp the core concepts of PEFT. For further research on PEFT, we propose a series of pos- sible directions from both algorithm and system perspectives, hoping to inspire more researchers to engage in further studies in these areas. A. Simplify hyperparameter tuning The effectiveness of PEFT is often sensitive to its hyperpa- rameters, such as the bottleneck dimension of the adapter, the rank of LoRA, and the arrangement of various additive PEFT layers. Manually tuning these hyperparameters will cost lots of effort. Therefore, future efforts could focus on developing"
            },
            {
                "section": "methods",
                "content": "that are less dependent on manual tuning of these parameters, or automatically find the optimal configuration settings. Several studies [82], [83], [84], [98], [99], [100] have started to address this issue, but there’s a need for more simple and efficient solutions optimizing these hyperparameters. B. Establish a unified benchmark Despite the existence of libraries like HuggingFace’s PEFT [253] and AdapterHub [254], a comprehensive bench- mark for PEFT is still lacking. This gap hinders the ability to fairly compare the performance and efficiency of different PEFT approaches. A well-accepted, up-to-date benchmark akin to MMDetection [255] for object detection would enable researchers to validate their methods against a standard set of tasks and metrics, fostering innovation and collaboration within the community. C. Enhance training efficiency The presumed parameter efficiency of PEFT is not always consistent with computational and memory savings during training. Given that trainable parameters are intertwined within the pre-trained model’s architecture, computing and storing activations and gradients for the full model often become necessary during fine-tuning. This oversight calls for a rethink- ing of what constitutes efficiency. As outlined in Section IV, potential solutions lie in the integration of model compres- sion techniques such as pruning and quantization, alongside innovations specifically designed to optimize memory during PEFT tuning [256]. Further research into enhancing the com- putational efficiency of PEFT methodologies is imperative. D. Explore scaling laws The design and effectiveness of PEFT methods originally developed for smaller Transformer models do not necessarily scale with larger models. As the size of foundation models increases, identifying and adapting PEFT strategies that remain effective is crucial. This investigation will aid in customizing PEFT methodologies to suit the evolving landscape of large model architectures. E. Serve more models and tasks The rise of large foundation models across various domains presents new opportunities for PEFT. Designing PEFT meth- ods tailored to the unique characteristics of models, such as Sora [257], Mamba [258], and LVM [259], can unlock new application scenarios and opportunities. F. Enhancing data privacy Trusting centralized systems to serve or fine-tune personal- ized PEFT modules is yet another issue for system developers. Multiple types of inversion attacks [260], [261] have been pro- posed to reconstruct user’s data by hijacking the intermediate"
            },
            {
                "section": "results",
                "content": ". One perspective of future trust-worthy LLM system design involves developing an encryption protocol for both personal data and intermediate training and inference results. 20 G. PEFT with model compression Model compression is one of the most effective ways to make LLM executable on resource-limited devices. Yet, the impact of model compression techniques on the performance of PEFT algorithms running on hardware remains another systemic challenge. Common compression techniques such as quantization and pruning necessitate dedicated hardware platforms to expedite the process, and building such hardware platforms for compressed models is yet another direction for future research."
            }
        ]
    },
    "Paper_12": {
        "title": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models Jiajun Zhou1,3† Yifan Yang1, Kai Zhen2, Ziyue Liu1, Yequan Zhao1, Ershad Banijamali2, Athanasios Mouchtaris2, Ngai Wong3, Zheng Zhang1 1University of California, Santa Barbara, 2Amazon AGI",
        "sections": [
            {
                "section": "Abstract",
                "content": "Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantiza- tion often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimiza- tion require backpropagation, which are error- prone in the low-precision settings. To over- come these limitations, we propose the Quan- tized Zeroth-Order (QuZO) framework, specif- ically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low- precision straight-through estimator, and uti- lizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the train- ing process, while achieving results compara- ble to first-order methods in FP8 and superior accuracy in INT8 and INT4 training. Experi- ments demonstrate that low-bit training QuZO achieves performance comparable to MeZO op- timization on GLUE, Multi-Choice, and Gen- eration tasks, while reducing memory cost by 2.94× in LLaMA2-7B fine-tuning compared to quantized first-order methods. 1"
            },
            {
                "section": "Introduction",
                "content": "Large Language Models (LLMs) have achieved state-of-the-art performance in natural language processing, impacting various science and engi- neering fields. However, deploying and fine-tuning LLMs consumes significant hardware resources because of their huge model size. To address this issue, extensive research has focused on LLM quantization (Brown et al., 2020; Yuan et al., 2024). Notable approaches include post-training quantization (Yao et al., 2022; Wu et al., 2023), quantization-aware training (Bhalgat et al., 2020; Liu et al., 2023c; Nagel et al., 2021), and fully quan- tized training (Choukroun et al., 2019; Xi et al., †Work undertaken during the visit at UC Santa Barbara 2023; Markidis et al., 2018). Post-training quantiza- tion can effectively reduce the latency and memory costs of inference, but often leads to a significant accuracy drop in low-precision formats, although various techniques (Shao et al., 2023; Xiao et al., 2023; Lin et al., 2023; Liu et al., 2023c) can par- tially mitigate this issue. Quantization-aware train- ing (Liu et al., 2023a) offers better accuracy, but is more expensive due to the use of high-precision computational graphs. Truly quantized training"
            },
            {
                "section": "methods",
                "content": "employ low-precision gradients, activa- tion, and weights to reduce hardware costs (Wang et al., 2018b; Banner et al., 2018; Micikevicius et al., 2017). However, implementing truly quan- tized training requires advanced hardware and soft- ware support for both forward and backward passes. Meanwhile, the straight-through estimator (Yin et al., 2019), which is commonly used for quan- tized gradient estimations, often causes unstable and inaccurate results in low-bit training. In practice, LLM users may afford only a low- cost LLM inference engine (e.g., an edge FPGA or embedded system) with limited precision (e.g., INT8 or INT4). This paper asks the following question: Can we leverage inference-only quan- tized hardware to fine-tune low-bit LLMs while achieving good performance? This seems chal- lenging because (1) inference-only hardware lacks sufficient memory bandwidth and storage to retain intermediate activations required for backpropaga- tion, and (2) the Straight-Through Estimator (STE) introduces increasing gradient approximation er- rors in lower-bit formats (Malinovskii et al., 2024). The recent MeZO (Malladi et al., 2024) enables memory-efficient zeroth-order (ZO) fine-tuning for LLMs, but suffers from an avoidable performance drop compared to first-order (FO) methods due to the bias and variance of ZO gradient estimation. In this paper, we show that a quantized zeroth-order optimizer (QuZO) can achieve better accuracy than its first-order counterparts in a low-precision set- 1 arXiv:2502.12346v1  [cs.LG]  17 Feb 2025 FP32 FP16 FP8 INT8 INT4 Datatypes 80 85 90 95 Accuracy (%) Quantized Zeroth-order vs. First-order Fine-tuning First-Order (FO) QuZOSGD QuZOAdam Figure 1: The proposed QuZO provides higher fine- tuning accuracy than first-order (FO) methods in ultra- low precision on the RoBERTa-Large model. ting. Fig. 1 shows that both the QuZO and FO meth- ods experience accuracy drops as the quantization precision decreases, which is well expected. How- ever, QuZO consistently outperforms FO methods when the quantization precision is INT8 or below. Unlike traditional FO quantized training that de- pends on error-prone STE (Yin et al., 2019), our QuZO optimizer is more resistant to quantization error. Our contributions are summarized below. • We identify the challenge of naive quantized ZO training, and propose a stochastic quantized per- turbation method with theoretical soundness to reduce bias in quantized ZO gradient estimation. • We introduce the implementation of QuZO as a plugin that integrates seamlessly with a quantized LLM inference engine, enabling accurate fine- tuning of low-bit LMs without backpropagation. • We provide detailed numerical analysis about the proposed gradient estimator and the QuZO training framework. We show the benefit of our quantized ZO gradient estimator and the better training behavior of QuZO in low-bit LLM fine- tuning (especially INT4-format trainig). • We apply QuZO to fine-tune 4/8-bit LLMs us- ing both full-model fine-tuning and Low-Rank Adaptation (LoRA). QuZO achieves much better accuracy than quantized first-order training while reducing the memory cost by 1.4 × −2.94×. 2 Related Work Zeroth-order method. Zero-order (ZO) opti- mization techniques use forward passes for gra- dient estimation. Since backpropagation is not re- quired during training, ZO methods reduce mem- ory consumption significantly compared to a FO method. MeZO (Malladi et al., 2024) employed a memory-efficient ZO stochastic gradient de- scent (ZO-SGD) algorithm to efficiently fine-tune LLMs exceeding 60 billion parameters, leverag- ing parameter-efficient approaches (Yang et al., 2024b; Liu et al., 2021) like LoRA (Hu et al., 2021). Other ZO methods include ZO-SGD (Ghadimi and Lan, 2013) and ZO-Sign-SGD (Liu et al., 2018) using sign-based gradient estimation, the ZO-Adam (Chen et al., 2019) optimizer exploiting momentum information, and parameter-efficient"
            },
            {
                "section": "methods",
                "content": "like AdaZeta (Yang et al., 2024a). Sparse MeZO (Liu et al., 2024) employs a sparse per- turbation for LLM fine-tuning. FP16 ZO train- ing (Zhang et al., 2024) performs well but still faces memory bottlenecks. Recent ZO quantization in- troduces fixed-point 16-bit but fails at 8-bit (Feng et al., 2024). However, we overcome the challenges of lower-precision quantization and enable accurate fine-tuning of LLMs below 8-bit quantization. Quantization of LLMs. Various quantiza- tion methods have been developed to reduce the memory and computing cost of LLMs. LLM.int8() (Dettmers et al., 2022) reduces the precision of model weights while keeping outliers in FP16. SmoothQuant (Xiao et al., 2023) introduces a fine-grained quantization method that supports INT8 operations exclusively. QLLM (Liu et al., 2023a) addresses the outlier problem via employing an adaptive channel reassembly technique. LLM-QAT (Liu et al., 2023c) employs quantization-aware training with a data-free strategy to achieve 4-bit quantization. Furthermore, the 4-bit training (Xi et al., 2023) and QLoRA (Dettmers et al., 2024) methods leverage a Hadamard Transform and a novel NF4 datatype, respectively, to accelerate training while preserving performance. While prior quantized training methods rely on backpropagation for gradient updates, our QuZO method eliminates the error-prune STE-based back propagation and used low-bit inference for truly quantized fine-tuning. 3 The QuZO Fine-Tuning Method We start with a high-level introduction to our QuZO framework. Given a quantized LLM inference model, QuZO uses a low-bit ZO optimizer to update quantized model parameters directly dur- ing training. We assume that the forward pass xl = F(xl−1, ¯wl) computes the output of the l- th layer using the quantized weight matrix ¯wl and the previous-layer feature xl−1, as shown in Fig. 2 (b). With just a few forward passes, our QuZO framework uses quantized RGE (see Section 3.2) 2 Weights Low-precision Weights Gradient Updates Quantization Update Inputs Backward with STE STE (a) Quantized First-Order (FO) Training Weights Low-precision Weights Gradient Updates Quantization Update Inputs Quantized RGE (b) Quantized Zeroth-Order (ZO) Training No Backward Figure 2: Computational graphs for quantized first-order (FO) and zeroth-order (ZO) training. to estimate ZO gradients, eliminating the need for BP in model updates. This approach fundamen- tally differs from existing quantized training meth- ods shown in FigFig. 2 (a), which uses STE in the BP to approximate quantized gradient ∂L( ¯w) ∂¯wl . Our method avoids the straight-through estimator (STE) (Yin et al., 2019) used in truly quantized FO training, enabling high-accuracy training on a low-precision hardware platform. In the following, we first show the challenges of ZO-SGD in the quantized setting, and then propose a solution to address this fundamental challenge. 3.1 Challenges of Quantized ZO Training Standard ZO-SGD uses a randomized gradi- ent estimator (RGE) (Nesterov and Spokoiny, 2017; Ghadimi and Lan, 2013) to approximate a full-precision gradient. Specifically, given full- precision model parameters w ∈Rd, a loss func- tion L(w, B) and a minibatch of dataset B, RGE computes the gradient as: ∇ˆL(w) = n X i=1 LB(w + ϵui) −LB(w −ϵui) 2nϵ ui ≈1 n n X i=1 uiuT i ∇LB(w), (1) where ϵ is a scaling factor, {ui}n i=1 are i.i.d. samples drawn from certain distributions with a unit variance (e.g., a standard Gaussian distribu- tion). While ∇ˆL(w) differs from the true gradient ∇LB(w), its expectation serves as a good gradient estimator because E h ∇ˆL(w) i ≈1 n n X i=1 E \u0000uiuT i \u0001 ∇LB(w) = ∇LB(w). (2) This statistical property ensures the asymptotical convergence of ZO-SGD. Assuming the quantized model parameters ¯w are available and only low- precision hardware is used for inference, the full- precision random perturbation ui cannot be directly applied to ¯w due to hardware limitations. To ad- dress this, ui is replaced with its quantized counter- part ˆui = Q(ui), leading to a low-precision RGE: ∇ˆL( ¯w) = n X i=1 LB ( ¯w + ϵˆui) −LB ( ¯w −ϵˆui) 2nϵ ˆui ≈1 n n X i=1 ˆuiˆuT i ∇LB( ¯w). (3) Taking the exception values on both sides, we have E h ∇ˆL( ¯w) i ≈1 n n X i=1 E \u0000ˆuiˆuT i \u0001 ∇LB( ¯w) ̸= ∇LB( ¯w) (4) Since the quantized perturbation ˆui = Q(ui) no longer maintains a unit variance, the above naive quantized RGE introduces bias during fine-tuning and may lead to divergence in training. 3.2 Proposed Quantized RGE We propose a new quantized RGE scheme to ad- dress the challenge in the previous subsection. Stochastic Quantization of ui. We first define a quantization operation of Q(ui) based on stochas- tic rounding (Connolly et al., 2021): Q(ui) = clamp \u0010 SQ, Lmin, Lmax \u0011 + z0, SQ = \u0010 ⌊suui⌋+ Ber(suui −⌊suui⌋) (5) The stochastic quantization formula Q(ui) con- verts the perturbation ui into a low-bit representa- tion by scaling it with a factor su as suui, perform- ing a downward rounding operation ⌊suui⌋, and applying stochastic up-rounding using a Bernoulli random variable Ber(suui −⌊suui⌋). The result- ing quantized value is clamped to the representable range [Lmin, Lmax] and shifted by the zero point z0. This stochastic rounding ensures that EQ [Q(ui)] = E [ui] . (6) 3 We can produce two different quantization re- sults by using two random seeds in the stochastic rounding full-precision ui: ui,1 = Q1(ui) = Q(ui) with random seed i1; ui,2 = Q2(ui) = Q(ui) with random seed i2; ui,1 ̸= ui,2. (7) The above stochastic quantizations ensure that (1) the expectation of the quantized perturbations ui,1 and ui,2 equals the original perturbation ui, (2) ui, 1 and ui,2 are conditionally independent to each other. As a result, we have EQ1(ui,1) = EQ2(ui,2) = ui, EQ1,Q2(ui,1uT i,2) = EQ1(ui,1)EQ2(uT i,2) = uiuT i . Our Quantized RGE. With the two condition- ally independent quantized vectors ui,1 and ui,2 defined in Eq. (7), we propose the following quan- tized RGE: ∇ˆL( ¯w) = nP i=1 LB( ¯w+ϵui,1)−LB( ¯w−ϵui,1) 2nϵ ui,2 (8) As ϵ →0, the RGE result is ∇ˆL( ¯w) ≈1 n n X i=1 ui,1uT i,2∇LB( ¯w). (9) The estimation results depend on three random vec- tors and functions: ui, Q1 and Q2. Taking expecta- tion values on both sides of Eq. (9), we have E h ∇ˆL( ¯w) i ≈1 n n X i=1 Eui,Q1,Q2 \u0002 ui,1uT i,2 \u0003 ∇LB( ¯w) = 1 n n X i=1 Eui \u0002 EQ1,Q2 \u0002 ui,1uT i,2 \u0003\u0003 ∇LB( ¯w) = 1 n n X i=1 E \u0000uiuT i \u0001 ∇LB( ¯w) = ∇LB( ¯w). (10) The expectation value of our quantized RGE re- mains a reliable estimator of the true gradient, which is similar to the full-precision RGE. This indicates that our proposed RGE will ensure asymp- totical convergence as in a full-precision ZO method. This theoretical property ensures excellent training performance even in low-precision settings (e.g. INT8 and INT4). 3.3 Implementation of QuZO Now we present the details of the implementation of the QuZO framework. Quantized Model Updates. Recall that in full- precision ZO-SGD, the gradient is computed in (1), and the model parameters are updated as wt+1 = wt −ηt · ∇ˆL(wt) (11) where wt represents the model parameters at itera- tion t, ηt is the learning rate and ∇ˆL(wt) denotes the estimated gradient of the loss function. Since wt ≈sw ¯wt, and sw is a scaling factor used in the quantization ¯wt = Q (wt/sw), with Q[·] repre- senting the stochastic quantization applied to the parameters. This approximation suggests: wt+1 ≈sw h ¯wt −ηt · ∇ˆL( ¯wt) i (12) To achieve a truly quantized training process suit- able for low-precision hardware, the model param- eters are updated as: ¯wt+1 = ¯wt −Q h ηt · ∇ˆL( ¯wt) i . (13) To refine the update process, multiple steps can be used. For each query i, we compute µi = LB( ¯w + ϵui,1) −LB( ¯w −ϵui,1) 2ϵ . (14) Then the quantized model ¯ W is updated as ¯wt+1 = ¯wt − n X i=1 Q \u0010ηtµi n ui,2 \u0011 . (15) Here ui,2 is a second quantized version of ui as explained in Eq. (7). Stochastic rounding Q[·] en- sures that no additional bias will be introduced when we update the LLM parameters directly at low precision. Algorithm Flow. The pseudo codes of QuZO are summarized in Algorithm 1. For each query i, two forward passes are performed to determine the sensitivity (µi) of the loss function with respect to a quantized perturbation direction ui,1 (lines 5-11). The resulting low-precision gradient asso- ciated with each inquiry is obtained by quantiz- ing a scaled version of ui,2, where the sensitivity (µi), the learning rate ηt, and the sample size n are taken into account. This low-precision ZO gradient allows us to directly update the quantized LLM model parameters with low-precision hardware. 4 Algorithm 1 QuZO: Quantized Zeroth-Order Training Require: LLM model parameters w ∈Rd, learning rate ηt, T is the step, perturbation scaling factor ϵ and dataset B. 1: Initial Pre-trained Model to Quantized Model or directly load a quantized model. 2: ¯w = Q(w) \u0001 Optionally, quantize the model if starting with a full-precision model 3: for t in T do 4: for i in n do 5: ui,1 ←Q1(ui), ui ∼N(0, Id) \u0001 Quantize the perturbation ui with a random seed i1 6: ui,2 ←Q2(ui) \u0001 Quantize the perturbation ui with a random seed i2 7: ¯wt ←¯wt + ϵ · ui,1 \u0001 Low-bit stochastic perturbation updates ¯wt using positive scaling 8: Li 1 ←F( ¯wt, B) \u0001 First zeroth-order forward pass 9: ¯wt ←¯wt −2ϵ · ui,1 \u0001 Low-bit stochastic perturbation updates ¯ wt using negative scaling 10: Li 2 ←F( ¯wt, B) \u0001 Second zeroth-order forward pass 11: µi ←(Li 1 −Li 2)/(2ϵ) \u0001 Sensitivity w.r.t. the quantized perturbation 12: ¯wt ←¯wt + ϵ · ui,1 \u0001 Recover ¯wt to its original state 13: ¯wt+1 ←¯wt −Q( ηtµi n ui,2) \u0001 Quantized LLM model update 14: end for 15: end for 16: return ¯w \u0001 Return a quantized model QuZO for LoRA. We can extend the QuZO framework by incorporating low-rank adaptation to allow low-precision parameter-efficient fine- tuning. Our approach uses the model quantization strategies of QLoRA (Dettmers et al., 2024) and LLM.int8() (Dettmers et al., 2022) without modi- fying the quantized model. QuZO significantly re- duces memory overhead by eliminating the storage of FO optimizer states and updating only the low- rank trainable matrices A ∈Rd×r and B ∈Rr×d using forward passes. In QuZO fine-tuning, the model parameters are quantized and frozen at low precision (e.g. 4 or 8 bits), and we update solely on the low-rank matrices A and B. The trainable low-rank matrices are quantized (denoted as Q[A] and Q[B]) in order to match the precision of the LLM . By doing so QuZO training can significantly further reduce the memory cost compared to tradi- tional LoRA for 4/8-bit LLM fine-tuning. 3.4 QuZO Analysis In this subsection, we analyze the quality of gradi- ent estimation in QuZO and its impact to training. QuZO Gradient Quality. We use a simple encoder-block transformer to analyze the asymp- totic behavior of two quantized ZO gradient esti- mators. Q-RGE1 refers to the quantized estimate in Eq. (3), and Q-RGE2 denotes our proposed es- timation in Eq. (8). Although we need only a few inquiries to compute actual ZO gradients, the sta- tistical behavior of a gradient (rather than the value of the individual gradient) decides the training per- formance. To verify statistical asymptotic behavior, we set n = 1000 to perform a Monte Carlo com- putation to get empirical mean values of Q-RGE1 and Q-RGE2, and then compare them with a full- INT8 INT4 INT3 Quantization Precision 0.0 0.5 1.0 1.5 Gradient Error Q-RGE2 Q-RGE1 0 3000 6000 Training Step 1.3 2.0 3.5 Training Loss Q-RGE1INT4 Q-RGE1INT8 Q-RGE2INT4 Q-RGE2INT8 (a) (b) Figure 3: (a) Errors of quantized gradient estimation Q- RGE1 in Eq. (3) and our proposed Q-RGE2 in Eq. (8). (b) Training loss of low-precision ZO optimizer with these two quantized gradient estimators, respectively. precision ZO gradient via the ℓ2 error. As shown in Fig. 3 (a), the expected values of both quantized estimators have larger errors as the precision re- duces from INT8 to INT3. However, our method (Q-RGE2) is much more resilient to quantization er- rors and has a more accurate expected value, since our quantized ZO gradient estimator can avoid the additional bias caused by quantization. Training Behavior. Figure 3 (b) further shows the training behavior of quantized ZO optimiza- tion using these two gradient estimators when fine- tuning the OPT-1.3B model. Experiments are per- formed on the DROP dataset under 8-bit and 4- bit settings. We observe that our QuZO with Q- RGE2 shows slightly better convergence compared to quantized training using Q-RGE1 in the 8-bit setting. In 4-bit training, our method demonstrates a stable and significantly better training behavior: it achieves a loss similar to 8-bit training, while INT 4 Q-RGE1 causes convergence failures. The above analysis clearly demonstrates the better nu- merical performance of our QuZO in low-bit LLM fine-tuning. 5 FP8 INT8 INT4 80 85 90 95 Accuracy (%) SST-2 FO LLM-QAT QuZO FP8 INT8 INT4 40 56 73 90 SNLI FO LLM-QAT QuZO FP8 INT8 INT4 Data types 20 33 46 60 SST-5 FO LLM-QAT QuZO FP8 INT8 INT4 50 63 76 90 RTE FO LLM-QAT QuZO FP8 INT8 INT4 50 61 73 85 MNLI FO LLM-QAT QuZO Figure 4: Experimental findings on RoBERTa-large (350M parameters) with prompts reveal that QuZO, leveraging full-parameter tuning, starts to surpass FO and LLM-QAT as precision reduces to INT8 or below. Table 1: QuZO demonstrates superior performance in full-parameter fine-tuning of LLaMa-2 7B. Note : WaAb quantization configurations, which refer to a-bit weight quantization and b-bit activation quantization. LLaMa-2 7B Model Classification Multiple-Choise Generation Data Precision Method SST-2 RTE WSC MultiRC COPA ReCoRD SQuAD DROP FP FO 95.41 63.73 63.46 65.10 86.00 81.00 90.71 51.38 W32A32 MeZO 94.80 54.60 58.80 62.60 82.70 70.80 72.50 46.80 FP FO 91.63 63.90 49.00 58.00 79.00 72.50 72.68 23.46 W8A8 QuZO 91.05 55.59 65.38 57.10 80.00 76.80 76.38 30.17 FO 90.81 52.34 61.53 50.60 62.00 74.83 70.13 20.06 INT SmoothQuant 91.05 66.78 59.51 61.50 72.02 79.10 73.07 29.94 W8A8 LLM.int8() 88.04 62.56 57.75 55.61 80.02 80.61 76.34 20.15 QuZO 92.00 61.01 63.46 60.00 81.00 79.00 77.71 30.11 FO 86.35 47.29 60.57 51.90 62.04 73.21 30.01 10.06 INT/FP MinMax 88.1 59.91 41.28 53.21 82.51 80.97 50.07 24.71 W4A8 LLM-FP4 91.20 66.82 61.38 58.81 82.90 81.25 51.07 24.99 QuZO 91.62 54.87 62.28 60.60 80.01 78.20 68.12 21.80 4 Experiments In this section, we evaluate the proposed QuZO method on several language models (LMs) with 4-8 bit precision. QuZO demonstrates performance comparable to or better than standard first-order (FO) truly quantized training across various model sizes and tasks, with significantly lower memory usage. We also explore fine-tuning quantized mod- els by combining QLoRA (Dettmers et al., 2024) with QuZO. For hardware costs, QuZO employs a forward-only framework with hardware require- ments similar to post-training quantization. In Sec- tion 4.3, we compare the memory consumption between truly quantized FO training and QuZO. Furthermore, we employ both medium-size models (e.g. RoBERTa-Large (Liu et al., 2019)) and large decoder-based LMs [e.g. OPT 1.3B (Zhang et al., 2022) and LLaMa-2 7B (Touvron et al., 2023)] in few-shot settings. All experiments were carried out on NVIDIA A100-40GB GPUs. The details of the experimental setup are provided in Appendix A. 4.1 Full-Parameter Quantized Fine Tuning We first summarize our experiments on full- parameter fine-tuning for medium- and large-scale models. These results demonstrate that QuZO pro- vides a practical approach for accurate fine-tuning of quantized LLMs directly on low-precision hard- ware, maintaining. For medium-scale models like RoBERTa-Large, QuZO surpasses truly quantized FO fine-tuning in most tasks in the 4-bit precision. For large-scale models such as LLaMA-2, QuZO achieves performance comparable to or better than truly quantized FO fine-tuning, particularly under ultra-low bit configurations. These findings high- light the ability of QuZO to enable low-cost hard- ware training without compromising performance. Performance on the RoBERTa-Large model. We evaluate the performance of various methods in the SST-2, SNLI, SST-5, RTE, and MNLI datasets and on the RoBERTa-Large model. The results in Fig. 4 leads to the following observations: • As expected, all training methods experience accuracy decline as quantization precision de- creases. This occurs because the model expres- sive power declines and the optimization be- comes more challenging in lower precision. • The performance of truly quantized FO fine- tuning drops most significantly due to the increas- ing errors in the straight-through estimators as precision decreases. • Quantization-aware training (QAT) can mitigate partially the accuracy drop of truly quantized FO training. As a faked quantized training, QAT 6 Table 2: Results of parameter-efficient fine-tuning of quantized LLMs. Model"
            },
            {
                "section": "Methods",
                "content": "Gradient SST-2 MultiRC ReCoRD SQuAD DROP 8bit LLaMa2-7B FO (LoRA) INT8 91.97 45.60 80.10 56.13 20.93 MeZO (LoRA) FP32 88.17 63.60 80.60 86.96 37.23 QuZO (LoRA) INT8 90.36 60.00 80.80 79.97 36.11 8bit OPT-1.3B FO (LoRA) INT8 91.97 55.30 70.50 71.92 18.35 MeZO (LoRA) FP32 89.56 57.30 70.80 68.11 18.53 QuZO (LoRA) INT8 88.76 55.90 70.20 66.53 23.17 4bit OPT-1.3B FO (LoRA) INT4 53.89 55.55 17.20 28.20 10.00 MeZO (LoRA) FP32 90.48 54.10 69.50 70.93 20.53 QuZO (LoRA) INT4 87.84 56.20 68.90 68.13 21.92 still needs backpropagation and updates LLM model parameters in full precision. Therefore, it remains memory-intensive and less suitable for resource-constrained low-precision hardware. • In contrast, the performance of QuZO is most re- silient to the decreased precision, and it works the best in a very low-precision (e.g., INT4). This is because (1) QuZO can bypass the error- prone straight-through estimator that is used in truly quantized FO training, and (2) the quantized RGE in Eqn.(8) can eliminate the bias caused by quantized perturbations. Performance of QuZO on LLaMA Models. We further apply QuZO to fine-tune the LLaMa-2 model, evaluating it on SuperGLUE (Wang et al., 2019) and generation tasks. Table 1 shows that QuZO outperforms its truly quantized FO coun- terparts on all multichoice and generation tasks under FP W8A8 quantization (i.e. FP8 for both weights and activations). Under the INT W8A8 quantization, QuZO outperforms SmoothQuant, LLM.int8(), and truly quantized FO methods in 4 out of 8 tasks. For 4-bit quantized FO train- ing, uniform quantization yields the worst accu- racy, but advanced methods such as LLM-FP4 im- prove performance. LLM-FP4 (Liu et al., 2023b) and its baseline MinMax use FP W4A8 quanti- zation and achieve a slight improvement in accu- racy, particularly for multichoice tasks. Our QuZO method maintains strong performance under W4A8 quantization with mixed-datatype (see Appendix B), achieving the best results in 4 out of 8 tasks. SmoothQuant, LLM.int8(), MinMax, and LLM- FP4 have introduced efficient quantization meth- ods that enhance performance. However, they are memory-intensive as they require fine-tuning using a FO optimizer. Table 3: Performance Comparison of QuZO on the LLaMa-2 13B Model Model (#Bit)"
            },
            {
                "section": "Methods",
                "content": "ReCoRD SQuAD DROP FO 81.70 63.23 25.90 LLaMa2-13B MeZO 82.10 63.71 25.20 (8-Bit) QuZO 82.20 78.19 37.61 FO 82.00 62.27 25.31 LLaMa2-13B MeZO 82.30 62.62 25.33 (4-Bit) QuZO 82.10 73.79 27.32 4.2 Parameter-Efficient Fine-Tuning Parameter-efficient fine-tuning"
            },
            {
                "section": "methods",
                "content": "like QLoRA (Dettmers et al., 2024) reduce memory usage with 4-bit precision compared to standard training but still rely on AdamW (Loshchilov, 2017), which requires backpropagation. QuZO improves inference efficiency and memory savings, achieving a 5.47× reduction in maximum memory cost compared to QLoRA in fine-tuning the 4-bit OPT-1.3B model (details in Appendix C). Our QuZO framework applies the LoRA (rank set as 8), allowing fine-tuning with far fewer train- able parameters than full-model tuning, signif- icantly reducing memory consumption, and ac- celerating convergence. Table 2 highlights the performance of QuZO with low-bit perturbation and gradient configurations for different tasks and models. For the OPT-1.3B model, QuZO utilizes INT8 RGE gradients with INT4 perturbations. De- spite the introduction of low-bit gradients, QuZO achieves competitive or superior performance com- pared to full-precision MeZO with LoRA in most tasks and demonstrates strong robustness in 4-bit fine-tuning, while truly quantized FO shows poor accuracy in 4-bit training. Furthermore, QuZO re- duces 2 −5.47× memory consumption compared to fully quantized FO methods in Table 15(see Appendix C). For the LLaMa2-7B model, QuZO achieves performance comparable to full-precision MeZO while allowing fine-tuning across all five 7 0 10 20 30 Batch size 5000 10000 15000 20000 25000 30000 Memory (MB) Sequence Length = 512 FO(FP16)-1.3B FO(INT8)-1.3B FO(FP16)-2.7B FO(INT8)-2.7B QuZO(INT8)-1.3B QuZO(INT8)-2.7B 0 10 20 30 Batch size 5000 10000 15000 20000 25000 30000 Sequence Length = 1024 FO(FP16)-1.3B FO(INT8)-1.3B FO(FP16)-2.7B FO(INT8)-2.7B QuZO(INT8)-1.3B QuZO(INT8)-2.7B Figure 5: Peak memory usage of FP16 and INT8 train- ing on the OPT 1.3B/2.7B model with sequence lengths of 512 (left) and 1024 (right). tasks on a single GPU. In contrast, truly quantized FO methods encounter out-of-memory (OOM) is- sues. This result highlights that the low-bit stochas- tic perturbation of QuZO effectively reduces mem- ory overhead while mitigating the bias caused by quantization errors, making accurate fine-tuning feasible on resource-constrained hardware. Fine-Tuning 13B LMs. Table 3 presents the per- formance comparison of QuZO fine-tuning against other methods with LoRA, including First-Order (FO) and MeZO, on the LLaMa-2 13B model un- der 8-bit and 4-bit quantization. The evaluation is conducted on three datasets: ReCoRD, SQuAD, and DROP, which assess reading comprehension and reasoning ability. The results indicate that QuZO consistently outperforms MeZO and FO, particularly in SQuAD and DROP, demonstrating its ability to better retain performance in a quan- tized setting. In the 8-bit setting, QuZO achieves a significant improvement. In the 4-bit setting, the trend remains similar, highlighting the robustness of QuZO in handling more aggressive quantization. 4.3 Memory Efficiency We follow Table 5 of (Zhang et al., 2024) to pro- vide the theoretical analysis for different optimiz- ers. As shown in Table 4, our QuZO demonstrates significant memory reduction compared to truly quantized FO fine-tuning at the same precision. We further compare the empirical memory costs of full fine-tuning the LLaMA-2 7B model in Ta- ble 5. Specifically, in the MultiRC task, QuZO (8- bit) reduces memory usage by 1.43×, while QuZO (4-bit) achieves a 1.39× reduction compared to their truly quantized FO counterparts with the same precision. Similarly, in the SQuAD task, QuZO (8- bit) reduces memory consumption by 2.94×, and Table 4: Comparison of peak memory consumption dur- ing full-model fine-tuning. Note: model storage (Weight Mem.) and dynamic allocations for gradients (Dynamic Mem.). |w| and |a| denote memory usage for model parameters and intermediate parameters, respectively, with l representing a specific layer. Method Weight Mem. Dynamic Mem Full Precision Optimizer FO-SGD |w| P l max {|a|, |w|} MeZO |w| maxl |w| Optimizer with Low Precision Model FO(8-bit) |w|/4 P l max {|a| 4 , |w| 4 } FO(4-bit) |w|/8 P l max {|a| 8 , |w| 8 } QuZO(8-bit) |w|/4 maxl |w| 4 QuZO(4-bit) |w|/8 maxl |w| 8 Table 5: Total memory consumption (GB) for different optimizers on LLaMa-2 7B. Method MultiRC (GB) SQuAD (GB) FO-SGD (8-bit) 11.66 21.29 FO-SGD (4-bit) 6.28 10.73 QuZO (8-bit) 8.15 7.24 QuZO (4-bit) 4.52 3.71 QuZO (4-bit) achieves a 2.89× reduction relative to FO-SGD at the same precision. To verify hardware efficiency, we profile the memory usage of our QuZO method with INT8 CUDA kernels, comparing it to the peak mem- ory consumption of INT8 and FP16 tensor-core GEMM implementations in full parameter tuning. In practice, QuZO achieves up to a 7.8× memory reduction with an INT8 model compared to the first-order FP16 trainning, as shown in Fig 5. 5"
            },
            {
                "section": "Conclusion",
                "content": "This work has proposed a Quantized Zeroth-Order (QuZO) method for truly qantized training of LLMs without using back propagation. We have identified the challenge of quantized ZO training, and proposed an new quantized ZO gradient to miti- gate the bias in low-precision settings. Since QuZO can avoid the error-prone straight-trough estimator, it can achieve better performance than first-order truly quantized training in low-bit settings. The su- perior performance of QuZO in low-bit (e.g., INT8 and INT4) training has been shown by a variety of fine-tuning experiments on the OPT-1.3B and LLaMA2-7B models. Our QuZO method is intrin- sically hardware efficient for fine-tuning LLMs on low-bit resource-constrained hardware. 8 Limitations The presented QuZO method can significantly im- pact practical LLM deployment. We have not yet implemented the real quantized training framework using low-precision kernels during training, as this requires much engineering effort. For instance, adding a minimal hardware block to an LLM in- ference accelerator can enable resource-efficient fine-tuning, making on-device learning of LLMs accessible and affordable for many downstream users. Additionally, QuZO can greatly reduce the latency and energy cost of fine-tuning due to its ca- pability to directly use an ultra low-bit LLM infer- ence accelerator. This will enable the deployment of LLMs in many resource-constrained scenarios, such as autonomous systems and robots."
            },
            {
                "section": "Methods",
                "content": "SST-2 MultiRC ReCoRD SQuAD DROP 8-bit OPT 1.3B LLM.int8() 9.01 23.97 6.76 22.09 31.29 QuZO 3.43 12.61 4.82 7.50 16.42 4-bit OPT 1.3B QLoRA 4.76 18.15 4.42 20.48 27.23 QuZO 1.72 6.30 2.41 3.74 11.70 8-bit LLaMa-2 7B LLM.int8() 31.47 OOM 19.06 OOM OOM QuZO 9.94 25.11 13.04 16.69 31.66 memory consumption (in GB) across various tasks when fine-tuning quantized models using QuZO with LoRA (rank = 8). The methods compared include QuZO, LLM.int8(), and QLoRA. Notably, QuZO employs 4-bit perturbations to fine-tune the models, achieving significant memory savings com- pared to LLM.int8 and QLoRA. For instance, in the OPT1.3B-int4 model, QuZO reduces memory usage by approximately 2.8× on SST-2 (1.72 GB vs. 4.76 GB in QLoRA) and by 5.47× on SQuAD (3.74 GB vs. 20.48 GB in QLoRA). Similarly, for the OPT1.3B-int8 model, QuZO achieves a mem- ory reduction of 1.4× on MultiRC (12.61 GB vs. 23.97 GB in INT8 FO fine tuning). In the 8-bit LLaMa-2 7B model, while LLM.int8 encounters Out-of-Memory (OOM) errors on sev- eral tasks, QuZO successfully completes fine- tuning with substantial memory efficiency, using just 9.94 GB on SST-2 compared to 31.47 GB for LLM.int8—a reduction of 3.2×. These results highlight the ability of QuZO to fine-tune quantized models effectively with minimal memory over- head, leveraging 4-bit perturbations for substantial efficiency gains while maintaining compatibility with LoRA architectures. This positions QuZO as a practical choice for resource-constrained fine- tuning in large-scale NLP tasks. 16"
            }
        ]
    },
    "Paper_13": {
        "title": "Fine-Tuning Language Models with Just",
        "sections": [
            {
                "section": "Abstract",
                "content": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zeroth- order optimizer (MeZO), adapting the classical ZO-SGD method to operate in- place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12× memory reduction and up to 2× GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.1 1"
            },
            {
                "section": "Introduction",
                "content": "Fine-tuning pre-trained language models (LMs) has been the dominant methodology for solving many language tasks [28], adapting to specialized domains [42], or incorporating human instructions and preferences [73]. However, as LMs are scaled up [13, 72], computing gradients for backpropagation requires a prohibitive amount of memory – in our test, up to 12× the memory required for inference – because it needs to cache activations during the forward pass, gradients during the backward pass, and, in the case of Adam [52], also store gradient history (see Section 3.4 for a detailed analysis). As a result, while it is possible to run inference with a 30-billion (30B) parameter LM on a single Nvidia A100 GPU (with 80GB memory), backpropagation with Adam is feasible only for a 2.7B LM. Parameter-efficient fine-tuning methods (PEFT [46, 57, 54]) update just a fraction of the network ∗Equal contribution and corresponding authors. 1Our code is available at https://github.com/princeton-nlp/MeZO. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2305.17333v3  [cs.LG]  11 Jan 2024 SST-2 RTE CB BoolQ WSC WIC MultiRC Copa ReCoRD SQuAD DROP 10 30 50 70 90 Accuracy/F1 (%) Zero-shot ICL MeZO FT (12x memory) Figure 1: OPT-13B results with zero-shot, in-context learning (ICL), MeZO (we report the best among MeZO/MeZO (LoRA)/MeZO (prefix)), and fine-tuning with Adam (FT). MeZO demonstrates superior results over zero-shot and ICL and performs on par with FT (within 1%) on 7 out of 11 tasks, despite using only 1/12 memory. See Table 1 for detailed numbers and Figure 3 for memory profiling. parameters, but still need to cache many activations, because the tuned parameters are scattered throughout the model. In our tests, fine-tuning an OPT-13B model with full parameter tuning or PEFT requires 12× and 6× more memory than inference respectively. In-context learning (ICL [13]) has allowed solving many tasks with a single inference pass, during which the model processes labeled examples (demonstrations) in its context and then outputs a prediction on a test example. While this allows for quick adaptation of the model to specific use cases, current models allow a limited context size (and thus, limited demonstrations) and the performance is sensitive to the formatting and choice of demonstrations [60, 66]. ICL can slow with the number of demonstrations, and it often performs worse than fine-tuning of medium-sized models [13]. Backpropagation also cannot optimize non-differentiable criteria, which have gained popularity in fine-tuning LMs according to human preference scores or set safety standards [89, 73]. Typically, these adaptations involve expensive reinforcement learning from human feedback (RLHF [20]). A classical zeroth-order optimization method, ZO-SGD [88], uses only differences of loss values to estimate the gradients. Thus, in principle, the method can update neural networks with just forward passes, though naive implementation still doubles the memory overhead and classical lower bounds [69, 32] suggest that convergence slows linearly with model size. As such, ZO methods have been applied in deep learning settings to find adversarial examples or tune input embeddings [91, 90] but not to directly optimize large-scale models (see Liu et al. [61] for a survey). In this work, we propose a memory-efficient zeroth-order optimizer (MeZO), which adapts the classical ZO-SGD algorithm and reduces its memory consumption to the same as inference. We apply MeZO to fine-tune large LMs and show that, both empirically and theoretically, MeZO can successfully optimize LMs with billions of parameters. Specifically, our contributions are: 1. In MeZO, we adapt the ZO-SGD algorithm [88] and a number of variants to operate in-place on arbitrarily large models with almost no memory overhead (see Algorithm 1 and Section 2). 2. We conduct comprehensive experiments across model types (masked LM and autoregressive LM), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice, and generation). MeZO consistently outperforms zero-shot, ICL, and linear probing. Moreover, with RoBERTa-large, MeZO achieves performance close to standard fine-tuning within 5% gap; with OPT-13B, MeZO outperforms or performs comparably to fine-tuning on 7 out of 11 tasks, despite requiring roughly 12× less memory (Figure 1 and Section 3). In our implemen- tation, MeZO requires only half as many GPU-hours as Adam fine-tuning for a 30B model (see Appendix F.6). 3. We demonstrate MeZO’s compatibility with full-parameter tuning and PEFT (e.g., LoRA [46] and prefix-tuning [57]) in Section 3. 4. Further exploration showcases that MeZO can optimize non-differentiable objectives such as accuracy or F1 score, while still requiring only the same memory as inference (Section 3.3). 5. Our theory suggests that adequate pre-training ensures the per-step optimization rate (Theorem 1) and global convergence rate (Lemma 3) of MeZO depend on a certain condition number of the landscape (i.e., the local effective rank, see Assumption 1) instead of numbers of parameters. This 2 Algorithm 1: MeZO Require: parameters θ ∈Rd, loss L : Rd →R, step budget T, perturbation scale ϵ, batch size B, learning rate schedule {ηt} for t = 1, ..., T do Sample batch B ⊂D and random seed s θ ←PerturbParameters(θ, ϵ, s) ℓ+ ←L(θ; B) θ ←PerturbParameters(θ, −2ϵ, s) ℓ−←L(θ; B) θ ←PerturbParameters(θ, ϵ, s) ▷Reset parameters before descent projected_grad ←(ℓ+ −ℓ−)/(2ϵ) Reset random number generator with seed s ▷For sampling z for θi ∈θ do z ∼N(0, 1) θi ←θi −ηt ∗projected_grad ∗z end end Subroutine PerturbParameters(θ, ϵ, s) Reset random number generator with seed s ▷For sampling z for θi ∈θ do z ∼N(0, 1) θi ←θi + ϵz ▷Modify parameters in place end return θ result is in sharp contrast to existing ZO lower bounds [69, 32] suggesting that the convergence rate can slow proportionally to the number of parameters (Section 4). 2 Zeroth-order optimization Zeroth-order (ZO) optimizers have long been studied in the context of convex and strongly convex objectives. In the following, we first introduce a classical ZO gradient estimator, SPSA (Defini- tion 1 [88]) and the corresponding SGD algorithm, ZO-SGD (Definition 2). Then we describe MeZO, our in-place implementation that requires the same memory as inference in Section 2.1 and Algorithm 1. We highlight that SPSA can also be used in more complex optimizers, such as Adam, and we provide memory-efficient implementations for those algorithms too (Section 2.2). Consider a labelled dataset D = {(xi, yi)}i∈[|D|] and a minibatch B ⊂D of size B, we let L(θ; B) denote the loss on the minibatch. We introduce a classical ZO gradient estimate in this setting.2 Definition 1 (Simultaneous Perturbation Stochastic Approximation or SPSA [88]). Given a model with parameters θ ∈Rd and a loss function L, SPSA estimates the gradient on a minibatch B as b∇L(θ; B) = L(θ + ϵz; B) −L(θ −ϵz; B) 2ϵ z ≈zz⊤∇L(θ; B) (1) where z ∈Rd with z ∼N(0, Id) and ϵ is the perturbation scale. The n-SPSA gradient estimate averages b∇L(θ; B) over n randomly sampled z. SPSA requires only two forward passes through the model to compute the gradient estimate (for n-SPSA, each estimate requires 2n forward passes). As ϵ →0, the SPSA estimate can be understood as a rank-1 reconstruction of the gradient. During training, n can be treated as a hyperparameter and follow a schedule [11, 15], though in cursory experiments (Appendix A), n = 1 is the most efficient. 2The original SPSA algorithm [88] perturbs the model by 1/z and thus requires that z has finite inverse moments, precluding the choice of z as Gaussian. 1/z is very large with high probability for a zero-mean Gaussian z, so we adopt the standard in many theoretical [70, 32] and empirical [64] works and perturb the parameters by z with z as a Gaussian random variable. 3 We use n = 1 as the default. It is widely known that the SPSA estimate can be used to replace the backpropagation gradient in any optimizer such as SGD. Definition 2 (ZO-SGD). ZO-SGD is an optimizer with learning rate η that updates parameters as θt+1 = θt −η b∇L(θ; Bt) where Bt is the minibatch at time t and b∇L is the SPSA gradient estimate. 2.1 Memory-efficient ZO-SGD (MeZO) The vanilla ZO-SGD algorithm costs twice the memory of inference, as it needs to store z ∈Rd. We propose a memory-efficient implementation of ZO-SGD called MeZO, as illustrated in Algorithm 1. At each step, we first sample a random seed s, and then for each of z’s four uses in Algorithm 1, we reset the random number generator by s and resample the relevant entry of z. Using this in-place implementation, MeZO has a memory footprint equivalent to the inference memory cost. We note that Algorithm 1 describes perturbing each parameter separately, which may be time- consuming for large models. In practice, we can save time by perturbing an entire weight matrix instead of each scalar independently. This incurs an additional memory cost as large as the largest weight matrix; usually, this is the word embedding matrix (e.g., 0.86GB for OPT-66B). Storage Efficiency of MeZO. Parameter-efficient fine-tuning (PEFT) techniques fine-tune just a fraction of the network parameters and have thus been proposed as a way to reduce the storage costs of fine-tuned model checkpoints. Fine-tuning with MeZO reduces the storage cost of the resulting checkpoint far more than popular PEFT techniques (e.g., LoRA [46] and prefix tuning [57]). We reconstruct the MeZO trajectory using a single seed, which spawns step-wise seeds to sample z, and the projected_grad at each step.3 As such, for fine-tuning a 66B model, MeZO requires saving the seed plus 20,000 (steps) × 2 bytes, which is less than 0.1MB. LoRA fine-tunes 19M parameters and requires 38MB storage, and prefix tuning fine-tunes 6M parameters and requires 12MB storage. 2.2 MeZO extensions We note that SPSA is a popular ZO gradient estimator but not the only one. Many one-point gradient estimators have been proposed in past works [34, 87, 95], and using such estimators in place of SPSA would halve the training time. However, cursory experiments with one such promising estimator [113] reveal that these are not as efficient as SPSA when fixing the number of forward passes (Appendix B.5). As such, we implement MeZO with the SPSA estimator. MeZO can also be combined with other gradient-based optimizers, including SGD with momentum or Adam. Though naive implementation would require additional memory to store the gradient moment estimates, MeZO-momentum and MeZO-Adam alleviate such overhead by recomputing the moving average of the gradients using saved past losses and z (see Appendix B for a full discussion). We also note that all of the coordinates of the SPSA gradient estimate have the same scale, but deep Transformers can have gradients of different scales for each layer [59, 61]. As such, we draw inspiration from layerwise adaptive optimizers [109, 110] to design several MeZO variants. Cursory experiments showed that these algorithms are not more efficient (in terms of forward passes), but we nevertheless present them as potential optimizers for more complex objectives. See Appendix B. Forward Auto-Differentiation Note that z⊤∇L(θ; B) is a Jacobian-vector product (JVP), which can be computed in parallel with an inference pass with excess memory consumption equivalent to that of the largest activation in the network [40]. In this case, z must be stored on the GPU in order to construct the gradient estimate, so this procedure requires slightly more than two times the memory needed for inference. We analyze this algorithm in detail in Appendix D. Note that using a non-zero ϵ in SPSA, which is not possible through the JVP method, may boost generalization by promoting a sharpness-minimizing term. Past works (e.g., Baydin et al. [9]) have also studied JVP-based training but achieved limited empirical success. 3Note that this reconstruction requires no additional forward passes through the model and no access to the data used during fine-tuning, since projected_grad implicitly encodes this information. 4 SST-2 SST-5 SNLI MNLI RTE TREC 30 40 50 60 70 80 90 100 Accuracy (%) k=16 RoBERTa-large SST-2 SST-5 SNLI MNLI RTE TREC 30 40 50 60 70 80 90 100 k=512 RoBERTa-large Zero-shot MeZO (prefix) LP FT MeZO FT (LoRA) MeZO (LoRA) FT (prefix) Figure 2: Experiments on RoBERTa-large. We report zero-shot, linear probing (LP), and MeZO and fine-tuning (FT) with full parameter, LoRA, and prefix-tuning. MeZO outperforms zero-shot and LP and approaches FT (within 5% for k = 512) with much less memory. Detailed numbers in Table 18. 3 Experiments Preliminary experiments (Appendix A) show that MeZO only works when using prompts [13, 84, 35]. Past works [83, 67] have demonstrated how the inclusion of a suitable prompt ensures the fine-tuning objective is closely related to the pre-training one. In Section 4, we extend these ideas to show how using a simple prompt simplifies the fine-tuning optimization procedure, thereby enabling zeroth order methods to work efficiently. All experiments below use prompts detailed in Appendix E.2. All fine-tuning with backpropagation (FT) experiments follow convention and use Adam, though we also report results when performing FT with SGD in Appendix F. We conduct comprehensive experiments on both medium-sized masked LMs (RoBERTa-large, 350M [65]) and large autoregressive LMs (OPT-13B, 30B, 66B [112]) in few-shot and many-shot settings with prompts. We also explore both full-parameter tuning and PEFT including LoRA [46] and prefix-tuning [57] (see Appendix E.5 for details). We compare MeZO with zero-shot, in-context learning (ICL), linear-probing (LP), and fine-tuning with Adam (FT). MeZO uses substantially less memory than FT but requires significantly more training steps. We first show that MeZO improves substantially over zero-shot, ICL, and LP across model types, sizes, and task types. Moreover, MeZO performs comparably to FT over a number of tasks, while drastically reducing the memory cost by, for example, 12× on OPT-13B. Further experiments demonstrate that MeZO can optimize non-differentiable objectives, such as accuracy and F1 score (Section 3.3). We compare the memory consumption of ICL, FT, LP, and MeZO in Figures 3 and 4. 3.1 Medium-sized masked language models We conduct experiments with RoBERTa-large on sentiment classification, natural language inference, and topic classification tasks. We follow past works [35, 67] in studying the few-shot and many-shot settings, sampling k examples per class for k = 16 and k = 512 (details in Appendix E). We run MeZO for 100K steps and fine-tuning for 1000 steps, noting that one MeZO step is substantially faster than one fine-tuning step (see Appendix F.6 for a comparison). We summarize the results from Figure 2 and Table 18 below. MeZO works significantly better than zero-shot, linear probing, and other memory-equivalent"
            },
            {
                "section": "methods",
                "content": ". On all six diverse tasks, MeZO can optimize the pre-trained model and consistently perform better than zero-shot and linear probing. We also show for several tasks that MeZO can outperform another ZO algorithm, BBTv2 [90], by up to 11% absolute (Appendix F.4).4 With enough data, MeZO achieves comparable performance (up to 5% gap) to FT. MeZO achieves close-to-fine-tuning performance on k = 16, with some tasks only having 2% gaps. When using k = 512 data, the gap between MeZO and FT further reduced to within 5% across all tasks. MeZO works well on both full-parameter tuning and PEFT. Full-parameter tuning (MeZO) and PEFT (MeZO with LoRA and prefix-tuning) achieve comparable performance, while MeZO (prefix) 4BBTv2 can only train low-dimensional projected prefixes instead of the full model. 5 Task SST-2 RTE CB BoolQ WSC WIC MultiRC COPA ReCoRD SQuAD DROP Task type ———————— classification ———————— – multiple choice – — generation — Zero-shot 58.8 59.6 46.4 59.0 38.5 55.0 46.9 80.0 81.2 46.2 14.6 ICL 87.0 62.1 57.1 66.9 39.4 50.5 53.1 87.0 82.5 75.9 29.6 LP 93.4 68.6 67.9 59.3 63.5 60.2 63.5 55.0 27.1 3.7 11.1 MeZO 91.4 66.1 67.9 67.6 63.5 61.1 60.1 88.0 81.7 84.7 30.9 MeZO (LoRA) 89.6 67.9 66.1 73.8 64.4 59.7 61.5 84.0 81.2 83.8 31.4 MeZO (prefix) 90.7 70.8 69.6 73.1 60.6 59.9 63.7 87.0 81.4 84.2 28.9 FT (12x memory) 92.0 70.8 83.9 77.1 63.5 70.1 71.1 79.0 74.1 84.9 31.3 Table 1: Experiments on OPT-13B (with 1000 examples). ICL: in-context learning; LP: linear probing; FT: full fine-tuning with Adam. MeZO outperforms zero-shot, ICL, and LP across the board, and achieves comparable (within 1%) or better performance than FT on 7 out of 11 tasks. Task SST-2 RTE BoolQ WSC WIC SQuAD 30B zero-shot 56.7 52.0 39.1 38.5 50.2 46.5 30B ICL 81.9 66.8 66.2 56.7 51.3 78.0 30B MeZO/MeZO (prefix) 90.6 72.6 73.5 63.5 59.1 85.2 66B zero-shot 57.5 67.2 66.8 43.3 50.6 48.1 66B ICL 89.3 65.3 62.8 52.9 54.9 81.3 66B MeZO/MeZO (prefix) 93.6 66.4 73.7 63.5 58.9 85.0 Table 2: Experiments on OPT-30B and OPT-66B (with 1000 examples). We report the best of MeZO and MeZO (prefix). See Appendix F.2 for more results. We see that on most tasks MeZO effectively optimizes up to 66B models and outperforms zero-shot and ICL. sometimes outperforms MeZO. We also show in Appendix F.3 that the three variants converge at similar rates, agreeing with our theory in Section 4, which shows that MeZO converges at a rate independent of the number of parameters being optimized. We show additional results with more FT and MeZO variants in Appendix F.1. We see that (1) ZO-Adam sometimes outperforms ZO-SGD but is not consistent across tasks; (2) LP and then MeZO, as suggested for fine-tuning [53], can sometimes improve the performance. 3.2 Large autoregressive language models With the promising results from RoBERTa-large, we extend MeZO to the OPT family [112], on a scale of 13B (Table 1), 30B, and 66B (Table 2). We select both SuperGLUE [98] tasks5 (including classification and multiple-choice) and generation tasks. We randomly sample 1000, 500, and 1000 examples for training, validation, and test, respectively, for each datset. We run MeZO for 20K steps and fine-tuning for 5 epochs, or 625 steps, noting that each step of MeZO is substantially faster than fine-tuning (see Appendix F.6 for a comparison). Please refer to Appendix E for details. Table 1 yields the following observations. MeZO outperforms memory-equivalent methods and closely approaches fine-tuning results. We see that on a 13B-parameter scale, MeZO and its PEFT variants outperform zero-shot, ICL, and LP across almost all tasks. When comparing to FT, which costs 12× more memory (Section 3.4), MeZO achieves comparable (within 1%) or better performance on 7 out of the 11 tasks. MeZO exhibits strong performance across classification, multiple-choice, and generation tasks. We investigate MeZO on generation tasks, which are regarded as more intricate than classification or multiple-choice tasks. We evaluate on two question answering datasets, SQuAD [80] and DROP [31]. We use teacher forcing for training and greedy decoding for inference (details in Appendix E). Table 1 shows that, on all generation tasks, MeZO outperforms zero-shot, ICL, and LP, and achieves comparable performance to FT. Considering that many applications of fine-tuning LMs – including instruction tuning or domain adaptation – target generation tasks, our results underscore the potential of MeZO as a memory-efficient technique to optimize large LMs for realistic and exciting applications. 5We also include SST-2, which is a simple sentiment classification task that we use for development. 6 Model RoBERTa-large (350M) OPT-13B Task SST-2 SST-5 SNLI TREC SQuAD Zero-shot 79.0 35.5 50.2 32.0 46.2 Cross entropy (FT) 93.9 55.9 88.7 97.3 84.2 Cross entropy (MeZO) 93.3 53.2 83.0 94.3 84.7 Accuracy/F1 (MeZO) 92.7 48.9 82.7 68.6 78.5 Table 3: MeZO with non-differentiable objectives. For classification (k = 512), we use MeZO with full-parameter and optimize accuracy; for SQuAD (1,000 examples), we use MeZO (prefix) and F1. \b\f\u000f \b\u000e\u000f \b\u000e\u000f \u000f \u000f \u0004\u0017\u001b#\u001b \u001c%\u001c#$ \u0012\u0017\u0019\u0003\u0015\u001c !#'\u0003\u0005\u0012\u000f\u0006 \u001a\u001c#!\u0007$\u001e!% \u0013\u0010\u0014 \u0011\u0018 \u0011\u0018\u0007\"#\u001c\u001d\u001f& \u001a\u0016 \b\f\u000f \b\u000e\u000f \b\u000e\u000f \u000f \u000f \u0004\u0017\u001b#\u001b \u001c%\u001c#$ \u0012\u0017\u0019\u0003\u0015\u001c !#'\u0003\u0005\u0012\u000f\u0006 \u001a\u001c#!\u0007$\u001e!% \u0013\u0010\u0014 \u0011\u0018 \u0011\u0018\u0003\u0005\"#\u001c\u001d\u001f&\u0006 \u0015\u001c\u001a\u0016 7x 8x 11x 12x 11x Figure 3: GPU memory consumption with different OPT models and tuning methods on MultiRC (400 to- kens per example on average). Hardware Largest OPT that can fit FT FT-prefix Inference 1×A100 (80GB) 2.7B 6.7B 30B 2×A100 (160GB) 6.7B 13B 66B 4×A100 (320GB) 13B 30B 66B 8×A100 (640GB) 30B 66B 175B† Figure 4: Largest OPT models that one can tune with specific hardwares and algorithms. † : projected results without actual testing. MeZO scales up to 66 billion parameter models. We demonstrate the efficacy of MeZO on even larger models, up to 66B, in Table 2. While directly fine-tuning models at such scales is extremely costly (Section 3.4), MeZO can effectively optimize these models and outperform zero-shot and ICL. 3.3 Training with non-differentiable objectives We demonstrate the efficacy of MeZO for optimizing non-differentiable objectives through initial experiments. Accuracy and F1 are used as the respective objectives (details in Appendix E.6). Table 3 reveals that MeZO with accuracy/F1 successfully optimizes LMs with superior performance to zero-shot. Although minimizing cross entropy results in stronger performance, these preliminary findings highlight the promising potential of applying MeZO to optimize non-differentiable objectives without clear differentiable surrogates, such as human preferences [73]. 3.4 Memory usage and wall-clock time analysis In this section we profile the memory usage of zero-shot, ICL, FT, FT (prefix), and MeZO. We test OPT models of various sizes with Nvidia A100 GPUs (80GB memory) on MultiRC (average #tokens=400), and report the peak GPU memory consumption (details in Appendix E.7). As shown in Figure 3 (refer to Appendix F.5 for detailed numbers), MeZO exhibits the same memory consumption as zero-shot while offering memory savings of up to 12 times compared to standard FT and 6 times compared to FT (prefix). This advantage enables training larger models within a fixed hardware budget, as illustrated in Figure 4. Specifically, using a single A100 GPU, MeZO allows for tuning a model that is 11 times larger than what is feasible with FT. In Appendix F.6, we compare the wall-clock time efficiencies of our implementations of MeZO and Adam fine-tuning. MeZO achieves 7.74× per-step speedup and requires 8× fewer GPUs with a 30B model, but takes more steps to converge. Overall, MeZO only requires half as many GPU-hours to fine-tune a 30B model compared to full-parameter fine-tuning. The wall-clock benefit of MeZO is not inherent to the algorithm and is highly dependent on the implementation. We primarily provide this information as a demonstration that MeZO does not take a prohibitively long time to run. The above measurements are dependent on the computing infrastructure. In Appendix C, we compare the theoretical time-memory tradeoff of MeZO and backpropagation and find that MeZO is always more memory-efficient than backpropagation and is often more time-efficient. The above analyses also do not consider recent advances (e.g., gradient checkpointing [18], FlashAttention [23], and quantization [27]). We leave investigating the how MeZO works with these methods to future work. 7 4 Theory Our theoretical analysis highlights why MeZO can optimize large LMs, although a number of classical"
            },
            {
                "section": "results",
                "content": "[69, 47, 79, 3, 70] suggest that optimization should be catastrophically slow when training so many parameters. The inclusion of a simple prompt is crucial for MeZO to succeed (Appendix A). Past works [83, 67] have suggested that including such a prompt ensures that the fine-tuning objective is closely related to the pre-training one. As such, here, we make the assumption that the model has already been trained for many steps on the fine-tuning objective, which implies that the loss landscape exhibits favorable conditions (Assumption 1). Then, we derive a convergence rate independent of the number of parameters. We show that the loss decreases per step at a rate independent of the parameter dimension d (Theorem 1), and that, under stronger conditions, the algorithm converges in time independent of d (Lemma 3). Together, these results imply that MeZO is not catastrophically slower than SGD when fine-tuning.6 For ease of illustration, we assume that z is sampled from a sphere with radius √ d, and in Appendix G.2, we derive the rate for a general Gaussian z, which was used in the experiments. We follow classical analyses of SGD and replace the minibatch gradient estimate with SPSA (Defi- nition 1). Consider the minibatch SGD update θt+1 ←θt −η∇L(θ; Bt) where Bt is a minibatch drawn uniformly from DB. Crucially, the SGD minibatch gradient estimate is unbiased. Definition 3 (Unbiased Gradient Estimate). Any minibatch gradient estimate g(θ, B) is said to be unbiased if E[g(θ, B)] = ∇L(θ). 4.1 Per-step analysis The classical descent lemma uses a Taylor expansion to study how SGD reduces the loss at each optimization step. It highlights that when the gradient covariance is large, the maximum possible decrease in loss at each optimization step is small, thereby resulting in slower optimization. Lemma 1 (Descent Lemma). Let L(θ) be ℓ-smooth.7 For any unbiased gradient estimate g(θ, B), E[L(θt+1) | θt] −L(θt) ≤−η ∥∇L(θt)∥2 + 1 2η2ℓ· E[∥g(θ, Bt)∥2]. (2) The descent lemma highlights the importance of the gradient norm, which we derive for MeZO below. Lemma 2. Let B be a random minibatch of size B. Then, the gradient norm of MeZO is Ex \u0014\r\r\rb∇L(θ; B) 2\u0015 = d + n −1 n E h ∥∇L(θ; B)∥2i . where n is the number of z sampled in n-SPSA (Definition 1) and d is the number of parameters. Thus, in the usual case where n ≪d, MeZO has a much larger gradient norm than SGD.8 The descent lemma also shows that to guarantee loss decrease, one needs to choose the learning rate as η ≤ 2 ∥∇L(θt)∥2 ℓ· E[∥g(θ, B)∥2] Lemma 2 =======⇒ ηZO = n d + n −1ηSGD (3) where ηZO and ηSGD are the maximum permissible learning rates for MeZO and SGD respectively. Thus we see that without any further assumptions, MeZO can slow optimization by decreasing the largest permissible learning rate by a factor of d. Moreover, MeZO reduces the loss decrease that can be obtained at each step and, as a consequence, slows convergence by a factor of d as well. Surprisingly, our experiments show that MeZO can quickly optimize pre-trained models with billions of parameters, and reducing the number of tuned parameters via PEFT techniques does not substan- tially accelerate optimization (Appendix F.3). We attribute these phenomena to the Hessian of the loss exhibiting small local effective rank. It is prohibitively expensive to directly measure the effective rank of the Hessian of a large LM on a reasonably sized dataset. However, many previous works have shown that the Hessian of the loss for deep neural networks trained by SGD has remarkably low 6Section 3 uses the standard choice of Adam for FT; we provide SGD experiments in Appendix F.1. 7This is satisfied for the standard cross-entropy objective. 8All of our experiments use n = 1. 8 effective rank [74, 75, 36, 107, 105, 82]. In particular, the bulk of the spectrum concentrates around 0 with only a small number of outliers, and the number of these outliers is an upper bound on the effective rank. In addition, prior works [4, 56] have demonstrated that LM fine-tuning can occur in a very low dimensional subspace (< 200 parameters), which further supports the below assumption. We formalize the assumption on the effective rank below. In particular, we require an upper bound on the Hessian in a neighborhood around the current iterate to have effective rank at most r. Assumption 1 (Local r-effective rank). Let G(θt) = max(x,y)∈D ∥∇L(θt; {(x, y)})∥. There exists a matrix H(θt) ⪯ℓ· Id such that: 1. For all θ such that ∥θ −θt∥≤ηdG(θt), we have ∇2L(θ) ⪯H(θt). 2. The effective rank of H(θt), i.e tr(H(θt))/ ∥H(θt)∥op, is at most r. Under this assumption, we show that the convergence rate of ZO-SGD does not depend on the number of parameters. Instead, the slowdown factor only depends on the effective rank of the Hessian. Theorem 1 (Dimension-Free Rate). Assume the loss exhibits local r-effective rank (Assumption 1). If θt+1 = θt −ηZO b∇L(θt; B) is a single step of ZO-SGD using the n-SPSA estimate with a minibatch of size B, then there exists a γ = Θ(r/n) such that the expected loss decrease can be bounded as E[L(θt+1) | θt] −L(θt) ≤−ηZO ∥∇L(θt)∥2 + 1 2η2 ZOℓ· γ · E[∥∇L(θ; B)∥2] (4) By applying Equation (3), we can directly compare to the SGD descent lemma. Corollary 1. Choosing the learning rate ηZO = γ−1 · ηSGD, ZO-SGD obtains a loss decrease of E[L(θt+1) | θt] −L(θt) ≤1 γ · \u0014 −ηSGD ∥∇L(θt)∥2 + 1 2η2 SGDℓ· E[∥∇L(θ; B)∥2] \u0015 . (5) Here we see that comparing to SGD, the slowdown factor of ZO-SGD scales with the local effective rank r, which we argue is much smaller than the number of parameters d. The above analysis focuses on how much ZO-SGD and SGD decrease the loss at each step. Below, we show that under stronger assumptions about the loss landscape, we can obtain rates for how quickly the ZO-SGD algorithm converges to an optimal value. 4.2 Global convergence analysis We show that the global convergence rate also slows by a factor proportional to the local effective rank under stronger assumptions about the loss landscape. We assume that the landscape obeys the classical PL inequality: the gradient norm grows quadratically with the suboptimality of the iterate. Definition 4 (PL Inequality). Let L∗= minθ L(θ). The loss L is µ-PL if, for all θ, 1 2 ∥∇L(θ)∥2 ≥ µ(L(θ) −L∗). The PL inequality is not as strong as assuming that optimization exhibits kernel-like dynamics, but it ensures that the landscape is amenable to analysis [50]. In addition to the PL inequality, we assume the trace of the gradient covariance is bounded, so noise does not disrupt the trajectory too drastically. Definition 5 (Gradient Covariance). The SGD gradient estimate on a minibatch of size B has covariance Σ(θ) = B(E \u0002 ∇L(θ; B)∇L(θ; B)⊤\u0003 −∇L(θ)∇L(θ)⊤). As we show in Appendix G.1, this assumption holds for common loss functions such as square loss or binary cross entropy for several settings (e.g., kernel behavior [67]). With these two assumptions, we show that ZO-SGD has a slowdown proportional to the effective rank r, not the parameter dimension. Lemma 3 (Global Convergence of ZO-SGD). Let L(θ) be µ-PL and let there exist α such that tr(Σ(θ)) ≤α(L(θ) −L∗) for all θ. Then after t = O       \u0010 r n + 1 \u0011 · \u0012 ℓ µ + ℓα µ2B \u0013 log L(θ0) −L∗ ϵ | {z } SGD rate (Lemma 4)       iterations of ZO-SGD we have E[L(θt)] ≤L∗+ ϵ. 9 5 Related work Zeroth-order optimization Many classical lower bounds have been derived for ZO-SGD in the strongly convex and convex settings [47, 3, 79, 32, 85, 69] as well as non-convex [101]. These bounds generally depended on the number of parameters d. More recently, [100, 6, 15] showed that if the gradient has low-dimensional structure, then the query complexity scales linearly with the intrinsic dimension and logarithmically with the number of parameters, though the estimation has at least Ω(sd log d) memory cost. Additional tricks such as sampling schedules [11] and other variance reduction methods [48, 62] can be added to ZO-SGD. ZO has inspired distributed methods [93, 43] and black-box adversarial example generation [14, 63, 17, 64] in deep learning. Ye et al. [108], Balasubramanian and Ghadimi [7] estimate the Hessian to perform ZO optimization along important directions. There are also ZO methods that optimize without estimating the gradient [38, 68, 44]. Memory-efficient backpropagation Several algorithms have been proposed to efficiently approx- imate backpropagation by sparsifying gradients [92, 102], approximating Jacobians [1, 19], and subsampling the computational graph [71, 2]. However, these methods may accrue large approxima- tion errors for deep networks. Gradient checkpointing [18] reduces memory cost of backpropagation at the cost of recomputing some activations. FlashAttention [23] also reduces memory cost by recomputing attention matrices. Dettmers et al. [26, 27] explore quantization of large LMs’ weights and optimizer states, which leads to memory reduction in both training and inference. Gradient-free adaptation of large language models BBT and BBTv2 [91, 90] use evolutionary algorithms to achieve gradient-free optimization; however, due to its sensitivity to high dimensionality, BBT is limited to only optimize a low-dimension projection of prefixes and they focus on RoBERTa- large size models and few-shot settings. Other works in “black-box tuning” of LMs focus on optimizing discrete prompts without updating the model, either via reinforcement learning [16, 25, 29], ensemble [45], or iterative search [78]. Concurrent work in [106] uses iterative forward passes to improve in-context learning performance. 6"
            },
            {
                "section": "Conclusion",
                "content": "We have shown that MeZO can effectively optimize large LMs across many tasks and scales. Further experiments suggest that MeZO can optimize non-differentiable objectives, which backpropagation usually cannot do. Our theory illustrates why MeZO is not catastrophically slow when tuning billions of parameters. As a limitation, MeZO takes many steps in order to achieve strong performance, though we show that the per-step speedup in MeZO can often make fine-tuning with MeZO run faster than a standard implementation of fine-tuning with backpropagation. We did not explore combining MeZO with other memory-efficient methods, such as FlashAttention [23] and quantization [26], though we hope to investigate this in the future. We are excited to explore the applicability of MeZO to a number of areas, including but not limited to: pruning, distillation, saliency, interpretability, and dataset selection for fine-tuning. Non-differentiable objectives are a particularly exciting area, given recent advances in tuning large LMs to adapt to human feedback. Conducting theoretical analyses for how these efficient gradient estimates impact the performance of different applications is also of interest. Acknowledgements We thank Xinyi Chen, Yin Tat Lee, Kaifeng Lyu, Tengyu Ma, Abhishek Panigrahi, Nikunj Saunshi, and Mengzhou Xia for their helpful feedback. SA and SM are funded by NSR, ONR, SRC, and Simons Foundation. JDL, AD, and EN acknowledge the support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. TG is supported by an IBM PhD Fellowship. This work is also partially funded by the National Science Foundation (IIS-2211779). 10"
            },
            {
                "section": "Methods",
                "content": "in Natural Language Processing (EMNLP), pages 5747–5763, 2020. [62] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-order stochastic variance reduction for nonconvex optimization. In Advances in Neural Information Processing Systems, volume 31, 2018. [63] Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In International Conference on Learning Representations, 2019. [64] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43–54, 2020. 14 [65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [66] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098, 2022. [67] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning. arXiv preprint arXiv:2210.05643, 2022. [68] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In Advances in Neural Information Processing Systems, volume 31, 2018. [69] Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. 1983. [70] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex func- tions. Foundations of Computational Mathematics, 17:527–566, 2017. [71] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. Randomized automatic differentiation. arXiv preprint arXiv:2007.10412, 2020. [72] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [73] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022. [74] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. arXiv preprint arXiv:1811.07062, 2018. [75] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal of Machine Learning Research, 21(252):1–64, 2020. [76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high- performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024–8035. 2019. [77] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267–1273, 2019. [78] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022. [79] Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036– 7056, 2011. [80] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, 2016. [81] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. 2011. 15 [82] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. [83] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations, 2021. [84] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255–269, 2021. [85] Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. The Journal of Machine Learning Research, 18(1):1703–1713, 2017. [86] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013. [87] James C Spall. A one-measurement form of simultaneous perturbation stochastic approxima- tion. Automatica, 33(1):109–112, 1997. [88] J.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332–341, 1992. [89] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008–3021, 2020. [90] Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. BBTv2: Towards a gradient-free future with large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916–3930, 2022. [91] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning, pages 20841–20855, 2022. [92] Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. meProp: Sparsified back propagation for accelerated deep learning with reduced overfitting. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 3299–3308, 2017. [93] Yujie Tang and Na Li. Distributed zero-order algorithms for nonconvex multi-agent optimiza- tion. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 781–786, 2019. [94] Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. Zeroth-order optimization meets human feedback: Provable learning via ranking oracles, 2023. [95] Alexander Timurovich Vakhitov, Oleg Nikolaevich Granichin, and LS Gurevich. Algorithm for stochastic approximation with trial input perturbation in the nonstationary problem of optimization. Automation and Remote Control, 70:1827–1835, 2009. [96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. [97] Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, 2000. 16 [98] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in neural information processing systems, volume 32, 2019. [99] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. [100] Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order optimization in high dimensions. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pages 1356–1365, 2018. [101] Zhongruo Wang, Krishnakumar Balasubramanian, Shiqian Ma, and Meisam Razaviyayn. Zeroth-order algorithms for nonconvex minimax problems with improved complexities. arXiv preprint arXiv:2001.07819, 2020. [102] Bingzhen Wei, Xu Sun, Xuancheng Ren, and Jingjing Xu. Minimal effort back propagation for convolutional neural networks. arXiv preprint arXiv:1709.05804, 2017. [103] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018. [104] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, 2020. [105] Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Under- standing common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261, 2020. [106] Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward tuning boosts in-context learning in language models, 2023. [107] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581–590, 2020. [108] Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and Tong Zhang. Hessian-aware zeroth-order optimization for black-box adversarial attack. arXiv preprint arXiv:1812.11377, 2018. [109] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. [110] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. [111] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018. [112] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [113] Yan Zhang, Yi Zhou, Kaiyi Ji, and Michael M Zavlanos. A new one-point residual-feedback oracle for black-box learning and control. Automatica, 136:110006, 2022. 17 A Algorithmic Ablations We perform a number of ablations to select the best algorithm. As is standard in ZO literature, we consider the main computational cost to be the number of forward passes. In our case, the number of forward passes can be affected by the number of gradient steps taken, any usage of gradient accumulation, and using more noise samples to reduce the variance of the gradient estimate. We observed that the performance of MeZO improves monotonically with the number of steps, and there does not appear to be any overfitting. Hence, when performing algorithmic ablations, we can focus on the efficiency of different algorithms without considering implicit bias. This is also reflected in our theoretical analysis. To ease the computational load, we fix the number of forward passes to 10, 000 and compare many different algorithms for RoBERTa-large on a smaller set of tasks that span sentiment analysis, entailment, and topic classification: SST-2, SNLI, and TREC. We emphasize that 10, 000 is a small budget and is only used as a setting to compare these ZO algorithms to each other. We find that using a linearly decreasing learning rate schedule during training, as was done for fine-tuning with backpropagation in [65], does not help or hurt MeZO. Similarly, using a learning rate warmup leads to identical results on these three tasks. For simplicity, we use a constant learning rate schedule with no warmup for all of the below experiments. We perform few-shot experiments with k = 16 and average the results across 5 seeds. Experiment Hyperparameters Values MeZO Batch size {16, 64} × Learning rate {1e−5, 1e−6, 1e−7} × ϵ {1e−3, 1e−5} × Weight Decay {0, 0.1} Table 4: The hyperparameter grid used in our ablation experiments. For simplicity, we use a constant learning rate schedule. A.1 Prompting We study if adding a prompt is crucial to the ability of MeZO to optimize the network. We use prompts from Gao et al. [35]. Malladi et al. [67] claimed the prompt makes the optimization trajectory well- behaved, though we note that the current paper considers RoBERTa-large and large autoregressive models while the previous work only studied RoBERTa-base. We note the similarity between kernel behavior and our theoretical setting in Section 4. MeZO succeeds on tasks that are reported to not exhibit kernel behavior in Malladi et al. [67], so we investigate whether or not the prompt is necessary. SST-2 SNLI TREC Prompt 89.6 (1.2) 65.1 (6.2) 66.7 (6.2) No Prompt 51.9 (2.9) 34.8 (2.1) 19.5 (9.0) Table 5: Experiments using MeZO to fine-tune models with and without a prompt. Both experiments followed the grid in Table 4, but we also expanded the grid to include a learning rate of 1e −4 for the no prompt case. As a result of these experiments, we fix the setting to prompt-based fine-tuning for all of the below experiments. A.2 Sample schedules One can sample nt noise vectors at the tth step and use nt-SPSA to compute the gradient estimate. Similar ideas were proposed in Bollapragada et al. [11], Cai et al. [15]. We study the effect of linearly increasing and constant sampling schedules in the ablation setting. The intuition for the linearly increasing schedule is that the optimizer may need a higher fidelity gradient as it approaches the minimum. Increasing the number of zs can speed up optimization by reducing the gradient variance, but doing so also increases the number of forward passes required for each optimization step, so there is a trade-off to study. We note that increasing the number of zs should be accompanied by 18 a proportional scaling of the learning rate, analogous to the linear scaling rule proposed in [39] (theoretical justification can follow the SDE technique [58]). Table 6 shows no consistent benefit in one schedule over the other, and it demonstrates that increasing the n in n-SPSA while fixing the number of forward passes allowed results in only marginal gains at best. n Schedule SST-2 SNLI TREC n = 1 Constant 89.6 (1.2) 65.1 (6.2) 66.7 (6.2) n = 4 Constant 89.5 (1.1) 68.6 (3.2) 62.3 (5.6) n = 4 Linear 89.6 (1.4) 65.3 (6.4) 66.1 (5.5) n = 16 Constant 90.4 (0.7) 67.0 (3.4) 62.8 (6.3) n = 16 Linear 88.9 (1.2) 62.8 (5.9) 64.2 (5.3) Table 6: Experiments using MeZO with different schedules for n. We scale the learning rate proportionally to the number of z’s sampled. B MeZO Variants There is a rich history of transferring ideas from first order optimization to enhance ZO algorithms. Below, we highlight several variants of MeZO that did not achieve as high performance as the algorithm presented in Algorithm 1. B.1 Memory-efficient n-SPSA We highlight how MeZO can perform n-SPSA (Definition 1) efficiently for n > 1 in Algorithm 2. In particular, if sampling n z vectors and averaging the projected gradients, we require storing 2n additional scalars: the random seeds and the projected gradients. The same caveat about perturbing individual weights versus entire weight matrices still applies here (see Section 2). B.2 Augmenting MeZO with Gradient History The n-SPSA algorithm merely provides a gradient estimate that can subsequently be used in place of the gradient in any gradient-based optimizer. Many popular optimizers, such as Adam and SGD with momentum, require storing some historical information about gradients (e.g., a moving average). This requirement causes such algorithms to require 2× or 3× the memory that is needed for SGD. However, one advantage of MeZO is that the gradient history can be recomputed at each step without requiring much additional memory. In reference to Algorithm 1, note that the gradient only needs projected_grad and the random seed s used to compute the perturbation z, so we need to only store 2 scalars per step to reproduce the gradient history (i.e., up to 2T scalars during training). This is a substantial reduction in added memory overhead that is usually needed for using Adam or momentum instead of vanilla SGD. Table 18 illustrates that MeZO-Adam can sometimes improve the performance of MeZO, though each gradient step requires additional computation (but no additional forward passes). We leave it to future work to investigate when MeZO-Adam may be more useful than MeZO. Experiment Hyperparameters Values MeZO-Adam Batch size 64 Learning rate {1e−6, 1e−5, 1e−4, 5e−4, 1e−3} ϵ 1e−3 Weight Decay 0 Table 7: The hyperparameter grid used for MeZO-Adam. For simplicity, we use a constant learning rate schedule. 19 Algorithm 2: MeZO with n > 1 Require: parameters θ ∈Rd, loss L : Rd →R, step budget T, perturbation scale ϵ, batch size B, learning rate schedule {ηt}, n for n-SPSA estimate (Definition 1) for t = 1, ..., T do seeds, projected_grads ←[] ▷Will each contain n scalars for j = 1, ..., n do Sample batch B ⊂DB and random seed s θ ←PerturbParameters(θ, ϵ, s) ℓ+ ←L(θ; B) θ ←PerturbParameters(θ, −2ϵ, s) ℓ−←L(θ; B) θ ←PerturbParameters(θ, ϵ, s) ▷Reset parameters projected_grad ←(ℓ+ −ℓ−)/(2ϵ) projected_grads[j] ←projected_grad seeds[j] ←s end for j = 1, ..., n do Reset random number generator with seed seeds[j] for θi ∈θ do z ∼N(0, 1) θi ←θi −(ηt/n) ∗projected_grads[j] ∗z ▷Avg grad for z1, ..., zn end end end Subroutine PerturbParameters(θ, ϵ, s) Reset random number generator with seed s ▷For sampling z for θi ∈θ do z ∼N(0, 1) θi ←θi + ϵz ▷Modify parameters in place end return θ B.3 Modifying the Variance of MeZO Our theory in Section 4 sketches the well-known fact that the variance of the stochastic gradient estimate can impact the rate of optimization. ZO methods can be combined with standard variance reduction techniques to possibly improve optimization speed. For example, Liu et al. [62] designed a variance reduced ZO algorithm, analogous to SVRG [49], to improve the speed of convergence. Below, we show that several variance reduction methods (e.g., using the gradient norm) can be implemented in a memory-efficient manner. However, when controlling for the total budget of forward passes (i.e., function queries), these methods are not as performant as MeZO. We nevertheless present them to demonstrate the ease with which MeZO can be adapted, and we suggest these methods may be useful for optimizing more complex objectives. First, we define a general SPSA estimate that has the same expectation (i.e., the true gradient) but has a scaled variance. Definition 6 (Variance-Modified SPSA). Given a matrix D = diag(d), the variance modified SPSA computes e∇L(θ; B) = L(θ + ϵ(d−1 ⊙z); B) −L(θ −ϵ(d−1 ⊙z); B) 2ϵ (d ⊙z) where d ∈Rd has nonzero entries and d−1 denotes the coordinatewise reciprocal. The above SPSA variant is an unbiased estimator of the gradient, because E[e∇L(θ; B)] = E[D−1zz⊤D∇L(θ; B)] = E[∇L(θ; B)]. We will draw inspiration from classical methods (i.e., “control variates”) and choose d to be a block vector with gradient norms or parameter norms [99]. 20 To select the parameter groups, we split the model by layer, keeping the embedding and the head separate (i.e., RoBERTa-large has 24 + 2 = 26 parameter groups). It is straightforward to measure the parameter norms without consuming additional memory. We can measure the gradient norms without performing backpropagation, as shown below. Proposition 1 (ZO Estimate of Gradient Norm of ℓth Layer). Define zℓto have z ∼N(0, 1) in each coordinate corresponding to parameters in the ℓth layer and 0 everywhere else. Then, we can estimate the norm of the gradient of the loss w.r.t. the ℓth layer ∇θℓas ∥∇θℓL(θ; B)∥2 ≈ L(θ + ϵzℓ; B) −L(θ −ϵzℓ; B) 2ϵ As is true for SPSA, increasing the number of zℓ’s sampled for each value of ℓand averaging the result reduces the variance of the estimate. The rationale for this estimate is that for any vector v, Ez[(⟨v, z⟩)2] = ∥v∥2 2 for Gaussian z. It is clear that this estimate can be computed in a memory efficient way, although it requires 2L forward passes to compute gradient norms for L parameter groups. We show the experimental results for modifying the variance below. We follow the ablation setting and use a fixed budget of 10, 000 steps (Appendix A). Generally, using the gradient norm to reduce the variance substantially hurts performance (Table 8). If we “cheat” and allow one backpropagation through the network to estimate the gradient norm, then we see that reducing the variance using the gradient norm does not substantially hurt or help performance. Modifying the variance using the parameter norm, analogous to layerwise adaptive rate methods, does not substantially impact the performance of MeZO (Table 9). Our observation is that decreasing the variance by setting d as the gradient norm does not improve optimization. This empirical result agrees with the exposition in Section 4 that the straightforward variance analysis (which yields a dependence on the number of parameters d) is not the best lens to study the rate of optimization when fine-tuning with MeZO. Our effective rank view in Theorem 1 and Lemma 3 is likely a better characterization of fine-tuning dynamics. We leave it to future work to explore if these methods can be useful for other more complex objectives. Recompute d ZO estimate of d SST-2 SNLI TREC Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2) 89.7 (0.8) 65.2 (5.2) 64.3 (6.4) 87.0 (2.5) 49.6 (9.2) 32.6 (7.7) 79.0 (10.3) 48.9 (2.2) 38.7 (7.5) Table 8: Experiments modifying the variance of MeZO using d as the gradient norm (see Definition 6). We sometimes recompute d at the start of each epoch or use Proposition 1 to estimate d without requiring backpropagation. Recompute d SST-2 SNLI TREC Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2) 89.2 (2.1) 65.4 (4.2) 64.8 (5.6) 88.2 (4.7) 65.2 (4.0) 64.7 (5.5) Table 9: Experiments modifying the variance of MeZO using d as the parameter norm (see Defini- tion 6). We sometimes recompute d at the start of each epoch. B.4 Modifying the Expectation of MeZO The above experiments show that modifying the variance of MeZO cannot consistently accelerate its convergence. However, a simple modification of Definition 6 allows us to change the expectation of MeZO as well. This can be used to efficiently estimate coordinate-wise normalized gradient-based optimizer updates (e.g., Adam). 21 Definition 7 (Expectation-Modified SPSA). Given a matrix D = diag(d), the variance modified SPSA computes e∇L(θ; B) = L(θ + ϵ(d−1 ⊙z); B) −L(θ −ϵ(d−1 ⊙z); B) 2ϵ z where d ∈Rd. Now, we see that e∇L(θ; B) = E[D−1zz⊤∇L(θ; B)] so the SPSA estimate is no longer an unbiased estimator for ∇L(θ). If we choose d to be the gradient norm, for example, then SPSA can estimate the normalized gradient. Concurrent work in Tang et al. [94] gives another ZO estimate of the normalized gradient while assuming access to only rankings of inputs (instead of the noisy function evaluations available in our setting). We find that estimating the normalized gradient does not perform as well as directly estimating the gradient (Table 10). Regardless, we present this algorithm as a way to highlight that any coordinate-wise operation to the gradient can be applied in a memory-efficient manner. Method SST-2 SNLI TREC Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2) Estimate of normalized gradient (Definition 7) 88.0 (1.2) 60.0 (2.4) 44.0 (14.0) Table 10: Experiments modifying the expectation of MeZO using d as the gradient norm (see Definition 7). We use the ZO estimate of the gradient norm (Proposition 1). B.5 One-point estimate Here, we investigate the efficacy of one-point gradient estimators in place of the two-point SPSA method. Using a one-point estimator instead of SPSA can reduce the MeZO running time by half. Many one-point estimators have been proposed in the past [34, 87, 95]. For simplicity, we focus on one estimator [113] that has a form reminiscent of SPSA but requires only one forward pass to estimate the gradient at each step. Definition 8 (One-Point Gradient Estimate, Zhang et al. [113]). For a loss function L evaluated on a batch Bt with parameters θt at step t, we can draw random noise zt ∼N(0, Id) and compute the gradient estimate using hyperparameter ϵ as written below. b∇L(θt; Bt) = L(θt + ϵzt; Bt) −L(θt−1 + ϵzt−1; Bt−1) ϵ Notably, this one-point gradient estimate uses the loss at the previous iterate instead of evaluating the loss again at the current iterate. As such, this estimator requires only one forward pass at each iterate to estimate the gradient. For well-behaved loss functions and slow-moving optimization, these two formulas are intuitively similar. However, Table 11 finds this estimator to be much less efficient than SPSA when fixing the number of forward passes. Steps SST-2 SST-5 SNLI MNLI RTE TREC —— sentiment —— —— natural language inference —— — topic — SPSA [88] 20K 92.8 (0.5) 51.3 (0.9) 82.9 (1.0) 74.4 (0.8) 76.7 (1.7) 92.7 (0.6) One-point estimate [113] 20K 90.0 (0.4) 44.6 (2.0) 70.1 (1.5) 57.2 (0.9) 64.1 (1.0) 57.3 (5.7) One-point estimate [113] 40K 91.8 (0.5) 45.9 (1.7) 74.4 (0.8) 61.0 (1.0) 68.7 (1.2) 73.0 (3.1) Table 11: Comparison between SPSA and a one-point estimate Zhang et al. [113]. One-point estimate only does one forward pass per step, thus is twice as fast as two-point estimate per step. As such, the number of forward passes after 40K steps with the one-point estimate is the same as the number of forward passes with SPSA after 20K steps. The results show that two-point estimate is much more effective than one-point estimate. C Memory Analysis The compute-memory tradeoff of backpropagation is complex to analyze. Griewank and Walther [40] provides a rigorous theoretical treatment of the problem. We empirically measure the memory 22 consumption of different methods for commonly used large language models, but here we hope to provide a more rigorous comparison of different gradient estimation algorithms, independent of the software used to implement them. Below, we summarize some key points that may help readers to understand how the MeZO compute-memory tradeoff compares to backpropagation. Given a network, the first step to perform backpropagation is to decompose the model into easily differentiable blocks. We note that this decomposition is not unique. For each block, one can choose to cache the resulting output during the forward pass (thereby consuming memory) or instead recompute the output when it is needed (thereby consuming compute). The below proposition, adapted from Rule 21 in Griewank and Walther [40], captures this tradeoff. Proposition 2 (Time-Memory Tradeoff for Backpropagation, Griewank and Walther [40]). Consider a network containing N bits. For any time-memory tradeoff hyperparameter c = O(1), there exists a backpropagation algorithm that runs in time O(cN) and consumes memory proportional to O(N 1/c). Grimm et al. [41] also gave sharp bounds for the memory-time product. Note that the popular gradient checkpointing [18] method allows one to tune c with limited precision (i.e., one cannot always further split a differentiable block and observe savings). Experiments in Chen et al. [18] choose c = 2 to achieve O( √ N) memory while consuming O(2N) computation. In the extreme case, gradient checkpointing allows one to use O(N log N) computation and O(log N) memory. MeZO always consumes 2N compute and O(1) memory, so it is more compute-efficient at the same memory cost as gradient checkpointing. Our exposition in Section 2 discusses that we can perturb groups of parameters together to save time while consuming additional memory. However, we do not consider that variant here because it is somewhere in the middle of the compute-memory pareto curve, where we cannot reason about what backpropagation will do. In particular, MeZO can split groups differently than backpropagation can, since MeZO does not require that each parameter group is easily differentiable, so it is hard to compare the two algorithms along the entire pareto curve. We also compare backpropagation for the c = 1 case (i.e., storing everything during the forward pass). When storing everything, backpropagation consumes O(N) time and O(N) memory. Hence, SPSA consumes slightly more time and substantially less memory than backpropagation at this end of the tradeoff. Unlike gradient checkpointing, MeZO computes only an approximation of the gradient. This approximation is only useful for fine-tuning with a prompt, making it less broadly useful than gradient checkpointing. There are other methods that approximate the gradient with less memory consumption than gradient checkpointing (see the Related Work section), though it is unclear how the memory consumption of those algorithms compare to MeZO. D Forward Auto-Differentiation We discuss the merits of using forward auto-differentiation instead of two forward passes to construct a gradient estimate for fine-tuning. As ϵ →0, the SPSA gradient estimate (Definition 1) can be written as zz⊤∇L(θ; B). The term z⊤∇L(θ; B) is a Jacobian-vector product (JVP), and it is well-known that this can be computed in parallel with a single forward pass while consuming additional memory equivalent to that of the largest activation in the model. To fully compute the gradient estimate, one must store z on the GPU while performing inference, so we observe that this algorithm requires more memory than MeZO. We note that implementation of the forward auto-differentiation algorithm is not well-supported in PyTorch at the time of writing. The autograd JVP function computes the JVP in a memory-inefficient way, as noted in the documentation, and the other available methods to compute a JVP are not sophisticated enough to easily scale to a complex LLM. Computing the JVP is straightforward when using JAX, so we profile the memory consumption of inference and the JVP for RoBERTa-large when using JAX. We use batch size 16 with the MultiRC task. Note that JAX may automatically use rematerialization to avoid out of memory errors so we focus on settings in which the memory utilization remains below 50%. The resulting memory usage during inference, backpropagation, and forward auto-differentiation are reported in Table 12. 23 We see that forward auto-differentiation is substantially more memory efficient than backpropagation but less memory efficient than inference. Furthermore, forward auto-differentiation selects ϵ = 0, which removes potentially beneficial third-and-higher order Taylor expansion terms from the estimate. Task Inference (and MeZO) Backpropagation Forward Auto-Differentiation Excess Memory (MB) 327.50 24156.23 830.66 Table 12: Memory consumption of RoBERTa-large when using batch size 16 with the MultiRC task. The reported memory does not include the cost of storing the model on the GPU, which is required for all three cases. E Experiment setup E.1 Datasets For RoBERTa-large, we consider classification datasets: SST-2 [86], SST-5 [86], TREC [97], MNLI [103], SNLI [12], and RTE [22, 8, 37, 10]. We follow Malladi et al. [67] in limiting the test set to 1, 000 examples for fast iteration. For training and validation, we have two settings: k = 16 and k = 512, which mean that we have 16 or 512 examples per class for both training and validation. For OPT experiments, we consider the SuperGLUE dataset collection [98], including: BoolQ [21], CB [24], COPA [81], MultiRC [51], ReCoRD [111], RTE [22, 8, 37, 10], WiC [77], and WSC [55]. We also include SST-2 [86] and two question answering (QA) datasets, SQuAD [80] and DROP [31]. We randomly sample 1,000 examples for training, 500 examples for validation, and 1,000 examples for testing. E.2 Prompts Table 13 shows the set of downstream tasks and prompts with which we fine-tune RoBERTa-large, which are adapted from [35]. Dataset C Type Prompt Label words SST-2 2 sentiment cls. <S1> It was [MASK] . {great, terrible} SST-5 5 sentiment cls. <S1> It was [MASK] . {great, good, okay, bad, terrible} TREC 6 topic cls. [MASK] : <S1> {Description, Expression, Entity, Human, Location, Number} MNLI 3 NLI <S1> ? [MASK] , <S2> {Yes, Maybe, No} SNLI 3 NLI <S1> ? [MASK] , <S2> {Yes, Maybe, No} RTE 2 NLI <S1> ? [MASK] , <S2> {Yes, No} Table 13: The prompts of the datasets we used in our RoBERTa-large experiments (Table 18 and Figure 2). The prompts are adapted from [35] and include a template and a set of label words that can fill in the [MASK]token. <S1> and <S2> refer to the first and the second (if any) input sentence. Table 14 demonstrates the prompts we use for OPT. Note that in OPT experiments we have three types of tasks: classification, multiple-choice, and question answering. Prompts are adopted from GPT-3 [13] and PromptSource with minor changes [5]. E.3 Hyperparameters We use the hyperparameters in Table 15 for MeZO experiments on RoBERTa-large (Table 18 and Figure 2). Experiments in Appendix A informed the grid; in particular, the choice of ϵ seemed to not significantly impact performance, and using a larger batch size consistently yielded faster optimization. We use the hyperparameters in Table 16 for MeZO experiments on OPT. Regarding learning rate scheduling and early stopping, we use linear learning scheduling for all fine-tuning with backpropagation experiments and constant learning rate for all MeZO experiments. For RoBERTa experiments, we evaluate the model on validation sets every 1/10 of total training steps and save the best validation checkpoint. All FT experiments use 1K steps and MeZO experiments use 24 Dataset Type Prompt SST-2 cls. <text> It was terrible/great RTE cls. <premise> Does this mean that \"<hypothesis>\" is true? Yes or No? Yes/No CB cls. Suppose <premise> Can we infer that \"<hypothesis>\"? Yes, No, or Maybe? Yes/No/Maybe BoolQ cls. <passage> <question>? Yes/No WSC cls. <text> In the previous sentence, does the pronoun \"<span2>\" refer to <span1>? Yes or No? Yes/No WIC cls. Does the word \"<word>\" have the same meaning in these two sentences? Yes, No? <sent1> <sent2> Yes/No MultiRC cls. <paragraph> Question: <question> I found this answer \"<answer\". Is that correct? Yes or No? Yes/No COPA mch. <premise> so/because <candidate> ReCoRD mch. <passage> <query>.replace(\"@placeholder\", <candidate>) SQuAD QA Title: <title> Context: <context> Question: <question> Answer: DROP QA Passage: <context> Question: <question> Answer: Table 14: The prompts of the datasets we used in our OPT experiments. There are three types of tasks: classification (cls.), multiple-choice (mch.), and question answering (QA). Prompts are adopted from GPT-3 [13] and PromptSource [5] with minor changes. <text> represents input from the dataset and Yes represents label words. For inference on multiple choice tasks, we put in different candidates in the prompt and calculate the average log-likelihood for each candidate, and choose the candidate with the highest score. For inference on QA tasks, we use greedy decoding to generate the answer. 100K steps. For OPT experiments, we evaluate the model on validation sets every 1/5 of the total training steps and save the best validation checkpoint. All FT experiments train for 5 epochs and all MeZO experiments use 20K steps. Note that FT experiments mostly converge within 5 epochs but we observe that MeZO performance can still improve with more training steps. E.4 Modeling and implementation For RoBERTa experiments, we follow [35] for the prompt-based fine-tuning paradigm for masked language models. Please refer to the original paper for more details. In OPT experiments, for classification tasks, we train the model similarly to [35], i.e., we take the logits corresponding to the label words and apply cross entropy loss on them; for multiple choice tasks and generation tasks (QA), we only keep the correct candidate and use teacher forcing to train on the correct examples. We only keep the loss on tokens in the candidate part and exclude the prompt part. For OPT inference on classification and multiple-choice tasks, we use the model to get the average log-likelihood (by tokens) of all the candidates/label words, and predict the one with the highest average log-likelihood. For generation tasks, we use greedy decoding to generate the answer. For in-context learning, we use 32 examples in the context. We also try filling in as many examples as possible in the context but this does not improve performance and sometimes leads to unstable"
            },
            {
                "section": "results",
                "content": ". Thus we keep the 32-example results. 25 Experiment Hyperparameters Values MeZO Batch size 64 Learning rate {1e−7, 1e−6, 1e−5} ϵ 1e−3 Weight Decay 0 MeZO (prefix) Batch size 64 Learning rate {1e−2, 5e−3, 1e−3} ϵ 1e−1 Weight Decay 0 # prefix tokens 5 MeZO (LoRA) Batch size 64 Learning rate {1e−5, 5e−5, 1e−4} ϵ 1e−3 Weight Decay 0.1 (r, α) (8, 16) FT with Adam Batch size (k = 16) {2, 4, 8} Batch size (k = 512) {8, 16, 32} Learning Rates {1e−5, 3e−5, 5e−5} Weight Decay 0 FT with SGD Batch size (k = 16) {2, 4, 8} Batch size (k = 512) {8, 16, 32} Learning Rates {1e−4, 5e−4, 1e−3, 5e−3, 1e−2} Weight Decay 0 FT (prefix) Batch size {8, 16, 32} Learning Rates {1e−2, 3e−2, 5e−2} Weight Decay 0 # prefix tokens 5 FT (LoRA) Batch size {4, 8, 16} Learning Rates {1e−4, 3e−4, 5e−4} (r, α) (8, 16) Table 15: The hyperparameter grids used for RoBERTa-large experiments. MeZO uses a constant learning rate schedule, and FT uses linear scheduling. All FT experiments use 1K steps and MeZO experiments use 100K steps. We check validation performance every 1/10 total training steps. For linear probing of classification tasks, we take the output feature and use scipy package to train a linear classifier. For multiple-choice tasks and generation tasks, we found that this leads to poor"
            },
            {
                "section": "results",
                "content": "since the output space is the whole vocabulary; instead, we do head-tuning, where the whole model is fixed except for the LM projection head. We use a batch size of 8 and a learning rate of {1e−4 5e−4}, and train the head for 5 epochs. For experiments on 30B and 66B OPT models, we largely follow the OPT hyperparameters except that we do not evaluate the intermediate validation performance and directly use the last checkpoint for evaluation, due to the high storage cost of intermediate checkpoints of large models. E.5 Parameter-efficient fine-tuning Fine-tuning and storing a copy of the large language model for each downstream task is expensive. Parameter-efficient fine-tuning (PEFT) techniques alleviate this problem: instead of tuning all model parameters, PEFT only tunes a small number of additional parameters (usually less than 1%) and can often achieve comparable or better performance [57, 54, 30]. The ZO optimizer is compatible with PEFT methods, since ZO can operate on any subset of the model parameters. We are interested in the following two common PEFT methods, designed for transformers [96]. LoRA [46] adds a tunable low-rank delta to a linear layer during fine-tuning. Suppose a linear layer performed W x + b during pre-training with W ∈Rm×n. When fine-tuning, LoRA introduces two smaller matrices A ∈Rm×r and B ∈Rr×n such that r ≪min(m, n). The linear layer is then 26 Experiment Hyperparameters Values MeZO Batch size 16 Learning rate {1e−6, 1e−7} or {1e−6, 5e−7, 1e−7} for SQuAD and DROP ϵ 1e−3 MeZO (prefix) Batch size 16 Learning rate {1e−2, 1e−3} or {5e−2, 1e−2, 5e−3} for SQuAD and DROP ϵ 1e−1 # prefix tokens 5 MeZO (LoRA) Batch size 16 Learning rate {1e−4, 5e−5} or {1e−4, 5e−5, 1e−5} for SQuAD and DROP ϵ 1e−2 (r, α) (8, 16) FT with Adam Batch size 8 Learning Rates {1e−5, 5e−5, 8e−5} Table 16: The hyperparameter grids used for OPT experiments. All weight decay is set to 0. FT uses 5 epochs and linear scheduled learning rates and MeZO uses 20K steps and constant learning rates. We check validation performance and save the best checkpoint every 1/5 total training steps. computed as \u0010 W + α r AB \u0011 x + b (6) where r and α are hyperparameters. A and B are trained on the downstream task while W is frozen at its pre-trained value. In transformers, this modification to the linear layer is applied to the query and value operations of each attention layer. Empirically, r can be very small, so the number of trainable parameters during fine-tuning is small. We choose r = 8 and α = 16. Prefix-tuning [57] adds a prefix of m tunable representations at each layer and freezes the rest of the model. The representations are added as new keys and values and treated as additional context during the attention operation. We initialize these tunable representations by randomly sampling tokens from the vocabulary and passing them through the LLM to get their keys and values at different attention layers. We found this crucial to make prefix tuning stable with MeZO, and this trick additionally boosts the performance of prefix tuning with backpropagation, as shown in Table 17. We also tried the reparameterization trick in [57], which does not help MeZO training. In our experiments, we find m = 5 to be sufficient to achieve good performance on most tasks. We also show that MeZO is compatible with parameter-efficient fine-tuning methods, such as prefix tuning and LoRA. Surprisingly, the performance of MeZO does not improve substantially when tuning much fewer parameters, as one might expect from classical analyses (see Section 4). Accordingly, our theoretical analysis in Section 4 suggests that the convergence rate of ZO-SGD does not depend on the parameter dimension during fine-tuning. Task SST-2 SST-5 SNLI MNLI RTE TREC Type —— sentiment —— —— natural language inference —— — topic — FT (prefix, random init) 90.7 (1.7) 47.2 (2.0) 70.7 (2.8) 62.6 (3.3) 63.5 (4.4) 83.4 (4.7) FT (prefix, real act init) 91.9 (1.0) 47.7 (1.1) 77.2 (1.3) 66.5 (2.5) 66.6 (2.0) 85.7 (1.3) Table 17: Prefix-tuning ablations. We compare randomly-initialized prefixes and real word activation prefixes. Using real word activations significantly outperforms random initialization. E.6 Training with non-differentiable objectives The experiments maximizing the accuracy of a RoBERTa-large model were all conducted using the same grid as MeZO in Table 15. For OPT experiments on SQuAD with F1 as objective, we use a batch size of 16. For MeZO, we use a learning rate of {1e−6, 5e−6, 1e−5} and ϵ = 1e−3. For MeZO (prefix), we use a learning rate of {1e−1, 5e−2, 1e−2} and ϵ = 1e−1. 27 E.7 Memory profiling In memory profiling, we use standard implementation with Huggingface’s transformers [104] package. We did not turn on any advance memory-saving options, e.g., gradient checkpointing. We set the per-device batch size as 1 to test the minimum hardware requirement to run the model with specific optimization algorithms. For multi-GPU backpropagation, we use fully sharded data parallel (FSDP) [33] provided by PyTorch [76]. For multi-GPU MeZO, we use transformers multi-GPU inference of large models. We use Nvidia’s nvidia-smi command to monitor the GPU memory usage. We call a run “successful” if there is no out of memory error from GPUs for at least 100 steps. We also profile fine-tuning with LoRA, but find its memory usage similar to that of fine-tuning with prefix-tuning. Hence here we only show the analysis with prefix-tuning. F More experiment results F.1 RoBERTa-large experiments Table 18 contains the detailed numbers corresponding to Figure 2 and also reports the performance of MeZO-Adam. Task SST-2 SST-5 SNLI MNLI RTE TREC Type —— sentiment —— —— natural language inference —— — topic — Zero-shot 79.0 35.5 50.2 48.8 51.4 32.0 Gradient-free methods: k = 16 LP 76.0 (2.8) 40.3 (1.9) 66.0 (2.7) 56.5 (2.5) 59.4 (5.3) 51.3 (5.5) MeZO 90.5 (1.2) 45.5 (2.0) 68.5 (3.9) 58.7 (2.5) 64.0 (3.3) 76.9 (2.7) MeZO (LoRA) 91.4 (0.9) 43.0 (1.6) 69.7 (6.0) 64.0 (2.5) 64.9 (3.6) 73.1 (6.5) MeZO (prefix) 90.8 (1.7) 45.8 (2.0) 71.6 (2.5) 63.4 (1.8) 65.4 (3.9) 80.3 (3.6) MeZO-Adam 90.4 (1.4) 45.4 (1.5) 74.1 (2.7) 64.3 (0.8)† 59.2 (11.1)† 78.3 (1.4) Gradient-based methods: k = 16 FT 91.9 (1.8) 47.5 (1.9) 77.5 (2.6) 70.0 (2.3) 66.4 (7.2) 85.0 (2.5) FT (LoRA) 91.4 (1.7) 46.7 (1.1) 74.9 (4.3) 67.7 (1.4) 66.1 (3.5) 82.7 (4.1) FT (prefix) 91.9 (1.0) 47.7 (1.1) 77.2 (1.3) 66.5 (2.5) 66.6 (2.0) 85.7 (1.3) Gradient-free methods: k = 512 LP 91.3 (0.5) 51.7 (0.5) 80.9 (1.0) 71.5 (1.1) 73.1 (1.5) 89.4 (0.5) MeZO 93.3 (0.7) 53.2 (1.4) 83.0 (1.0) 78.3 (0.5) 78.6 (2.0) 94.3 (1.3) MeZO (LoRA) 93.4 (0.4) 52.4 (0.8) 84.0 (0.8) 77.9 (0.6) 77.6 (1.3) 95.0 (0.7) MeZO (prefix) 93.3 (0.1) 53.6 (0.5) 84.8 (1.1) 79.8 (1.2) 77.2 (0.8) 94.4 (0.7) MeZO-Adam 93.3 (0.6) 53.9 (0.8) 85.3 (0.8) 79.6 (0.4) 79.2 (1.2) 95.1 (0.3) Gradient-based methods: k = 512 FT 93.9 (0.7) 55.9 (0.9) 88.7 (0.8) 84.4 (0.8) 82.7 (1.4) 97.3 (0.2) FT (LoRA) 94.2 (0.2) 55.3 (0.7) 88.3 (0.5) 83.9 (0.6) 83.2 (1.3) 97.0 (0.3) FT (prefix) 93.7 (0.3) 54.6 (0.7) 88.3 (0.7) 83.3 (0.5) 82.5 (0.8) 97.4 (0.2) Table 18: Experiments on RoBERTa-large (350M parameters). LP: Linear probing; ZO, ZO (LoRA), and ZO (prefix): our memory-efficient ZO-SGD (Section 2.1) with full-parameter tuning, LoRA, and prefix-tuning respectively; FT: fine-tuning with Adam. All reported numbers are averaged accuracy (standard deviation). All experiments use prompts (Appendix E.2). ZO outperforms zero-shot and LP by a large margin and approaches FT performance with much less memory cost. LP-MeZO We also compare MeZO to performing linear probing and then subsequently performing fine-tuning via MeZO, following the analogous suggestion for fine-tuning in Kumar et al. [53]. We use the MeZO grid described in Table 15. Note that the linear probing checkpoints used here have early stopping, unlike the ones reported in Table 18. We heuristically implement early stopping by limiting the number of iterations (from 5000 to 1000) and increasing the convergence tolerance (from 1e−4 to 0.01) in the scipy solver. Experiments on a few settings show that LP-MeZO can sometimes improve performance without increasing the memory consumption (see Table 19). However, sometimes, linear probing first can severely hurt performance. 28 Task SST-2 SST-5 SNLI TREC Zero-shot 79.0 35.5 50.2 32.0 FT 91.9 (1.8) 47.5 (1.9) 77.5 (2.6) 85.0 (2.5) MeZO 90.5 (1.2) 45.5 (2.0) 68.5 (3.9) 76.9 (2.7) LP-MeZO 91.4 (1.4) 41.9 (3.3) 70.7 (3.4) 54.0 (4.5) Table 19: Performing linear probing before fine-tuning with MeZO, as suggested previously [53], can sometimes improve performance without increasing the memory overhead. We use k = 16 for these experiments. F.2 OPT experiments Table 20 present the full results of OPT-30B and OPT-66B, with detailed MeZO numbers. Task SST-2 RTE BoolQ WSC WIC SQuAD 30B zero-shot 56.7 52.0 39.1 38.5 50.2 46.5 30B ICL 81.9 66.8 66.2 56.7 51.3 78.0 30B MeZO 90.6 66.4 67.2 63.5 56.3 85.2 30B MeZO (prefix) 87.5 72.6 73.5 55.8 59.1 83.9 66B zero-shot 57.5 67.2 66.8 43.3 50.6 48.1 66B ICL 89.3 65.3 62.8 52.9 54.9 81.3 66B MeZO 91.2 65.7 72.7 63.5 58.9 * 66B MeZO (prefix) 93.6 66.4 73.7 57.7 58.6 85.0 Table 20: Experiments on OPT-30B and OPT-66B (with 1,000 examples). *: MeZO requires further tuning to successfully optimize. F.3 Convergence of MeZO with full-parameter and PEFT We demonstrate the convergence rate of MeZO, MeZO (LoRA) and MeZO (prefix) on SST-2 and SNLI for the first 5,000 steps in Figures 5. We see that despite the different number of parameters they optimize, MeZO demonstrates similar training speed on full parameter and PEFT. This agrees with our theory in Section 4, which shows that MeZO’s optimization speed is independent of the number of parameters. 0 500 1000 1500 2000 2500 3000 Steps 0.25 0.30 0.35 0.40 0.45 0.50 0.55 Loss SST-2 MeZO Train MeZO (LoRA) Train MeZO (prefix) Train 0 500 1000 1500 2000 2500 3000 Steps 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Loss SNLI MeZO Train MeZO (LoRA) Train MeZO (prefix) Train Figure 5: MeZO does not optimize significantly faster when tuning fewer parameters, agreeing with our theory in Section 4. 29 F.4 ZO vs BBTv2 We compare ZO with BBTv2 [90] on mutually assessed tasks in Table 21. ZO significantly outperform BBTv2. Furthermore, BBTv2 is limited to optimize in low-dimensional space and requires prefix- tuning and a down-projection to reduce the number of optimized parameters. BBTv2 also employs an iterative scheme which only optimizes one layer at a time. In contrast, ZO works with both full-parameter tuning and PEFT, as shown in our experiments (Section 3) and theory (Section 4). Task SST-2 SNLI RTE Task type —— sentiment —— – natural language inference – Zero-shot 79.0 50.2 51.4 BBTv2 90.3 (1.7) 57.3 (2.3) 56.7 (3.3) MeZO 90.5 (1.2) 68.5 (3.9) 64.0 (3.3) MeZO (LoRA) 91.4 (0.9) 69.7 (6.0) 64.9 (3.6) MeZO (prefix) 90.8 (1.7) 71.6 (2.5) 65.4 (3.9) Table 21: ZO vs BBTv2 with RoBERTa-large. BBTv2 performance is from Sun et al. [90]. F.5 Memory profiling We show the detailed numbers of memory profiling results Table 22, which also corresponds to Figure 3. For how we profile the memory usage, please refer to Appendix E.7. Method Zero-shot / MeZO ICL Prefix FT Full-parameter FT 1.3B 1xA100 (4GB) 1xA100 (6GB) 1xA100 (19GB) 1xA100 (27GB) 2.7B 1xA100 (7GB) 1xA100 (8GB) 1xA100 (29GB) 1xA100 (55GB) 6.7B 1xA100 (14GB) 1xA100 (16GB) 1xA100 (46GB) 2xA100 (156GB) 13B 1xA100 (26GB) 1xA100 (29GB) 2xA100 (158GB) 4xA100 (316GB) 30B 1xA100 (58GB) 1xA100 (62GB) 4xA100 (315GB) 8xA100 (633GB) 66B 2xA100 (128GB) 2xA100 (134GB) 8xA100 16xA100 Table 22: Memory usage on the MultiRC (avg #tokens=400) dataset. F.6 Wallclock time efficiency In this section, we measure the wallclock time efficiency of MeZO compared to full-parameter FT, with respect to different model sizes. We conduct our experiments with 80GB A100s connected by NVLink and InfiniteBand, which are state-of-the-art solutions for distributed training. As shown in Table 23, on the MultiRC datasets, training with MeZO brings 7.74× speedup per step compared to full-parameter FT on a 30B model. This is due to (1) MeZO does not require costly backpropagation and (2) MeZO requires fewer GPUs and reduces the multi-GPU communication overhead. We can see that MeZO has a bigger advantage when training larger models—the multi-GPU overhead for fine-tuning is larger. Note that even though MeZO has better per-step wallclock efficiency, it requires significantly more steps than standard FT. Taking our OPT-30B experiments as an example: MeZO takes 32× more steps than standard FT, while FT takes 8× more GPUs and 7.74× more time per step. Overall, MeZO requires only half as many GPU-hours as FT for a 30B model. 30 1.3B 2.7B 13B 30B 66B MeZO (bsz=16) 0.815s (1) 1.400s (1) 2.702s (1) 5.896s (1) 12.438s (4) MeZO (bsz=8) 0.450s (1) 0.788s (1) 1.927s (1) 4.267s (1) 7.580s (2) FT (bsz=8) 0.784s (1) 1.326s (1) 13.638s (4) 45.608s (8) 84.098s (20) bspd=2, ga=4 bspd=2, ga=4 bspd=1, ga=2 bspd=1, ga=1 bspd=1, ga=1 Table 23: Wallclock time per step of different training methods. Numbers in brackets are numbers of GPUs required. It is measured on 80GB A100s with NVLink and InfiniteBand connections. The wallclock time is averaged over 100 steps. It is measured on the MultiRC task with the OPT family. We use a batch size (“bsz”) of 8 for FT and 16 for MeZO (consistant with our main experiment setting). For comparison we also add MeZO with a batch size of 8. For FT (FSDP), we show the following additional information. “bspd”: batch size per device. “ga”: gradient accumulation steps. The effective batch size is bspd×ga× #GPUs. Note that for FT 66B, the effective batch size 20. 31 G Proofs Proof of Lemma 2. We first note that in the ϵ →0 limit, we have b∇L(θ; B) = 1 Bn X (x,y)∈B X i∈[n] ziz⊤ i ∇L(θ; {(x, y)}). Taking expectation over the batch B and the zi, we have E[b∇L(θ; B)] = ∇L(θ), so b∇L(θ; B) is an unbiased estimator of the gradient. Computing the second moment, we get E h b∇L(θ; B)b∇L(θ; B)⊤i = 1 B2n2 X (x1,y1),(x2,y2)∈B X i,j∈[n] E \u0002 (ziz⊤ i ∇L(θ; {(x1, y1)}))(zjz⊤ j ∇L(θ; {(x2, y2)}))⊤\u0003 Let u, v be two arbitrary vectors. We have that Ezi,zj[ziz⊤ i uv⊤zjz⊤ j ] = uv⊤ when i ̸= j, and Ezi[ziz⊤ i uv⊤ziz⊤ i ] = Ez[z⊗4](u, v) = 3d d + 2Sym(I⊗2)(u, v) = d d + 2 · u⊤v · I + 2d d + 2 · uv⊤. Therefore E h b∇L(θ; B)b∇L(θ; B)⊤i = 1 B2 X (x1,y1),(x2,y2)∈B \u0012n −1 n + 2d n(d + 2) \u0013 E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 + d n(d + 2) · E \u0002 L(θ; {(x1, y1)})⊤L(θ; {(x2, y2)}) \u0003 I. Next, note that when (x1, y1) ̸= (x2, y2), we have E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 = ∇L(θ)∇L(θ)⊤, and when (x1, y1) = (x2, y2) we have E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 = ∇L(θ)∇L(θ)⊤+ ΣMB(θ). Therefore 1 B2 X (x1,y1),(x2,y2)∈B E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 = ∇L(θ)∇L(θ)⊤+ 1 B Σ(θ), and plugging this yields E h b∇L(θ; B)b∇L(θ; B)⊤i = \u0012 1 + d −2 n(d + 2) \u0013 · \u0012 ∇L(θ)∇L(θ)⊤+ 1 B Σ(θ) \u0013 + d n(d + 2)I · \u0012 ∥∇L(θ)∥2 + 1 B tr(Σ(θ)) \u0013 . (7) Finally, we have E \u0014\r\r\rb∇L(θ; B) 2\u0015 = \u0012 1 + d2 + d −2 n(d + 2) \u0013 · \u0012 ∥∇L(θ)∥2 + 1 B tr(Σ(θ)) \u0013 = d + n −1 n · E h ∥∇L(θ; B)∥2i . 32 Proof of Theorem 1. By Taylor’s theorem with remainder, we have that L(θt+1) = L(θt) + ∇L(θt)⊤(θt+1 −θt) + Z 1 0 λ(θt+1 −θt)⊤∇2L(λθt+1 + (1 −λ)θt)(θt+1 −θt)⊤dλ Next, note that ∥θt+1 −θt∥= η b∇L(θ; B) ≤η √ d · 1 Bn X \f\fz⊤ i ∇L(θ; {(x, y)}) ≤ηdG(θt). Therefore ∥λθt+1 + (1 −λ)θt −θt∥≤ηdG(θt). By the assumption we have the upper bound ∇2L(λθt+1 + (1 −λ)θt) ⪯H(θt), and thus L(θt+1) ≤L(θt) + ∇L(θt)⊤(θt+1 −θt) + (θt+1 −θt)⊤H(θt)(θt+1 −θt) = L(θt) −η∇L(θt)⊤b∇L(θt; B) + 1 2η2 b∇L(θt; B)⊤H(θt)b∇L(θt; B). Taking the conditional expectation with respect to θt and plugging in (9), the formula for the covariance of our ZO estimate b∇L(θt; B), yields E[L(θt+1) | θt] ≤L(θt) −η ∥∇L(θt)∥2 + η2 2 D H(θt), E h b∇L(θ; B)b∇L(θ; B)⊤iE = L(θt) −η ∥∇L(θt)∥2 + η2 2 · d n(d + 2) \u0012 ∥∇L(θt)∥2 + 1 B tr(Σ(θt)) \u0013 tr(H(θt)) + η2 2 \u0012 1 + d −2 n(d + 2) \u0013 \u0012 ∇L(θt)⊤H(θt)∇L(θt) + 1 B ⟨Σ(θt), H(θt)⟩ \u0013 By assumption, the Hessian upper bound H(θt) satisfies ∥H(θt)∥op ≤ℓand tr(H(θt)) ≤ℓr. Thus E[L(θt+1) | θt] ≤L(θt) −η ∥∇L(θt)∥2 + η2ℓ 2 · \u0012dr + d −2 n(d + 2) + 1 \u0013 · \u0012 ∥∇L(θt)∥2 + 1 B tr(Σ(θt)) \u0013 = L(θt) −η ∥∇L(θt)∥2 + η2ℓ 2 · \u0012dr + d −2 n(d + 2) + 1 \u0013 · E h ∥∇L(θt; B)∥2i , as desired. G.1 Proofs of Global Convergence Lemma 4. Let L(θ) be µ-PL and let there exist α such that tr(Σ(θ)) ≤α(L(θ) −L∗) for all θ. Then after t = O \u0012\u0012 ℓ µ + ℓα µ2B \u0013 log L(θ0) −L∗ ϵ \u0013 iterations of SGD we have E[L(θt)] ≤L∗+ ϵ. Proof of Lemma 4. The descent lemma for SGD yields E[L(θt+1) | θt] −L(θt) ≤−η ∥∇L(θt)∥2 + 1 2η2ℓ· E[∥∇L(θt; B)∥2]. Plugging in E[∥∇L(θt; B)∥2] = ∥∇L(θt)∥2 + 1 B tr(Σ(θt)) and selecting a learning rate η ≤1 ℓ yields E[L(θt+1) | θt] ≤L(θt) −η 2 ∥∇L(θt)∥2 + η2ℓ 2B tr(Σ(θt)) Since L is µ-PL, we get E[L(θt+1) | θt] ≤L(θt) −ηµ(L(θt) −L∗) + η2ℓ 2B tr(Σ(θt)). 33 Since tr(Σ(θt)) ≤α(L(θt) −L∗), we have E[L(θt+1) | θt] ≤L(θt) −ηµ(L(θt) −L∗) + η2ℓα 2B (L(θt) −L∗). Altogether, E[L(θt+1)] −L∗≤ \u0012 1 −ηµ + η2ℓα 2B \u0013 (E[L(θt)] −L∗) Choosing η = min( 1 ℓ, µB ℓα ), we obtain E[L(θt+1)] −L∗≤ \u0012 1 −min( µ 2ℓ, µ2B 2ℓα ) \u0013 (E[L(θt)] −L∗). Therefore we reach a solution with E[L(θt)] −L∗≤ϵ after t = max \u00122ℓ µ , 2ℓα µ2B \u0013 log \u0012L(θ0) −L∗ ϵ \u0013 = O \u0012\u0012 ℓ µ + ℓα µ2B \u0013 log L(θ0) −L∗ ϵ \u0013 iterations. Proof of Lemma 3. By Corollary 1, ZO-SGD with ηZO = γ−1ηSGD yields E[L(θt+1) | θt] −L(θt) ≤1 γ · \u0014 −ηSGD ∥∇L(θt)∥2 + 1 2η2 SGDℓ· E[∥∇L(θ; B)∥2] \u0015 . As in the proof for SGD, choosing ηSGD ≤1 ℓyields E[L(θt+1) | θt] −L(θt) ≤γ−1 · \u0014 −ηSGD 2 ∥∇L(θt)∥2 + η2 SGDℓ 2B tr(Σ(θt)) \u0015 . Therefore under µ-PL and the tr(Σ(θt)) ≤α(L(θt) −L∗) assumption we obtain E[L(θt+1)] −E[L(θt)] ≤γ−1 · \u0014 −ηSGDµ + η2 SGDℓα 2B \u0015 · (E[L(θt)] −L∗) =⇒E[L(θt+1)] −L∗≤ \u0012 1 −γ−1 \u0012 ηSGDµ −η2 SGDℓα 2B \u0013\u0013 (E[L(θt)] −L∗). Choosing ηSGD = min( 1 ℓ, µB ℓα ) yields E[L(θt+1)] −L∗≤ \u0012 1 −γ−1 · min( µ 2ℓ, µ2B 2ℓα ) \u0013 (E[L(θt)] −L∗). Therefore we reach a solution with E[L(θt)] −L∗≤ϵ after t = γ · max \u00122ℓ µ , 2ℓα µ2B \u0013 log \u0012L(θ0) −L∗ ϵ \u0013 = O \u0012\u0010 r n + 1 \u0011 · \u0012 ℓ µ + ℓα µ2B \u0013 log L(θ0) −L∗ ϵ \u0013 iterations. G.1.1 Verification of assumptions We show that the tr(Σ(θt)) ≤α(L(θt) −L∗) assumption holds for certain losses. First, consider optimizing the model f(x; θ) with square loss, so that L(θ) = 1 N X i∈[N] (f(xi; θ) −yi)2. One then has that Σ(θ) = 2 N X i∈[N] (f(xi; θ) −yi)2∇f(xi; θ)∇f(xi; θ)⊤−∇L(θ)∇L(θ)⊤. 34 Therefore tr(Σ(θ)) ≤2 N X i∈[N] (f(xi; θ) −yi)2 ∥∇f(xi; θ)∥2 ≤2L(θ) X i∈[N] ∥∇f(xi; θ)∥2 . Assume that the data can be interpolated, i.e., L∗= 0. If the function is L-Lipschitz, i.e., ∥∇f(x; θ)∥≤L, then the condition holds with α = 2NL2. If we are in the kernel regime, i.e., f(xi; θ) = ϕ(xi)⊤θ for some feature map ϕ, then ∇2L(θ) = 2 N X i∈[N] f(xi; θ)∇f(xi; θ)⊤. Thus tr(Σ(θ)) ≤N tr(∇2L(θ)) · L(θ) ≤Nℓr · L(θ). So the condition holds for α = Nℓr. Next, consider the cross entropy loss function, i.e L(θ) = 1 N X i∈[N] exp(−yif(xi; θ)). One then has that Σ(θ) = 1 N X i∈[N] exp(−2yif(xi; θ))y2 i ∇f(xi; θ)∇f(xi; θ)⊤−L(θ)L(θ)⊤, Assume that the targets yi are bounded in [−1, 1] (which is true for binary classification tasks), and that L∗= 0 (which can be achieved if |f(x; θ)| can be sent to ∞) we have that tr(Σ(θ)) ≤1 N X i∈[N] exp(−2yif(xi; θ)) ∥∇f(xi; θ)∥2 . In the kernel regime, f(xi; θ) = ϕ(xi)⊤θ, and thus ∇2L(θ) = 1 N X i∈[N] exp(−yif(xi; θ))∇f(xi; θ)∇f(xi; θ)⊤. Therefore tr(Σ(θ)) ≤N tr(∇2L(θ)) · L(θ) ≤Nℓr · L(θ). Therefore the condition holds with α = Nℓr as well. G.2 Proofs for Gaussian perturbations The first lemma computes the second moment of the covariance estimate b∇L(θ; B) when z is drawn N(0, I). Lemma 5. Let zi ∼N(0, I) i.i.d. Then E h b∇L(θ; B)b∇L(θ; B)⊤i = \u0012 1 + 1 n \u0013 · \u0012 ∇L(θ)∇L(θ)⊤+ 1 B ΣMB(θ) \u0013 + 1 nI · \u0012 ∥∇L(θ)∥2 + 1 B tr(ΣMB(θ)) \u0013 . (8) Proof. As in the proof of Lemma 2, we have that in the ϵ →0 limit E h b∇L(θ; B)b∇L(θ; B)⊤i = 1 B2n2 X (x1,y1),(x2,y2)∈B X i,j∈[n] E \u0002 (ziz⊤ i ∇L(θ; {(x1, y1)}))(zjz⊤ j ∇L(θ; {(x2, y2)}))⊤\u0003 35 For vectors u, v, we have that Ezi,zj[ziz⊤ i uv⊤zjz⊤ j ] = uv⊤ when i ̸= j, and Ezi[ziz⊤ i uv⊤ziz⊤ i ] = Ez[z⊗4](u, v) = 3Sym(I⊗2)(u, v) = u⊤v · I + 2uv⊤. Therefore E h b∇L(θ; B)b∇L(θ; B)⊤i = 1 B2 X (x1,y1),(x2,y2)∈B \u0012n −1 n + 2 n \u0013 E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 + 1 n · E \u0002 L(θ; {(x1, y1)})⊤L(θ; {(x2, y2)}) \u0003 I. In the proof of Lemma 2 we showed that 1 B2 X (x1,y1),(x2,y2)∈B E \u0002 L(θ; {(x1, y1)})L(θ; {(x2, y2)})⊤\u0003 = ∇L(θ)∇L(θ)⊤+ 1 B Σ(θ). Plugging this yields E h b∇L(θ; B)b∇L(θ; B)⊤i = \u0012n + 1 n \u0013 · \u0012 ∇L(θ)∇L(θ)⊤+ 1 B Σ(θ) \u0013 + 1 nI · \u0012 ∥∇L(θ)∥2 + 1 B tr(Σ(θ)) \u0013 . (9) We can prove an analog to Theorem 1 in the case where the zi are Gaussian. One challenge is that ∥θt+1 −θt∥is no longer bounded; instead we the r-local effective rank assumption only holds with high probability, and thus to bound the expected loss decrease we must control the probability of the ∥θt+1 −θt∥being large. Consider the following modified version of the local r-effective rank assumption, where the upper bound on the Hessian is measured over a ball of radius twice as large as the one in Assumption 1. Assumption 2 (Local r-effective rank, Gaussian). Let G(θt) = max(x,y)∈D ∥∇L(θt; {(x, y)})∥. There exists a matrix H(θt) such that: 1. For all θ such that ∥θ −θt∥≤2ηdG(θt), we have ∇2L(θ) ⪯H(θt). 2. The effective rank of H(θt), i.e., tr(H(θt))/ ∥H(θt)∥op, is at most r. Theorem 2 (Dimension-Free Rate, Gaussian z). Assume the loss exhibits local r-effective rank (Assumption 2). If θt+1 = θt −ηZO b∇L(θt; B) is a single step of ZO-SGD using the n-SPSA estimate with a minibatch of size B, then there exists a γ = Θ(r/n) such that the expected loss decrease can be bounded as E[L(θt+1) | θt] −L(θt) ≤−ηZO ∥∇L(θt)∥2 + 1 2η2 ZOℓ· γ · E[∥∇L(θt; B)∥2] + η2 ZOℓG(θt)2 exp(−Ω(nd)). Proof of Theorem 2. Let A be the event that ∥θt+1 −θt∥≤2ηdG(θt). On A, we have that L(θt+1) ≤L(θt) −η∇L(θt)⊤b∇L(θ; B) + 1 2η2 b∇L(θt; B)⊤H(θ)b∇L(θt; B). Likewise, since L is ℓ-smooth, we have that L(θt+1) ≤L(θt) −η∇L(θt)⊤b∇L(θ; B) + 1 2η2ℓ b∇L(θt; B) 2 . 36 Therefore E[L(θt+1) | θt] ≤L(θt+1) −η ∥∇L(θt)∥2 + 1 2η2 D E h b∇L(θ; B)b∇L(θ; B)⊤· 1(A) i , H(θt) E + 1 2η2ℓE \u0014\r\r\rb∇L(θt; B) 2 · 1(¬A) \u0015 = L(θt+1) −η ∥∇L(θt)∥2 + 1 2η2 D E h b∇L(θ; B)b∇L(θ; B)⊤i , H(θt) E 1 2η2 D E h b∇L(θ; B)b∇L(θ; B)⊤· 1(¬A) i , ℓI −H(θt) E . The latter term can be bounded as follows 1 2η2 D E h b∇L(θ; B)b∇L(θ; B)⊤· 1(¬A) i , ℓI −H(θt) E ≤η2ℓE \u0014\r\r\rb∇L(θ; B) 2 · 1(¬A) \u0015 ≤η2ℓE \u0014\r\r\rb∇L(θ; B) 4\u0015 1 2 Pr[¬A]1/2. The gradient estimate b∇L(θ; B) satisfies b∇L(θ; B) ≤1 n X i∈[n] z⊤ i ∇L(θ; B) · ∥zi∥ The expectation term is upper bounded as E \u0014\r\r\rb∇L(θ; B) 4\u0015 ≤1 n X i∈[n] E h\f\fz⊤∇L(θ; B) 4 · ∥z∥4i ≤E h\f\fz⊤∇L(θ; B) 8i1/2 E h ∥z∥8i1/2 ≤ √ 105(d + 6)2G(θt)4, where we have plugged in explicit formulas for moments of Gaussian and χ2 random variables. Next, note that on the event ¬A, we have 2ηdG(θt) ≤∥θt+1 −θt∥= η b∇L(θt; B) ≤η · 1 n X i∈[n] ∥zi∥2 G(θt). Therefore Pr[¬A] ≤Pr  X i∈[n] ∥zi∥2 ≥2nd   Lemma 6 (Standard χ2-tail bound). Let Z be a χ2 random variable with k degrees of freedom. Then Pr[Z ≥k + u] ≤exp \u0012 −min \u0012 u2 16k , u 16 \u0013\u0013 Since P i∈[n] ∥zi∥2 is a χ2 random variable with nd degrees of freedom, we thus have that Pr[¬A] ≤exp \u0012 −nd 16 \u0013 . Altogether, 1 2η2 D E h b∇L(θ; B)b∇L(θ; B)⊤· 1(¬A) i , ℓI −H(θt) E ≤η2ℓ1051/4(d + 6)G(θt)2 exp(−nd 32 ) = η2ℓG(θt)2 exp(−Ω(nd)). 37 Finally, plugging in (8), along with the fact that ∥H(θt)∥op ≤ℓand tr(H(θt)) ≤ℓr, D E h b∇L(θ; B)b∇L(θ; B)⊤i , H(θt) E = r + n + 1 n · ℓ \u0012 ∥∇L(θt)∥2 + 1 B tr(Σ(θt)) \u0013 = r + n + 1 n · E h ∥∇L(θt; B)∥2i Thus letting γ = r+n+1 n yields E[L(θt+1) | θt] −L(θt) ≤−η ∥∇L(θt)∥2 + 1 2η2ℓ· γ · E[∥∇L(θt; B)∥2] + η2ℓG(θt)2 exp(−Ω(nd)), as desired. 38"
            }
        ]
    }
}