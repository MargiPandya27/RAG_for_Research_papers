Source: Pyramid Scene Parsing Network
Section Titles: ['Abstract', 'Introduction', 'methods', 'methods', 'results', 'Results', 'methods', 'methods', 'methods', 'results']
Start Index: 4000
--------------------------------------------------
validation and testing sets. As shown in column (d) of Fig. 2, PSPNet solves the common problems in FCN. Fig. 6 shows another few parsing results on validation set of ADE20K. Our results contain more accurate and detailed structures compared to the baseline. 5.3. PASCAL VOC 2012 Our PSPNet also works satisfyingly on semantic seg- mentation. We carry out experiments on the PASCAL VOC 2012 segmentation dataset , which contains 20 object categories and one background class. Following the proce- dure of , we use augmented data with the anno- tation of resulting 10,582, 1,449 and 1,456 images for training, validation and testing. Results are shown in Ta- ble 6, we compare PSPNet with previous best-performing on the testing set based on two settings, i.e., with or without pre-training on MS-COCO dataset . Meth- ods pre-trained with MS-COCO are marked by ‘†’. For fair comparison with current ResNet based frameworks in scene parsing/semantic segmentation task, we build our architecture based on ResNet101 while without post- processing like CRF. We evaluate PSPNet with several- scale input and use the average results following . Figure 7. Visual improvements on PASCAL VOC 2012 data. PSP- Net produces more accurate and detailed results. As shown in Table 6, PSPNet outperforms prior meth- ods on both settings. Trained with only VOC 2012 data, we achieve 82.6% accuracy2 – we get the highest accuracy on all 20 classes. When PSPNet is pre-trained with MS-COCO dataset, it reaches 85.4% accuracy3 where 19 out of the 20 classes receive the highest accuracy. Intriguingly, our PSP- Net trained with only VOC 2012 data outperforms existing trained with the MS-COCO pre-trained model. One may argue that our based classiﬁcation model is more powerful than several prior methods since ResNet was recently proposed. To exhibit our unique contribu- tion, we show