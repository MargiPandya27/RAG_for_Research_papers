Source: Pyramid Scene Parsing Network
Section Titles: ['Abstract', 'Introduction', 'methods', 'methods', 'results', 'Results', 'methods', 'methods', 'methods', 'results']
Start Index: 2500
--------------------------------------------------
layers, we let the two loss functions pass through all pre- vious layers. The auxiliary loss helps optimize the learning process, while the master branch loss takes the most respon- sibility. We add weight to balance the auxiliary loss. In the testing phase, we abandon this auxiliary branch and only use the well optimized master branch for ﬁnal pre- diction. This kind of deeply supervised training strategy for ResNet-based FCN is broadly useful under different ex- perimental settings and works with the pre-trained ResNet model. This manifests the generality of such a learning strategy. More details are provided in Section 5.2. 5. Experiments Our proposed method is successful on scene parsing and semantic segmentation challenges. We evaluate it in this section on three different datasets, including ImageNet scene parsing challenge 2016 , PASCAL VOC 2012 semantic segmentation and urban scene understanding dataset Cityscapes . 5.1. Implementation Details For a practical deep learning system, devil is always in the details. Our implementation is based on the public plat- form Caffe . Inspired by , we use the “poly” learning rate policy where current learning rate equals to the base one multiplying (1 − iter maxiter)power. We set base learning rate to 0.01 and power to 0.9. The performance can be improved by increasing the iteration number, which is set to 150K for ImageNet experiment, 30K for PASCAL VOC and 90K for Cityscapes. Momentum and weight decay are set to 0.9 and 0.0001 respectively. For data augmentation, we adopt ran- dom mirror and random resize between 0.5 and 2 for all datasets, and additionally add random rotation between - 10 and 10 degrees, and random Gaussian blur for ImageNet and PASCAL VOC. This comprehensive data augmentation scheme makes the network resist overﬁtting. Our network contains dilated convolution following . During the course