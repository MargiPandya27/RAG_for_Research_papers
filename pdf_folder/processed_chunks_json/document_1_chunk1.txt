Source: Pyramid Scene Parsing Network
Section Titles: ['Abstract', 'Introduction', 'methods', 'methods', 'results', 'Results', 'methods', 'methods', 'methods', 'results']
Start Index: 0
--------------------------------------------------
Scene parsing is challenging for unrestricted open vo- cabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region- based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is ef- fective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel- level prediction. The proposed approach achieves state-of- the-art performance on various datasets. It came ﬁrst in Im- ageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes. Scene parsing, based on semantic segmentation, is a fun- damental topic in computer vision. The goal is to assign each pixel in the image a category label. Scene parsing pro- vides complete understanding of the scene. It predicts the label, location, as well as shape for each element. This topic is of broad interest for potential applications of automatic driving, robot sensing, to name a few. Difﬁculty of scene parsing is closely related to scene and label variety. The pioneer scene parsing task is to clas- sify 33 scenes for 2,688 images on LMO dataset . More recent PASCAL VOC semantic segmentation and PASCAL context datasets include more labels with similar context, such as chair and sofa, horse and cow, etc. The new ADE20K dataset is the most challenging one with a large and unrestricted open vocabulary and more scene classes. A few representative images are shown in Fig. 1. To develop an effective algorithm for these datasets needs to conquer a few difﬁculties. State-of-the-art scene parsing frameworks are mostly based on the fully convolutional network (FCN) . The deep convolutional neural network (CNN)