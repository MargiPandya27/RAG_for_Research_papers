Source: Image Segmentation
Section Titles: ['Abstract', 'Introduction', 'Conclusion']
Start Index: 1750
--------------------------------------------------
to balance the class frequencies, d1 : Ω→R denotes the distance to the border of the nearest cell and d2 : Ω→R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and σ ≈5 pixels. In deep networks with many convolutional layers and diﬀerent paths through the network, a good initialization of the weights is extremely important. Oth- erwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of p 2/N, where N denotes the number of incoming nodes of one neu- ron . E.g. for a 3x3 convolution and 64 feature channels in the previous layer N = 9 · 64 = 576. 3.1 Data Augmentation Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 6 microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation. 4 Experiments We demonstrate the application of the u-net to three diﬀerent segmentation