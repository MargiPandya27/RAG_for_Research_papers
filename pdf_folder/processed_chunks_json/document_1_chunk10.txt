Source: Pyramid Scene Parsing Network
Section Titles: ['Abstract', 'Introduction', 'methods', 'methods', 'results', 'Results', 'methods', 'methods', 'methods', 'results']
Start Index: 2250
--------------------------------------------------
They are fused as the global prior. Then we concatenate the prior with the original feature map in the ﬁnal part of (c). It is followed by a convolution layer to generate the ﬁnal prediction map in (d). To explain our structure, PSPNet provides an effective global contextual prior for pixel-level scene parsing. The pyramid pooling module can collect levels of information, more representative than global pooling . In terms of computational cost, our PSPNet does not much increase it compared to the original dilated FCN network. In end-to- end learning, the global pyramid pooling module and the local FCN feature can be optimized simultaneously. 4. Deep Supervision for ResNet-Based FCN Deep pretrained networks lead to good performance . However, increasing depth of the network may introduce additional optimization difﬁculty as shown in for image classiﬁcation. ResNet solves this prob- lem with skip connection in each block. Latter layers of deep ResNet mainly learn residues based on previous ones. We contrarily propose generating initial results by super- vision with an additional loss, and learning the residue af- terwards with the ﬁnal loss. Thus, optimization of the deep network is decomposed into two, each is simpler to solve. An example of our deeply supervised ResNet101 model is illustrated in Fig. 4. Apart from the main branch using softmax loss to train the ﬁnal classiﬁer, another clas- siﬁer is applied after the fourth stage, i.e., the res4b22 residue block. Different from relay backpropagation that blocks the backward auxiliary loss to several shallow layers, we let the two loss functions pass through all pre- vious layers. The auxiliary loss helps optimize the learning process, while the master branch loss takes the most respon- sibility. We add weight to balance the auxiliary loss. In the testing phase, we abandon this auxiliary branch and only