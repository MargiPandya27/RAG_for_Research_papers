Source: Pyramid Scene Parsing Network
Section Titles: ['Abstract', 'Introduction', 'methods', 'methods', 'results', 'Results', 'methods', 'methods', 'methods', 'results']
Start Index: 1250
--------------------------------------------------
re- sults should be excluded so that the whole object is either skyscraper or building, but not both. This problem can be remedied by utilizing the relationship between categories. Inconspicuous Classes Scene contains objects/stuff of arbitrary size. Several small-size things, like streetlight and signboard, are hard to ﬁnd while they may be of great im- portance. Contrarily, big objects or stuff may exceed the Figure 2. Scene parsing issues we observe on ADE20K dataset. The ﬁrst row shows the issue of mismatched relationship – cars are seldom over water than boats. The second row shows confusion categories where class “building” is easily confused as “skyscraper”. The third row illustrates inconspicuous classes. In this example, the pillow is very similar to the bed sheet in terms of color and texture. These inconspicuous objects are easily misclassiﬁed by FCN. receptive ﬁeld of FCN and thus cause discontinuous pre- diction. As shown in the third row of Fig. 2, the pillow has similar appearance with the sheet. Overlooking the global scene category may fail to parse the pillow. To im- prove performance for remarkably small or large objects, one should pay much attention to different sub-regions that contain inconspicuous-category stuff. To summarize these observations, many errors are par- tially or completely related to contextual relationship and global information for different receptive ﬁelds. Thus a deep network with a suitable global-scene-level prior can much improve the performance of scene parsing. 3.2. Pyramid Pooling Module With above analysis, in what follows, we introduce the pyramid pooling module, which empirically proves to be an effective global contextual prior. In a deep neural network, the size of receptive ﬁeld can roughly indicates how much we use context information. Although theoretically the receptive ﬁeld of ResNet is already larger than the input image, it is shown by Zhou