Source: Image Segmentation
Section Titles: ['Abstract', 'Introduction', 'Conclusion']
Start Index: 1500
--------------------------------------------------
where ak(x) denotes the activation in feature channel k at the pixel position x ∈Ωwith Ω⊂Z2. K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈1 for the k that has the maximum activation ak(x) and pk(x) ≈0 for all other k. The cross entropy then penalizes at each position the deviation of pℓ(x)(x) from 1 using E = X x∈Ω w(x) log(pℓ(x)(x)) (1) 5 a b c d Fig. 3. HeLa cells on glass recorded with DIC (diﬀerential interference contrast) mi- croscopy. (a) raw image. (b) overlay with ground truth segmentation. Diﬀerent colors indicate diﬀerent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels. where ℓ: Ω→{1, . . . , K} is the true label of each pixel and w : Ω→R is a weight map that we introduced to give some pixels more importance in the training. We pre-compute the weight map for each ground truth segmentation to com- pensate the diﬀerent frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d). The separation border is computed using morphological operations. The weight map is then computed as w(x) = wc(x) + w0 · exp −(d1(x) + d2(x))2 2σ2 ! (2) where wc : Ω→R is the weight map to balance the class frequencies, d1 : Ω→R denotes the distance to the border of the nearest cell and d2 : Ω→R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and σ ≈5 pixels. In deep networks with many convolutional