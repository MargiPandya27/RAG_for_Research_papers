Source: Image Segmentation
Section Titles: ['Abstract', 'Introduction', 'Conclusion']
Start Index: 1250
--------------------------------------------------
by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each fol- lowed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the ﬁnal layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. 3 Training The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caﬀe . Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. The energy function is computed by a pixel-wise soft-max over the ﬁnal feature map combined with the cross entropy loss function. The soft-max is deﬁned as pk(x) = exp(ak(x))/ PK k′=1 exp(ak′(x))  where ak(x) denotes the activation in feature channel k at the pixel position x ∈Ωwith Ω⊂Z2. K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈1 for the k that has the maximum activation ak(x) and pk(x) ≈0 for all other k. The cross entropy